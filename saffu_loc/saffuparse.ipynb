{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import importlib to reload modules and sys and os to add the path for other imports\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Append the parent directory to the path to import the necessary modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# Import the utilities and the dataloader\n",
    "from utils import saffuutil\n",
    "\n",
    "# Now reload the modules to ensure they are up-to-date\n",
    "importlib.reload(saffuutil)\n",
    "\n",
    "# Import the model and funcs required from utils\n",
    "from utils.saffuutil import get_saffutok_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAFFU Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Prof Adapted\n",
    "\n",
    "This chunk signals the code in this section is custom and before trying to adapt/emulate professor code in saffu folder pulled from professor's repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing files:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1177/1960 [00:40<00:15, 49.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file ../data/all_train/AeMBR_LCI_Cost_9-9-15.xls: cannot convert float infinity to integer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing files:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1823/1960 [00:59<00:02, 47.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1960/1960 [01:12<00:00, 26.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files/Tokens: 1960/5479125\n"
     ]
    }
   ],
   "source": [
    "# Set the directory for the tokenizer data and call func\n",
    "tok_dir = \"../data/all_train/\"\n",
    "tok_train_data = get_saffutok_data(dir_path=tok_dir, threads=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region = \"Abandoned custom import, workout with prof\"\n",
    "# # Imports that are diff since we import only the func needed\n",
    "# from saffu import utilities_saffu\n",
    "# importlib.reload(utilities_saffu)\n",
    "# # For tokenizer setup - [5] ipynb\n",
    "# from saffu.utilities_saffu import get_config\n",
    "# endregion\n",
    "\n",
    "# Execute the necessary scripts to set up the environment\n",
    "exec(open(\"../src/configuration_saffu.py\").read())\n",
    "exec(open(\"../src/tokenization_saffu.py\").read())\n",
    "exec(open(\"../src/utilities_saffu.py\").read())\n",
    "exec(open(\"../src/data_saffu.py\").read())\n",
    "exec(open(\"../src/modeling_saffu.py\").read())\n",
    "exec(open(\"../src/training_saffu.py\").read())\n",
    "exec(open(\"../src/inference_saffu.py\").read())\n",
    "exec(open(\"../src/initialization_saffu.py\").read())\n",
    "exec(open(\"../src/load_data.py\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of model sizes applicable for get_config\n",
    "model_sizes = [\"micro\", \"tiny\", \"small\", \"medium\", \"big\"]\n",
    "\n",
    "# Custom name the DS\n",
    "data_set = \"500_train\"\n",
    "\n",
    "# Define the dict to store tokenizer cache\n",
    "tokenizer_directory = \"./cache/\"\n",
    "\n",
    "# Pick a model name from the list\n",
    "model_size = model_sizes[0]\n",
    "\n",
    "# Get the config for the current model size\n",
    "config = get_config(model_size=model_size)\n",
    "\n",
    "# Set the tokenizer name and create the tokenizer\n",
    "tokenizer_name = f\"{data_set}-{model_size}\"\n",
    "tokenizer = SAFFUTokenizer(config)\n",
    "\n",
    "# Set the vocab file path/name\n",
    "vocab_file = os.path.join(\n",
    "    tokenizer_directory,\n",
    "    tokenizer._model_path,\n",
    "    (tokenizer_name + \"-\" if tokenizer_name else \"\") + \"vocab.json\",\n",
    ")\n",
    "\n",
    "# Flag to reload the tokenizer or not\n",
    "reload = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: 500_train-micro\n",
      "Vocabulary size for experiment:  4034\n"
     ]
    }
   ],
   "source": [
    "# If reload = False and vocab file exists, load the tokenizer\n",
    "if not reload and os.path.exists(vocab_file):\n",
    "\n",
    "    ## Progress message\n",
    "    print(f\"Loading tokenizer: {tokenizer_name}\")\n",
    "\n",
    "    # Load the tokenizer\n",
    "    result = tokenizer.load(tokenizer_name, load_directory=tokenizer_directory)\n",
    "\n",
    "# Else if either reloading or vocab file does not exist, train the tokenizer\n",
    "else:\n",
    "\n",
    "    ## Progress message\n",
    "    print(f\"Training tokenizer: {tokenizer_name}\")\n",
    "\n",
    "    # Do the pretokenization\n",
    "    pretokenized_data = tokenizer.pretokenize_documents(tok_train_data)\n",
    "\n",
    "    # Train the tokenizer with the pretokenized data\n",
    "    tokenizer.train(pretokenized_data)\n",
    "\n",
    "    # Save the tokenizer vocabulary in the specified directory\n",
    "    tokenizer.save_vocabulary(tokenizer_name, save_directory=tokenizer_directory)\n",
    "\n",
    "# Print the vocabulary size for the experiment\n",
    "print(\"Vocabulary size for experiment: \", len(tokenizer._vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size for experiment:  4034\n",
      "['Th', 'es', 'e', ' c', 'as', 's', 'er', 'ol', 'es', ' d', 'is', 'gu', 'st', ' K', 'ay', 'la', '.']\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size for experiment: \", len(tokenizer._vocabulary))\n",
    "\n",
    "data_file = os.path.join(\n",
    "    tokenizer_directory,\n",
    "    tokenizer._model_path,\n",
    "    (tokenizer_name + \"-\" if tokenizer_name else \"\")\n",
    "    + f\"data-space_{tokenizer.config._space}-r_{tokenizer.config._r}-b_{tokenizer.config._b}-heads_{tokenizer.config._heads}-N_{tokenizer.config._N}.json\",\n",
    ")\n",
    "\n",
    "# Tokenize a sentence\n",
    "print(tokenizer._tokenize(\"These casseroles disgust Kayla.\"))\n",
    "\n",
    "# Check if tokenized stuff is in the vocabulary\n",
    "print(\n",
    "    [\n",
    "        x in tokenizer._vocabulary\n",
    "        for x in tokenizer._tokenize(\"These casseroles disgust Kayla.\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/500_train/retention-in-kindergarten.xlsx',\n",
       " '../data/500_train/meta%20deta%20set.xlsx',\n",
       " '../data/500_train/michelle_lokay_000_1_2_1.pst.263.xls',\n",
       " '../data/500_train/wais2012co2.xls',\n",
       " '../data/500_train/Table%20OTU%20within%20treatment%20SIMPER.xlsx',\n",
       " '../data/500_train/NPL%20Contaminants%2008%2025%202017.xlsx',\n",
       " '../data/500_train/SwintekEtAlSupplementaryData.xlsx',\n",
       " '../data/500_train/Vendor%20Study%20Metadata%20for%20Rapid.xlsx',\n",
       " '../data/500_train/appendix1.xls',\n",
       " '../data/500_train/darron_c_giron_000_1_1_1.pst.324.xls',\n",
       " '../data/500_train/darron_c_giron_002_1_1_1.pst.145.xls',\n",
       " '../data/500_train/Test%204.xlsx',\n",
       " '../data/500_train/Regeneration%20study%20GG2-90.xlsx',\n",
       " '../data/500_train/Figure2b_Right_Daytime%20East%20Dominated%20Wind%20Net%20NO2%20over%205%20ppb.xlsx',\n",
       " '../data/500_train/darrell_schoolcraft_000_1_1_1.pst.469.xls',\n",
       " '../data/500_train/Ohio%20County%20WV%20oil%20and%20gas%20mobile%20study%20dataset%20and%20data%20dictionary.xlsx',\n",
       " '../data/500_train/AirfoilCoordinatesComparison.xls',\n",
       " '../data/500_train/aicc2012official.xls',\n",
       " '../data/500_train/Data%20Set.xlsx',\n",
       " '../data/500_train/covid19-pwr-reactor.xlsx',\n",
       " '../data/500_train/cdfw-dmp-ldsvm-metadata-worksheet.xlsx',\n",
       " '../data/500_train/Data%20submission%20for%20A-8kq1.xlsx',\n",
       " '../data/500_train/james_steffes_000_1_1.pst.220.xls',\n",
       " '../data/500_train/NFRO%20Data%20V2.xlsx',\n",
       " '../data/500_train/odp_hcp_flu_vaccination_by_hospital_and_county_2023-24_datadictionary.xlsx',\n",
       " '../data/500_train/dmp-000048.xlsx',\n",
       " '../data/500_train/licensing-and-certification-district-offices-data-dictionary_mar2024.xlsx',\n",
       " '../data/500_train/311-data-dictionary.xlsx',\n",
       " '../data/500_train/Hg%20lamp%20Bg%20vs%20Ames.xlsx',\n",
       " '../data/500_train/pilant_mdpi_rs_dataTables_final.xlsx',\n",
       " '../data/500_train/darrell_schoolcraft_000_1_1_1.pst.523.xls',\n",
       " '../data/500_train/UW_Quillinan_REE_Rock%203rd%20Upload.xls',\n",
       " '../data/500_train/Supplemental_file%202.xlsx',\n",
       " '../data/500_train/EPIC_fig3_table.xlsx',\n",
       " '../data/500_train/keigwin2014.xls',\n",
       " '../data/500_train/darrell_schoolcraft_000_1_1_1.pst.293.xls',\n",
       " '../data/500_train/Dataset%20with%20metadata.xlsx',\n",
       " '../data/500_train/LUPA%20Experimental%20Meta%20Data.xlsx',\n",
       " '../data/500_train/NAFEC1-DOE-GDR.xls',\n",
       " '../data/500_train/pauldouglasrates201718.xls',\n",
       " '../data/500_train/lisa_gang_000_1_1.pst.316.xls',\n",
       " '../data/500_train/ZetaPotential_bananaPeel.xlsx',\n",
       " '../data/500_train/2022_diagnosiscodefrequencies_pdd.xlsx',\n",
       " '../data/500_train/Z_GC.xlsx',\n",
       " '../data/500_train/odp_hcp_flu_vaccination_by_county_2023-24_datadictionary.xlsx',\n",
       " '../data/500_train/scott_neal_000_1_1.pst.689.xls',\n",
       " '../data/500_train/bias_data_for_hclust_LT_avg_04232018_508.xlsx',\n",
       " '../data/500_train/Pt_XR_hypersensitivity_Science%20Hub_%20Data.xlsx',\n",
       " '../data/500_train/Table%202%20Data%20Kinetic%20Modeling.xlsx',\n",
       " '../data/500_train/Multiregional%20Passive%20Sampler%20Study%20Data%20Set%20and%20Dictionary.xlsx',\n",
       " '../data/500_train/Test5TiBlankDataOnly.xlsx',\n",
       " '../data/500_train/wais2013sulfate-nitrate-d17o.xlsx',\n",
       " '../data/500_train/Shu_etal_2023GMD_data.xlsx',\n",
       " '../data/500_train/Services_MetricNames.xlsx',\n",
       " '../data/500_train/hsiccraa-grantees2008.xls',\n",
       " '../data/500_train/section-619-allocation-table.xls',\n",
       " '../data/500_train/Cook16%20-%20Amphibole%20Data%20Set%20Unleached%20Surface%20Area%20%20Figure%206%20%28FibercellAsaU%29.xls',\n",
       " '../data/500_train/readme_infosheet.xlsx',\n",
       " '../data/500_train/retention-in-grade-3.xlsx',\n",
       " '../data/500_train/sara_shackleton_000_1_2.pst.114.xls',\n",
       " '../data/500_train/diana_scholtes_000_1_1.pst.3.xls',\n",
       " '../data/500_train/Haselman_et_al_tOP%26TB_data_Ar22z.xlsx',\n",
       " '../data/500_train/edward2007.xls',\n",
       " '../data/500_train/Figure_4_data.xlsx',\n",
       " '../data/500_train/2004-table01.xls',\n",
       " '../data/500_train/lynn_blair_000_1_1.pst.56.xls',\n",
       " '../data/500_train/brown2011.xls',\n",
       " '../data/500_train/GeothermalFlexibilityCharacteristics6-2-20.xlsx',\n",
       " '../data/500_train/ESH%20well%20tops%20to%20estimate%20Cornell%20tops.xlsx',\n",
       " '../data/500_train/harassment-bullying-on-basis-of-race-disciplined.xlsx',\n",
       " '../data/500_train/OMEGOZ%20data%20combined-RAPID.xlsx',\n",
       " '../data/500_train/Combined_original_data_Clean_production_Zhang_.xlsx',\n",
       " '../data/500_train/Chemical%20Analysis%20Summary%20for%202011%20091322.xlsx',\n",
       " '../data/500_train/33-7-DOE-GDR.xls',\n",
       " '../data/500_train/LCOE%20Content%20Model%20v1.1%20-%20Centipod-SPAI-Baseline.xlsx',\n",
       " '../data/500_train/darrell_schoolcraft_000_1_1_1.pst.243.xls',\n",
       " '../data/500_train/Mayfly%3B%20Chronic%20Cu%2B%2B%20Ref%20Tox%20Round%20III.xlsx',\n",
       " '../data/500_train/harassment-bullying-on-basis-of-disability-disciplined.xlsx',\n",
       " '../data/500_train/CalWave%20MP%207-20-17.xlsx',\n",
       " '../data/500_train/john_hodge_000_1_1.pst.29.xls',\n",
       " '../data/500_train/TABLE%205%20Range%20of%20Estimated%20Total%20Annual%20Water%20Costs%20for%20EGS%20Projects%20%28in%20%24%3Ayear%29%20%281%29.xlsx',\n",
       " '../data/500_train/SARS-CoV-2%20and%20PMMoV%20concentration.xlsx',\n",
       " '../data/500_train/tx3c00398_si_001.xlsx',\n",
       " '../data/500_train/LCFFigure.xlsx',\n",
       " '../data/500_train/richard_sanders_000_1_1_1.pst.41.xls',\n",
       " '../data/500_train/1-s2.0-S0041008X19303655-mmc3.xlsx',\n",
       " '../data/500_train/CMAQ_fig13a_monthly_mean_NH4_dep.xlsx',\n",
       " '../data/500_train/SI%20Tables_Jan19.xlsx',\n",
       " '../data/500_train/opendata-omb-budget-2015.xlsx',\n",
       " '../data/500_train/data-revolving-funds-may-2019.xlsx',\n",
       " '../data/500_train/Model_data_file_catalogue_FSV-Scen.xlsx',\n",
       " '../data/500_train/March%202019%20Summary%20and%20Assignment%20Report%20-%20Final%20508.xls',\n",
       " '../data/500_train/Coweeta%20hi-vol%20Chen%20et%20al%20for%20science%20hub.xlsx',\n",
       " '../data/500_train/D5%20Risk%20Register%2063258_IVC%20DOE_%2002%2021%2017.xlsx',\n",
       " '../data/500_train/darrell_schoolcraft_000_1_1_1.pst.147.xls',\n",
       " '../data/500_train/eiriksson2006.xls',\n",
       " '../data/500_train/aguada-xcaamal2005.xls',\n",
       " '../data/500_train/09q2ffelstate.xls',\n",
       " '../data/500_train/DE-EE0007347%20T14.4%20SR-RR-0520%20Safety%20PD%20v3.7%2006-27-2018%20%281%29.xlsx',\n",
       " '../data/500_train/GTO_data_deposit_092017_Desorption.xlsx',\n",
       " '../data/500_train/Data%20submission%20for%20A-d25m.xlsx',\n",
       " '../data/500_train/Zn-binding-S-layers.xlsx',\n",
       " '../data/500_train/gerald_nemec_000_1_1.pst.157.xls',\n",
       " '../data/500_train/lynn_blair_000_1_1.pst.121.xls',\n",
       " '../data/500_train/Supp%20Files.xlsx',\n",
       " '../data/500_train/July%202014%20Summary%20and%20Assignment%20Report%20-%20FINAL%20508.xls',\n",
       " '../data/500_train/gravity-dvnew-final.xlsx',\n",
       " '../data/500_train/diacetyl%20figure%204A%20sci%20hub.xlsx',\n",
       " '../data/500_train/alaska.xls',\n",
       " '../data/500_train/transfer-encounters.xlsx',\n",
       " '../data/500_train/fy2017mseipnccawards.xlsx',\n",
       " '../data/500_train/darrell_schoolcraft_000_1_1_1.pst.146.xls',\n",
       " '../data/500_train/Zimmerman_A_w9h6_Dataset_20160921.xlsx',\n",
       " '../data/500_train/fiscaldata1997-98fws-st.xls',\n",
       " '../data/500_train/paleosol_breecker_2014.xlsx',\n",
       " '../data/500_train/police-litigations.xlsx',\n",
       " '../data/500_train/don_baughman_000_1_1.pst.224.xls',\n",
       " '../data/500_train/darrell_schoolcraft_000_1_1_1.pst.391.xls',\n",
       " '../data/500_train/richard_sanders_001_1_2.pst.91.xls',\n",
       " '../data/500_train/scott_neal_000_1_1.pst.837.xls',\n",
       " '../data/500_train/Economics_Improved.xlsx',\n",
       " '../data/500_train/darrell_schoolcraft_000_1_1_1.pst.352.xls',\n",
       " '../data/500_train/2017-MEPS-account-release-table.xlsx',\n",
       " '../data/500_train/2023_externalcauseofmorbidityfrequencies_pdd.xlsx',\n",
       " '../data/500_train/Bill%20of%20Materials%20LUPA.xls',\n",
       " '../data/500_train/2022_procedurecodefrequencies_pdd.xlsx',\n",
       " '../data/500_train/harassment-bullying-on-basis-of-sex-reported.xlsx',\n",
       " '../data/500_train/takahula2010.xls',\n",
       " '../data/500_train/WO-2%20Rock%20Strength%20parameters.xlsx',\n",
       " '../data/500_train/Van%20Generden%20et%20al.%202020%20Supplemental%20Information%20Data.xlsx',\n",
       " '../data/500_train/Analysis%20of%20Microcystins%20in%20Alum%20Sludges%20Science%20Hub%20Supporting%20Data.xlsx',\n",
       " '../data/500_train/HoffmanJoel_A_sj4f_Dataset_20160819.xlsx',\n",
       " '../data/500_train/maintext_figs.xlsx',\n",
       " '../data/500_train/ScienceHub_Eckley%20et%20al_FrontEnvChem_2023.xlsx',\n",
       " '../data/500_train/CAEandLactoneHydrolysisDataset_final.xlsx',\n",
       " '../data/500_train/lindy_donoho_000_1_1_1.pst.114.xls',\n",
       " '../data/500_train/November%202016%20%20Summary%20and%20Assignment%20Report%20-%20FINAL%20508.xls',\n",
       " '../data/500_train/Greenhouse%20Climate%20Dates%2012%2005%2022.xlsx',\n",
       " '../data/500_train/scott_neal_000_1_1.pst.836.xls',\n",
       " '../data/500_train/pell-inst-13-14.xls',\n",
       " '../data/500_train/ssi_sc14.xlsx',\n",
       " '../data/500_train/8-4-17-All-Full-scan-MC-LR%20Peak%20Area%20Comparison.xlsx',\n",
       " '../data/500_train/Transportation%20-%20Bikeshare%20Statistics.xlsx',\n",
       " '../data/500_train/StableNanoparticleData.xlsx',\n",
       " '../data/500_train/don_baughman_000_1_1.pst.219.xls',\n",
       " '../data/500_train/advanced-placement-science-enrollment (1).xlsx',\n",
       " '../data/500_train/SciHub_ExtruderPaper_Data.xlsx',\n",
       " '../data/500_train/jeffrey_a_shankman_000_1_2.pst.139.xls',\n",
       " '../data/500_train/lisa_gang_000_1_1.pst.167.xls',\n",
       " '../data/500_train/Dhond_and_Barron_Data.xlsx',\n",
       " '../data/500_train/dlbyforbearancetype.xls',\n",
       " '../data/500_train/CCDC-Manuscript-Data_STICKS-SCIHUB_FINAL.xls',\n",
       " '../data/500_train/all-mental-health-hpsas.xlsx',\n",
       " '../data/500_train/StateCounty2006.xls',\n",
       " '../data/500_train/2016-2017-health-care-quality-report-card-ratings.xlsx',\n",
       " '../data/500_train/StateCounty2012.xls',\n",
       " '../data/500_train/Site%20Info%202008_2014.xlsx',\n",
       " '../data/500_train/yamen2010.xls',\n",
       " '../data/500_train/ALFA_Parameters.xlsx',\n",
       " '../data/500_train/WWT%20LCAresults%20.xlsx',\n",
       " '../data/500_train/CDD_2014_US_Disposition_Estimates_Using_CDDPath_v2.xlsx',\n",
       " '../data/500_train/imgw-poland-t-p-indices.xls',\n",
       " '../data/500_train/scott_neal_000_1_1.pst.947.xls',\n",
       " '../data/500_train/09q4ffelstate.xls',\n",
       " '../data/500_train/1-s2.0-S0041008X19303655-mmc5.xlsx',\n",
       " '../data/500_train/Harassment-Bullying-on-basis-of-sex_reported.xlsx',\n",
       " '../data/500_train/October%202018%20Summary%20and%20Assignment%20Report%20-%20Final%20508.xls',\n",
       " '../data/500_train/opendata-finance-publicly-available-properties-2015.xlsx',\n",
       " '../data/500_train/tracy_geaccone_000_1_1.pst.185.xls',\n",
       " '../data/500_train/Log_Acronyms_DDU%20Geothermal.xlsx',\n",
       " '../data/500_train/Copy%20of%20mucus%20and%20iron%20figure%207A.xlsx',\n",
       " '../data/500_train/quelccaya2017.xlsx',\n",
       " '../data/500_train/darrell_schoolcraft_000_1_1_1.pst.381.xls',\n",
       " '../data/500_train/mt-arthur1998.xls',\n",
       " '../data/500_train/Data%20for%20Science%20Hub.xlsx',\n",
       " '../data/500_train/EnviroAtlas%20-%202010%20Dasymetric%20Population%20for%20the%20Conterminous%20United%20States%20v3%20-%20County%20Level%20Error.xlsx',\n",
       " '../data/500_train/Harassment-Bullying-on-basis-of-race_disciplined.xlsx',\n",
       " '../data/500_train/awsda-reporting-tables-2022-04-20.xlsx',\n",
       " '../data/500_train/arizona.xls',\n",
       " '../data/500_train/brad_mckay_000_1_1.pst.24.xls',\n",
       " '../data/500_train/20162017data.xls',\n",
       " '../data/500_train/Metabolite%20data%20for%20ETNC.xlsx',\n",
       " '../data/500_train/SNP_Count_Totals.xlsx',\n",
       " '../data/500_train/0220%20Extracted%20DNA%20concentrations_022018.xlsx',\n",
       " '../data/500_train/Figure%201%20AB%20buxco%20data%20Cyphert%202016%20JTEHA.xlsx',\n",
       " '../data/500_train/municipal-building-energy-and-water-use.xlsx',\n",
       " '../data/500_train/StateCounty2011.xls',\n",
       " '../data/500_train/FG_ranking_data_for_pub.xlsx',\n",
       " '../data/500_train/richard_ring_000_1_1.pst.90.xls',\n",
       " '../data/500_train/March%202016%20Summary%20and%20Assignment%20Report%20-%20FINAL%20508.xls',\n",
       " '../data/500_train/Tularosa_PFA_Phase2_Water_Well_Database.xlsx',\n",
       " '../data/500_train/Figure%201%20paper%202%20Method%20200.8.xlsx',\n",
       " '../data/500_train/ABS-Blue_Raw%20Data.xlsx',\n",
       " '../data/500_train/scott_neal_000_1_1.pst.978.xls',\n",
       " '../data/500_train/Figure%20Data.xlsx',\n",
       " '../data/500_train/FORGE_groundwater_tables.xlsx',\n",
       " '../data/500_train/richard_sanders_001_1_1_1.pst.0.xls',\n",
       " '../data/500_train/shelley_corman_000_1_1.pst.61.xls',\n",
       " '../data/500_train/MARGA_Chen%20et%20al_2016.xlsx',\n",
       " '../data/500_train/US-AP-and-IB.xlsx',\n",
       " '../data/500_train/teacher-certification-and-years-of-experience.xlsx',\n",
       " '../data/500_train/Community%20Services%20Resources%20-%20Historic%20Districts.xlsx',\n",
       " '../data/500_train/dnc-data-book-fy-12_0.xlsx',\n",
       " '../data/500_train/SSI_IP_ExperienceThru17.xlsx',\n",
       " '../data/500_train/TEMMS_GW-SW_Monitoring_Data.xlsx',\n",
       " '../data/500_train/Master%20FORGE%20XRD%20data%20table.xlsx',\n",
       " '../data/500_train/Nanofiltration.xlsx',\n",
       " '../data/500_train/SciHub_CoV_012422.xlsx',\n",
       " '../data/500_train/StateCounty2020.xls',\n",
       " '../data/500_train/copy%20of%20table%20for%207%20nano%20on%20BEAS2B%205-9-18.xlsx',\n",
       " '../data/500_train/CAPs%20Acrolein%20Source%20Apportionment.xlsx',\n",
       " '../data/500_train/di_asr09.xlsx',\n",
       " '../data/500_train/Data%20in%20LPG%20paper-%2020180505.xlsx',\n",
       " '../data/500_train/Arsenic_Data_Clearance.xlsx',\n",
       " '../data/500_train/lisa_gang_000_1_1.pst.196.xls',\n",
       " '../data/500_train/darrell_schoolcraft_000_1_1_1.pst.403.xls',\n",
       " '../data/500_train/korallgrottan2010.xls',\n",
       " '../data/500_train/DEG-0.05-GRC336_resultsubset_66_30%20.xlsx',\n",
       " '../data/500_train/dutch_quigley_000_1_1.pst.197.xls',\n",
       " '../data/500_train/META%20analysis%20-%20PLS%20Regression.xlsx',\n",
       " '../data/500_train/RN12%20%26%20RN19%20Fluid.xls',\n",
       " '../data/500_train/TotalEmployment2018.xls',\n",
       " '../data/500_train/dp02-2021-acs-5-yr-census-language-data-dictionary.xlsx',\n",
       " '../data/500_train/guliya1997.xls',\n",
       " '../data/500_train/KineticRates_10Feb2017.IM.xlsx',\n",
       " '../data/500_train/water_balance_to_water_budget_pilot_workbook.xlsx',\n",
       " '../data/500_train/kevin_hyatt_000_1_1.pst.51.xls',\n",
       " '../data/500_train/10-3-16%20Paper%20samples%20for%20PFAS%20extraction.xlsx',\n",
       " '../data/500_train/Yuan_etal_2021_SI_003_PhotolysisRateData.xlsx',\n",
       " '../data/500_train/antarctica2011iso.xls',\n",
       " '../data/500_train/scott_neal_000_1_1.pst.802.xls',\n",
       " '../data/500_train/Scientific%20Reports2017%20Data%20Tables%20and%20Dictionary.xlsx',\n",
       " '../data/500_train/Sec4.2%202010_agedistribution.xls',\n",
       " '../data/500_train/Metadata_Dictionary_for_Meta-dataset_for_property_values_and_water_quality.xlsx',\n",
       " '../data/500_train/Hance_fish_detection_data1.xlsx',\n",
       " '../data/500_train/machine_readable_puf.xlsx',\n",
       " '../data/500_train/cdfw-dmp-metadata-worksheet_isp_qzmm.xlsx',\n",
       " '../data/500_train/Collated%20Water%20Chemistry%20All%20dates.xlsx',\n",
       " '../data/500_train/sara_shackleton_003_1_1.pst.77.xls',\n",
       " '../data/500_train/table02fl1718_se.xlsx',\n",
       " '../data/500_train/SciHub%20data%20ORD-019271.xlsx',\n",
       " '../data/500_train/PnP%20Sector%20Boilers%20Dataset.xlsx',\n",
       " '../data/500_train/Hill%20et%20al%20Toxicol%20Sci_Table%203%20NPV%20of%20subchronic%20to%20tumor%20v6.xlsx',\n",
       " '../data/500_train/darrell_schoolcraft_000_1_1_1.pst.174.xls',\n",
       " '../data/500_train/darrell_schoolcraft_000_1_1_1.pst.612.xls',\n",
       " '../data/500_train/SUMMARY%20Fused%20Silica%20samples%20fracking_Q4.xlsx',\n",
       " '../data/500_train/Final%20Prenatal%20Exposure%20Data.xlsx',\n",
       " '../data/500_train/Rapid%20Uterotrophic%20Assay%20UTA%20Data%20Supplemental%20File%202024%20PFAS.xlsx',\n",
       " '../data/500_train/RECRUIT_REEF_FISH-HABITAT_QUADRATS_2005.xls',\n",
       " '../data/500_train/Figure%203%20and%20Figure%204.xlsx',\n",
       " '../data/500_train/2005-table06.xls',\n",
       " '../data/500_train/ubfy2018.xlsx',\n",
       " '../data/500_train/TotalEmployment2015_0.xls',\n",
       " '../data/500_train/kimberly_watson_000_1_1.pst.116.xls',\n",
       " '../data/500_train/barcode%2016s_2018%20Feb%20Toledo.xlsx',\n",
       " '../data/500_train/lisa_gang_000_1_1.pst.224.xls',\n",
       " '../data/500_train/1819_DSC_TGA_MIT_HT_Hy_all_08082016.xlsx',\n",
       " '../data/500_train/john_hodge_000_1_1.pst.96.xls',\n",
       " '../data/500_train/gerald_nemec_000_1_1.pst.159.xls',\n",
       " '../data/500_train/Figure%202%20metadata.xlsx',\n",
       " '../data/500_train/Figure9b_timeseries_ptile_LRTfrac_NAM_Summer.xlsx',\n",
       " '../data/500_train/allocationshbcutccumsisip.xlsx',\n",
       " '../data/500_train/Science%20Hub_sulfate%20radical.xlsx',\n",
       " '../data/500_train/CRED_REA_FISH_GUGUAN_2005.xls',\n",
       " '../data/500_train/ubmsfy2018.xlsx',\n",
       " '../data/500_train/TableS2.xlsx',\n",
       " '../data/500_train/Cued%20CRT_SciHub.xlsx',\n",
       " '../data/500_train/jeffrey_a_shankman_000_1_2.pst.131.xls',\n",
       " '../data/500_train/kevin_hyatt_000_1_1.pst.55.xls',\n",
       " '../data/500_train/NE_SedimentDiatom_SAR_LakeCat_20230926%20with%20metadata.xlsx',\n",
       " '../data/500_train/composite-table-every-table.xlsx',\n",
       " '../data/500_train/PTU%20and%20MMI%20TPO%20Inhibition%20Data%20for%20Science%20Hub.xlsx',\n",
       " '../data/500_train/MS2%20UVC%20Manuscript_Complete%20Data.xlsx',\n",
       " '../data/500_train/john_hodge_000_1_1.pst.160.xls',\n",
       " '../data/500_train/lisa_gang_000_1_1.pst.192.xls',\n",
       " '../data/500_train/water-rights-data-dictionary-annual-report.xlsx',\n",
       " '../data/500_train/AnMBR%20Mixed%20Wastewater%20Building%20Scale%20LCI%20Calcs_SF%20Reuse.xlsx',\n",
       " '../data/500_train/idrportfolio-by-school-type.xls',\n",
       " '../data/500_train/2005fair.xls',\n",
       " '../data/500_train/Fig_9_DOexposure.xlsx',\n",
       " '../data/500_train/cdfw-dmp-metadata-worksheet_reilly_81.xlsx',\n",
       " '../data/500_train/GEOPHIRES_InputParameterDescriptions.xlsx',\n",
       " '../data/500_train/documentation-city-owned-properties.xlsx',\n",
       " '../data/500_train/Figure9a_timeseries_ptile_LRTfrac_NAM_Spring.xlsx',\n",
       " '../data/500_train/upper-colorado-recons.xls',\n",
       " '../data/500_train/jellybean2005.xls',\n",
       " '../data/500_train/epi_suite_physicochemical_properties.xlsx',\n",
       " '../data/500_train/Newberry_backflow_prelim_report_12_24_14.xlsx',\n",
       " '../data/500_train/BHRS_well_data_111607.xlsx',\n",
       " '../data/500_train/brine%20composition%20density%20compilation.xlsx',\n",
       " '../data/500_train/NG07-DOE-GDR.xls',\n",
       " '../data/500_train/CIvesSupplementaryTables_revised_08-04-17.xlsx',\n",
       " '../data/500_train/SCH-0005-Overall-Enrollment.xlsx',\n",
       " '../data/500_train/shelley_corman_000_1_1.pst.40.xls',\n",
       " '../data/500_train/cdfw-dmp-metadata-van-damme-temperature-22-06-29.xlsx',\n",
       " '../data/500_train/Part2_Iso-Aging_ScienceHub_entry-Data_Final_gcms.xlsx',\n",
       " '../data/500_train/darrell_schoolcraft_000_1_1_1.pst.348.xls',\n",
       " '../data/500_train/Community%20Services%20Resources%20-%20Athletic%20Fields.xlsx',\n",
       " '../data/500_train/SupTable2-RawPPBData.xlsx',\n",
       " '../data/500_train/District.xlsx',\n",
       " '../data/500_train/TABLE%202%20%20Power%20Plant%20Scenarios%20and%20Water%20Consumption%20Rates%20%281%29.xlsx',\n",
       " '../data/500_train/2022_externalcauseofmorbidityfrequencies_pdd.xlsx',\n",
       " '../data/500_train/Methane%20Transect%20Data%20Set.xlsx',\n",
       " '../data/500_train/lisa_gang_000_1_1.pst.321.xls',\n",
       " '../data/500_train/%21T02%20Modeling%20parameters.xlsx',\n",
       " '../data/500_train/SWAT_RockCreek_Baseline_Calibration_Validation.xlsx',\n",
       " \"../data/500_train/paul_y'barbo_000_1_1.pst.53.xls\",\n",
       " '../data/500_train/Data%20Set%20Rifle%20and%20Pistol%202024.xlsx',\n",
       " '../data/500_train/Data%20for%20Table%20S6.xlsx',\n",
       " '../data/500_train/ssi_sc10.xlsx',\n",
       " '../data/500_train/%23sequnecing%20sample%20list%26DNA%20concentration%20result.xlsx',\n",
       " '../data/500_train/Sets%201-4%20composites%20pesticides%20%26%20phth%20rev3.2.xls',\n",
       " '../data/500_train/Monthly%20Statistics%202013%262014.xlsx',\n",
       " '../data/500_train/Forecast%20-%20Arlington%20County%20Forecast.xlsx',\n",
       " '../data/500_train/darron_c_giron_002_1_1_1.pst.165.xls',\n",
       " '../data/500_train/Seahorse%20Manuscript%20Data.xlsx',\n",
       " '../data/500_train/allocations.xls',\n",
       " '../data/500_train/1617-bassessment-1.xlsx',\n",
       " '../data/500_train/fiscaldata1999-2000fws-st.xls',\n",
       " '../data/500_train/Agg_rGO.Silver_Metals.xls',\n",
       " '../data/500_train/GordonChristopher_E331%20TP%20HF%20O3%20SHC%202_63_dataset_20170309%20SCID%20A-bvqk.xlsx',\n",
       " '../data/500_train/lisa_gang_000_1_1.pst.296.xls',\n",
       " '../data/500_train/Compiled%20Meteorology.xlsx',\n",
       " '../data/500_train/lisa_gang_000_1_1.pst.297.xls',\n",
       " '../data/500_train/1-s2.0-S0160412022003129-mmc2.xlsx',\n",
       " '../data/500_train/ARG_NRSA_stics_scott_keely.xlsx',\n",
       " '../data/500_train/Figure%202%20with%20data%205-15-13.xlsx',\n",
       " '../data/500_train/us_low_temp_hydro_080316.xlsx',\n",
       " '../data/500_train/louise_kitchen_001_1_1_1.pst.60.xls',\n",
       " '../data/500_train/alabama.xlsx',\n",
       " '../data/500_train/darron_c_giron_002_1_1_1.pst.158.xls',\n",
       " '../data/500_train/ScienceHub_FIMofNielsenAndCPCat_2020-06-03.xlsx',\n",
       " '../data/500_train/cdfw-dmp-metadata-worksheet-dmp-000452-recreational-angler-surveys-lake-silverwood-1999-2020.xlsx',\n",
       " '../data/500_train/SciHub_Figures_Food%20Waste%20Diversion_April%202022.xlsx',\n",
       " '../data/500_train/StonyCoral_PR2011_FNDec052016.xlsx',\n",
       " '../data/500_train/dmp-000051-metadata-worksheet.xlsx',\n",
       " '../data/500_train/Neurite%20Growth%20Human%20iCell.xlsx',\n",
       " '../data/500_train/c-compare.xls',\n",
       " '../data/500_train/index_models_hydrothermal.xlsx',\n",
       " '../data/500_train/arizona-az-matrix-2014c.xls',\n",
       " '../data/500_train/GordonChristopher_Thermal%20Floor_ACE_121_As-sf86_Data%20Set.xlsx',\n",
       " '../data/500_train/Spreadsheet%20of%20Tables%20and%20Figures.xlsx',\n",
       " '../data/500_train/richard_sanders_000_1_2.pst.15.xls',\n",
       " '../data/500_train/Droplet%20size%20distribution%20Clorox360%20DI%20Tap%20Puretabs%20Ecolab.xlsx',\n",
       " '../data/500_train/Fig.%202%20and%20Supp%20Fig.%202_Six1%20Knockout%20DES_Image%20Analysis_Immuno2_Epithelium%20Analyzed.xlsx',\n",
       " '../data/500_train/August%202016%20%20Summary%20and%20Assignment%20Report%20-%20FINAL%20508.xls',\n",
       " '../data/500_train/HWBI_LAParishes_20221101.xlsx',\n",
       " '../data/500_train/richard_sanders_001_1_1_1.pst.10.xls',\n",
       " '../data/500_train/PCB%20data%20for%20ScienceHub_food_no_carbon_Final.xlsx',\n",
       " '../data/500_train/15action.xls',\n",
       " '../data/500_train/Raposa%20and%20Chintala%20data.xlsx',\n",
       " '../data/500_train/2003-table03.xls',\n",
       " '../data/500_train/Figure%201%20CD%20flexivent%20data%20Cyphert%202016%20JTEHA.xlsx',\n",
       " '../data/500_train/water-rights-status-history-data-dictionary.xlsx',\n",
       " '../data/500_train/RECRUIT-REEF-FISH_BELT-TRANSECTS_2006-07.xls',\n",
       " '../data/500_train/Education%20-%20Graduate%20School%20Enrollment.xlsx',\n",
       " '../data/500_train/Copy%20of%20GCMS%20Analysis%20Results%20N10%20for%20Naphthalene%20Knm%20Test.xlsx',\n",
       " '../data/500_train/2006fair.xls',\n",
       " '../data/500_train/A_Summary_For_CaseControl_20161002_508.xlsx',\n",
       " '../data/500_train/AmbiTaxa2017pub_ScienceHub_rawdata.xlsx',\n",
       " '../data/500_train/james_steffes_000_1_1.pst.166.xls',\n",
       " '../data/500_train/retention-in-grade-1.xlsx',\n",
       " '../data/500_train/Figure%20Data%20V1.xlsx',\n",
       " '../data/500_train/Data%20set%20for%20degassed%20media%20and%20brine.xlsx',\n",
       " '../data/500_train/legdemographicsnov22-2023-08-15.xlsx',\n",
       " '../data/500_train/lynn_blair_000_1_1.pst.158.xls',\n",
       " '../data/500_train/richard_ring_000_1_1.pst.173.xls',\n",
       " '../data/500_train/DE-EE0007627%20D1.2%20LP-RR-0520%20Emergency%20Systems%20PD%2007-20-2019.xlsx',\n",
       " '../data/500_train/Data%20in%20Cookstove%20UFP%20paper%20-%202017Aug17.xlsx',\n",
       " '../data/500_train/MagnusonMatthew_A-f1w2_dataset_20200228.xlsx',\n",
       " '../data/500_train/Perchlorate_studies.xlsx',\n",
       " '../data/500_train/SciHub_DataEntry.xlsx',\n",
       " '../data/500_train/darrell_schoolcraft_000_1_1_1.pst.472.xls',\n",
       " '../data/500_train/icon-arto1-2015-data.xlsx',\n",
       " '../data/500_train/highlands_crop_area.xlsx',\n",
       " '../data/500_train/chris_germany_000_1_2.pst.1016.xls',\n",
       " '../data/500_train/ssi_asr19.xlsx',\n",
       " '../data/500_train/dosvao_data.xls',\n",
       " '../data/500_train/negev2010.xls',\n",
       " '../data/500_train/Wilkin%20et%20al.%20%282020%29%20dataset.xlsx',\n",
       " '../data/500_train/Table%202%20-%20GETEM%20Hydrothermal%20Scenarios.xlsx',\n",
       " '../data/500_train/5%20ORD-013745%20CTD%20Chemical-gene%20interactions.xlsx',\n",
       " '../data/500_train/portfoliobyloanstatus copy.xls',\n",
       " '../data/500_train/sara_shackleton_000_1_2.pst.123.xls',\n",
       " '../data/500_train/BG%20by%20InletOutlet.xlsx',\n",
       " '../data/500_train/DPK%20full%20study%20HUB%20file.xls',\n",
       " '../data/500_train/Adapting_Urban_BMPs_datasources1_508.xlsx',\n",
       " '../data/500_train/pzdatadictionary.xlsx',\n",
       " '../data/500_train/lisa_gang_000_1_1.pst.292.xls',\n",
       " '../data/500_train/Figure-data_Fahey.xlsx',\n",
       " '../data/500_train/Hataway%20definitions.xls',\n",
       " '../data/500_train/WallaceMichelle_A-6hdz_Data_20170508.xlsx',\n",
       " '../data/500_train/GTO_data_Deposit%20Caulobacter%20Biofilms%2007302018.xlsx',\n",
       " '../data/500_train/Fig2_Nmax.xlsx',\n",
       " '../data/500_train/calendaryear_annual_data_20240409.xlsx',\n",
       " '../data/500_train/darron_c_giron_002_1_1_1.pst.149.xls',\n",
       " '../data/500_train/Data%20for%20Table%20S7.xlsx',\n",
       " '../data/500_train/phthalate_intakes.xlsx',\n",
       " '../data/500_train/PFOA-17%20data%20for%20scihub.xls',\n",
       " '../data/500_train/james_steffes_000_1_1.pst.212.xls']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.saffuutil import get_saffu_tensors\n",
    "from utils.selfutil import get_fileList\n",
    "\n",
    "# Files\n",
    "train_files, _ = get_fileList(\"../data/500_train/\")\n",
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import parseutil, saffuutil\n",
    "\n",
    "importlib.reload(saffuutil)\n",
    "importlib.reload(parseutil)\n",
    "\n",
    "from utils.saffuutil import get_saffu_tensors\n",
    "\n",
    "\n",
    "class LoaderSAFFU(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_paths: List[str],\n",
    "        tokenizer,\n",
    "        max_rows: int = 100,\n",
    "        max_cols: int = 100,\n",
    "        pad_length: int = 32,\n",
    "    ):\n",
    "        self.max_rows = max_rows\n",
    "        self.max_cols = max_cols\n",
    "        self.pad_length = pad_length\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.x_tok = []\n",
    "        self.x_masks = []\n",
    "        self.y_tok = []\n",
    "        self.file_paths = []\n",
    "        self.failed_files = []\n",
    "\n",
    "        for file_path in tqdm(file_paths, desc=\"Processing files\"):\n",
    "            x_tok, x_masks, y_tok = self.featurize(file_path)\n",
    "            if x_tok is not None:\n",
    "                self.x_tok.append(x_tok)\n",
    "                self.x_masks.append(x_masks)\n",
    "                self.y_tok.append(y_tok)\n",
    "                self.file_paths.append(file_path)\n",
    "            else:\n",
    "                self.failed_files.append(file_path)\n",
    "\n",
    "        print(\n",
    "            f\"\\n{len(self.file_paths) + len(self.failed_files)}(P) = {len(self.file_paths)}(G) + {len(self.failed_files)}(E)\"\n",
    "        )\n",
    "\n",
    "    def featurize(self, file_path):\n",
    "        \"\"\"Featurize a single spreadsheet file into tensors. Return None on failure.\"\"\"\n",
    "        try:\n",
    "            return get_saffu_tensors(\n",
    "                file_path,\n",
    "                max_rows=self.max_rows,\n",
    "                max_cols=self.max_cols,\n",
    "                pad_length=self.pad_length,\n",
    "                tokenizer=self.tokenizer,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Featurization failed for {file_path}: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_tok)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            \"x_tok\": self.x_tok[index],\n",
    "            \"x_masks\": self.x_masks[index],\n",
    "            \"y_tok\": self.y_tok[index],\n",
    "            \"file_paths\": self.file_paths[index],\n",
    "        }\n",
    "\n",
    "    def get_imbalance(self) -> float:\n",
    "        # Get bold cell count\n",
    "        bold_count = sum((tensor[:, :, 6] == 1).sum().item() for tensor in self.y_tok)\n",
    "\n",
    "        # Raise error if no bold cells\n",
    "        if not bold_count:\n",
    "            raise ValueError(\n",
    "                \"No bold cells found in the dataset. Cannot calculate imbalance ratio.\"\n",
    "            )\n",
    "\n",
    "        # Return ratio of non-bold to bold\n",
    "        return (\n",
    "            sum((tensor[:, :, 6] == 0).sum().item() for tensor in self.y_tok)\n",
    "            / bold_count\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 377/400 [04:04<00:22,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [04:27<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "400(P) = 400(G) + 0(E)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = LoaderSAFFU(\n",
    "    file_paths=train_files,\n",
    "    tokenizer=tokenizer,\n",
    "    max_rows=100,  # Optional, defaults to 100\n",
    "    max_cols=100,  # Optional, defaults to 100\n",
    "    pad_length=32,  # Optional, defaults to 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = [\n",
    "    (0, 0),  # A1: header with slashes (Val/1)\n",
    "    (1, 2),  # C2: \"test\" (plain text)\n",
    "    (6, 2),  # C7: scientific notation\n",
    "    (5, 0),  # A6: purple fill (styled number)\n",
    "    (7, 3),  # D8: numeric + formatting\n",
    "    (2, 4),  # E3: float (36.285)\n",
    "    (18, 4),\n",
    "    (17, 4),\n",
    "    (3, 5),  # F4: float with decimals (43.519)\n",
    "    (4, 6),  # G5: \"Test2\" (text)\n",
    "    (6, 6),  # G7: \"Test3\" (string next to number)\n",
    "    (13, 7),  # H14: orange cell with \"Hey\"\n",
    "    (14, 7),  # H15: red-bordered time cell\n",
    "    (21, 0),  # A22: DateTime (italic)\n",
    "    (24, 1),  # B25: DateTime with minutes\n",
    "    (27, 6),  # G28: \"al\" (short text)\n",
    "    (28, 7),  # H29: \"Click Here\" (string hyperlink)\n",
    "    (27, 7),  # H28: number as text?\n",
    "    (20, 6),  # G21: \"#NAME?\" (Excel error)\n",
    "    (21, 8),  # I22: float (numeric cell)\n",
    "    (14, 8),  # I15: styled float with currency\n",
    "    (25, 8),  # I26: styled number (footer-aligned)\n",
    "]\n",
    "\n",
    "\n",
    "def inspect_cell(x_tok, x_masks, y_tok, row, col, tokenizer):\n",
    "    token_ids = x_tok[row, col].tolist()\n",
    "    decoded = tokenizer.decode(token_ids)\n",
    "    mask = x_masks[row, col].tolist()\n",
    "    meta = y_tok[row, col].tolist()\n",
    "\n",
    "    print(f\"\\nüîπ Cell ({row}, {col})\")\n",
    "    print(f\"‚Üí Decoded Text     : {decoded}\")\n",
    "    print(f\"‚Üí Token IDs        : {token_ids}\")\n",
    "    print(f\"‚Üí Attention Mask   : {mask}\")\n",
    "    print(f\"‚Üí Metadata Vector  : {meta}\")\n",
    "\n",
    "\n",
    "# Loop through chosen positions\n",
    "for row, col in positions:\n",
    "    inspect_cell(x_tok, x_masks, y_tok, row, col, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"mps:0\")\n",
    "model = SAFFUDecoder(SAFFUEncoder(tokenizer)).to(device)\n",
    "stage = \"init\"\n",
    "reload = False\n",
    "if reload or (\n",
    "    not os.path.exists(f\"./models_to_test/{data_set}-{model_size}-{stage}.state\")\n",
    "):\n",
    "    save_model(model, data_set, model_size, stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the data set(s)\n",
    "data_set = \"babylm_10M\"  # babylm_10M+babylm_100M+BWB\n",
    "model_size = \"small\"  # \"medium\" # \"tiny\" # \"micro\" # \"big\" #\n",
    "training_sizes = {\n",
    "    \"babylm_10M\": 10,\n",
    "    \"babylm_100M\": 100,\n",
    "    \"BWB\": 1000,\n",
    "}  # in millions of word-tokens\n",
    "devsample = 10  # dev data is 1/10 of total available\n",
    "dataset_size = sum([training_sizes[data_subset] for data_subset in data_set.split(\"+\")])\n",
    "downsample = int(dataset_size / 5)  # roughly 5 million word-tokens per split\n",
    "eta = 2**-3\n",
    "texts, dtexts, ttexts = [], [], []\n",
    "docs, ddocs, tdocs = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3171fe4f8e4e48ddbb358158607740e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Initializing embedding matrix V:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 48\u001b[0m\n\u001b[1;32m     26\u001b[0m initial_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     27\u001b[0m     [\n\u001b[1;32m     28\u001b[0m         data_file[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     ]\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m initializer \u001b[38;5;241m=\u001b[39m SAFFUInitializer(\n\u001b[1;32m     37\u001b[0m     ignore_case,\n\u001b[1;32m     38\u001b[0m     ignore_space,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     icf\u001b[38;5;241m=\u001b[39micf,\n\u001b[1;32m     47\u001b[0m )\n\u001b[0;32m---> 48\u001b[0m eval_docs \u001b[38;5;241m=\u001b[39m \u001b[43minitializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mddocs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownsample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevsample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevsample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:837\u001b[0m, in \u001b[0;36minitialize\u001b[0;34m(self, model, docs, ddocs, downsample, seed, max_epochs, patience, devsample, model_file, reload, verbose, training_targets, development_targets, combined_loss, eval_data, augment, MaxU)\u001b[0m\n",
      "File \u001b[0;32m<string>:436\u001b[0m, in \u001b[0;36mset_embeddings\u001b[0;34m(self, model, splits, splits_dis, splits_targets, model_file, V_sum_1, p, tokens)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/vmenv/lib/python3.10/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vmenv/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vmenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/vmenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/vmenv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m<string>:158\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, si)\u001b[0m\n",
      "File \u001b[0;32m<string>:364\u001b[0m, in \u001b[0;36mencode_split\u001b[0;34m(self, split, split_dis, split_targets, verbose, terminate, bs)\u001b[0m\n",
      "File \u001b[0;32m<string>:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "File \u001b[0;32m<string>:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "model.encoder._V.weight.requires_grad = False\n",
    "\n",
    "seed = 691\n",
    "ignore_space = False\n",
    "ignore_case = False\n",
    "initialize = True\n",
    "verbose = True\n",
    "co_vecs = 1 * (2**0 + 0.99999)\n",
    "identity_ratio = 2 ** (-1)\n",
    "icf = True\n",
    "log_label = False\n",
    "nlabels = 1 * (2**0)\n",
    "centroids = False\n",
    "label_iterations = 1 * (2**7)  # None\n",
    "\n",
    "max_epochs = 2**-1\n",
    "patience = 0 * (2**0)\n",
    "reload = False\n",
    "\n",
    "# Custom\n",
    "devstr = \"mps\"\n",
    "docs = tok_train_data\n",
    "downsample = 10\n",
    "ddocs = []\n",
    "\n",
    "initial_file = \"\".join(\n",
    "    [\n",
    "        data_file[:-5] + \"-\",\n",
    "        f\"b_{tokenizer.config._bits}-hb_{tokenizer.config._hidden}-\",\n",
    "        f\"we_{int(tokenizer.config._wave_encode)}-oa_{tokenizer.config._o_agg}-ra_{tokenizer.config._r_agg}-ba_{tokenizer.config._b_agg}-\",\n",
    "        f\"mr_{int(tokenizer.config._mask_r)}-mb_{int(tokenizer.config._mask_b)}-md_{tokenizer.config._model_documents}-\",\n",
    "        f\"is_{int(ignore_space)}-ic_{int(ignore_case)}-ws_{int(initialize)}-wv_{int(co_vecs)}-ds_{downsample}-seed_{seed}\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "initializer = SAFFUInitializer(\n",
    "    ignore_case,\n",
    "    ignore_space,\n",
    "    devstr,\n",
    "    co_vecs,\n",
    "    identity_ratio=identity_ratio,\n",
    "    label_iterations=label_iterations,\n",
    "    log_label=log_label,\n",
    "    nlabels=nlabels,\n",
    "    centroids=centroids,\n",
    "    icf=icf,\n",
    ")\n",
    "eval_docs = initializer.initialize(\n",
    "    model,\n",
    "    docs,\n",
    "    ddocs,\n",
    "    downsample,\n",
    "    seed,\n",
    "    max_epochs,\n",
    "    patience,\n",
    "    devsample=devsample,\n",
    "    model_file=initial_file,\n",
    "    reload=reload,\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
