{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import importlib to reload modules and sys and os to add the path for other imports\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Append the parent directory to the path to import the necessary modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# Import the utilities and the dataloader\n",
    "from utils import saffuutil\n",
    "\n",
    "# Now reload the modules to ensure they are up-to-date\n",
    "importlib.reload(saffuutil)\n",
    "\n",
    "# Import the model and funcs required from utils\n",
    "from utils.saffuutil import get_saffutok_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAFFU Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Prof Adapted\n",
    "\n",
    "This chunk signals the code in this section is custom and before trying to adapt/emulate professor code in saffu folder pulled from professor's repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing files: 100%|██████████| 400/400 [00:12<00:00, 32.81file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "Files/Tokens: 400/1276661\n"
     ]
    }
   ],
   "source": [
    "# Set the directory for the tokenizer data and call func\n",
    "tok_dir = \"../data/500_train/\"\n",
    "tok_train_data = get_saffutok_data(dir_path=tok_dir, threads=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region = \"Abandoned custom import, workout with prof\"\n",
    "# # Imports that are diff since we import only the func needed\n",
    "# from saffu import utilities_saffu\n",
    "# importlib.reload(utilities_saffu)\n",
    "# # For tokenizer setup - [5] ipynb\n",
    "# from saffu.utilities_saffu import get_config\n",
    "# endregion\n",
    "\n",
    "# Execute the necessary scripts to set up the environment\n",
    "exec(open(\"../src/configuration_saffu.py\").read())\n",
    "exec(open(\"../src/tokenization_saffu.py\").read())\n",
    "exec(open(\"../src/utilities_saffu.py\").read())\n",
    "exec(open(\"../src/data_saffu.py\").read())\n",
    "exec(open(\"../src/modeling_saffu.py\").read())\n",
    "exec(open(\"../src/training_saffu.py\").read())\n",
    "exec(open(\"../src/inference_saffu.py\").read())\n",
    "exec(open(\"../src/initialization_saffu.py\").read())\n",
    "exec(open(\"../src/load_data.py\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of model sizes applicable for get_config\n",
    "model_sizes = [\"micro\", \"tiny\", \"small\", \"medium\", \"big\"]\n",
    "\n",
    "# Custom name the DS\n",
    "data_set = \"500_train\"\n",
    "\n",
    "# Define the dict to store tokenizer cache\n",
    "tokenizer_directory = \"./cache/\"\n",
    "\n",
    "# Pick a model name from the list\n",
    "model_size = model_sizes[0]\n",
    "\n",
    "# Get the config for the current model size\n",
    "config = get_config(model_size=model_size)\n",
    "\n",
    "# Set the tokenizer name and create the tokenizer\n",
    "tokenizer_name = f\"{data_set}-{model_size}\"\n",
    "tokenizer = SAFFUTokenizer(config)\n",
    "\n",
    "# Set the vocab file path/name\n",
    "vocab_file = os.path.join(\n",
    "    tokenizer_directory,\n",
    "    tokenizer._model_path,\n",
    "    (tokenizer_name + \"-\" if tokenizer_name else \"\") + \"vocab.json\",\n",
    ")\n",
    "\n",
    "# Flag to reload the tokenizer or not\n",
    "reload = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Tokenizer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer: 500_train-micro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-tokenizing 1276661 documents: 100%|██████████| 1276661/1276661 [00:15<00:00, 81312.28it/s]\n",
      "Counting token frequencies: 100%|██████████| 1276661/1276661 [00:00<00:00, 5625587.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training bpe tokenizer\n",
      "\n",
      "numbers of samples, pre-tokens, and target bpe pieces for covering of pre-tokens:  1276661 3053249 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing: 100%|██████████| 1048576/1048576 [00:22<00:00, 47047.15it/s]\n",
      "Fitting:  91%|█████████ | 1868/2048 [00:43<00:04, 43.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built a vocabulary of 2048 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building sub-token reference dictionary: 100%|██████████| 1048576/1048576 [01:36<00:00, 10892.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portion of model's 1048576 reference tokens covered: 0.9997978210449219\n",
      "Portion of model's 1048576 reference tokens covered: 0.9997978210449219\n",
      "Vocabulary size for experiment:  4035\n"
     ]
    }
   ],
   "source": [
    "# If reload = False and vocab file exists, load the tokenizer\n",
    "if not reload and os.path.exists(vocab_file):\n",
    "\n",
    "    ## Progress message\n",
    "    print(f\"Loading tokenizer: {tokenizer_name}\")\n",
    "\n",
    "    # Load the tokenizer\n",
    "    result = tokenizer.load(tokenizer_name, load_directory=tokenizer_directory)\n",
    "\n",
    "# Else if either reloading or vocab file does not exist, train the tokenizer\n",
    "else:\n",
    "\n",
    "    ## Progress message\n",
    "    print(f\"Training tokenizer: {tokenizer_name}\")\n",
    "\n",
    "    # Do the pretokenization\n",
    "    pretokenized_data = tokenizer.pretokenize_documents(tok_train_data)\n",
    "\n",
    "    # Train the tokenizer with the pretokenized data\n",
    "    tokenizer.train(pretokenized_data)\n",
    "\n",
    "    # Save the tokenizer vocabulary in the specified directory\n",
    "    tokenizer.save_vocabulary(tokenizer_name, save_directory=tokenizer_directory)\n",
    "\n",
    "# Set the vocabulary for the tokenizer finally\n",
    "tokenizer.set_vocabulary()\n",
    "\n",
    "# Print the vocabulary size for the experiment\n",
    "print(\"Vocabulary size for experiment: \", len(tokenizer._vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size for experiment:  4035\n",
      "['Th', 'es', 'e', ' c', 'as', 's', 'er', 'ol', 'es', ' d', 'is', 'gu', 'st', ' K', 'ay', 'la', '.']\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size for experiment: \", len(tokenizer._vocabulary))\n",
    "\n",
    "data_file = os.path.join(\n",
    "    tokenizer_directory,\n",
    "    tokenizer._model_path,\n",
    "    (tokenizer_name + \"-\" if tokenizer_name else \"\")\n",
    "    + f\"data-space_{tokenizer.config._space}-r_{tokenizer.config._r}-b_{tokenizer.config._b}-heads_{tokenizer.config._heads}-N_{tokenizer.config._N}.json\",\n",
    ")\n",
    "\n",
    "# Tokenize a sentence\n",
    "print(tokenizer._tokenize(\"These casseroles disgust Kayla.\"))\n",
    "\n",
    "# Check if tokenized stuff is in the vocabulary\n",
    "print(\n",
    "    [\n",
    "        x in tokenizer._vocabulary\n",
    "        for x in tokenizer._tokenize(\"These casseroles disgust Kayla.\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[872, 180, 94, 300, 263, 90, 88, 207, 180, 1167, 733, 124, 1146, 171, 1390, 1787, 335, 287, 1438, 312, 426, 930, 206, 0]\n",
      "Th es e  c as s er ol es  pa ag al  ha i  k ya  d is gu st  K ay la .\n"
     ]
    }
   ],
   "source": [
    "sentence = \"These casseroles paagal hai kya disgust Kayla.\"\n",
    "checl = tokenizer.encode(sentence)\n",
    "print(checl)\n",
    "print(tokenizer.decode(checl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[872, 180, 94, 1167, 733, 124, 1146, 171, 1390, 1787, 335, 287, 1438, 312, 426, 930, 206, 0]\n"
     ]
    }
   ],
   "source": [
    "yes = tokenizer.preprocess(\"These paagal hai kya disgust Kayla.\")\n",
    "print(yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
