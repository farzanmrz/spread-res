{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d7e7cf3",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Imports and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68731c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "# Read and execute saffu files for using functionality\n",
    "exec(open(\"../saffu/configuration_saffu.py\").read())\n",
    "exec(open(\"../saffu/tokenization_saffu.py\").read())\n",
    "exec(open(\"../saffu/utilities_saffu.py\").read())\n",
    "exec(open(\"../saffu/data_saffu.py\").read())\n",
    "exec(open(\"../saffu/modeling_saffu.py\").read())\n",
    "exec(open(\"../saffu/training_saffu.py\").read())\n",
    "exec(open(\"../saffu/inference_saffu.py\").read())\n",
    "exec(open(\"../saffu/tuning_saffu.py\").read())\n",
    "exec(open(\"../saffu/load_data.py\").read())\n",
    "\n",
    "## Set environment variables\n",
    "# Creates logger object named __main__ for debug messages\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "# Doesn't split memory chunks of more than 256 MB\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256\"\n",
    "\n",
    "# Makes code synchronous meaning GPU finishes running then CPU rund\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# Enable dynamic shape allocation of tensor sizes without predefining them\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "# Set the gpu or cpu device\n",
    "devstr = \"cuda:2\" # \"cpu\" \n",
    "gpu = False if (devstr == 'cpu') else True\n",
    "device = 'cpu' if (devstr == 'cpu') else (torch.device(devstr if torch.cuda.is_available() else 'cpu') \n",
    "                                          if devstr else torch.cuda.current_device())\n",
    "# Observe the device\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ec2e94",
   "metadata": {},
   "source": [
    "## Dataset setup\n",
    "\n",
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "240aed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset being used, can also combine different ones with a +\n",
    "data_set = \"helpful-base\" # +harmless-base+babylm_10M+babylm_100M+BWB\n",
    "\n",
    "# Define model size from tiny, micro, small, medium, big\n",
    "model_size = \"tiny\"\n",
    "\n",
    "# Define training size in millions of word-tokens so helpful-base = 5 = 5 million tokens\n",
    "training_sizes = {\n",
    "    \"helpful-base\": 5, \"harmless-base\": 5, \"babylm_10M\": 10, \"babylm_100M\": 100, \"BWB\": 1000\n",
    "}\n",
    "\n",
    "# Define the % of data held out for development so 1/10 of total available below\n",
    "devsample = 10 \n",
    "\n",
    "# Total size of all datasets in millions, currently 5 million should be\n",
    "dataset_size = sum([training_sizes[data_subset] for data_subset in data_set.split(\"+\")])\n",
    "\n",
    "# Get downsample size which would be 1 = 1 million below\n",
    "downsample = int(dataset_size/5) # roughly 5 million word-tokens per split\n",
    "\n",
    "# Hyperparameter for learning rate probably\n",
    "eta = 0.05 # 0.05\n",
    "\n",
    "# Empty lists to store document or conversation level data for normal, dev and test\n",
    "docs, ddocs, tdocs = [], [], []\n",
    "convos, dconvos, tconvos = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2e3fe6",
   "metadata": {},
   "source": [
    "### Loading conversations for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55f12d3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of training and development threads:  27281 2280\n"
     ]
    }
   ],
   "source": [
    "########################## BASE ##############################################\n",
    "# Contains human assistant interactions\n",
    "if (\"helpful-base\" in data_set) or (\"harmless-base\" in data_set):\n",
    "    train_conversations = []; dev_conversations = []; test_conversations = []\n",
    "    \n",
    "    # Load one of the helpful subsets\n",
    "    if \"helpful-base\" in data_set:\n",
    "        \n",
    "        # Load the human-assistant training examples split by chosen and rejected\n",
    "        train_conversations += load_hh_rlhf(\"/cephfs/data/hh_rlhf/backup/helpful_train.json\")        \n",
    "        dev_conversations += load_hh_rlhf(\"/cephfs/data/hh_rlhf/backup/helpful_dev.json\")\n",
    "        # test_conversations += load_hh_rlhf(\"/cephfs/data/hh_rlhf/backup/helpful_test.json\")\n",
    "    \n",
    "    # Load one of the harmless subsets\n",
    "    if \"harmless-base\" in data_set:\n",
    "        train_conversations += load_hh_rlhf(\"/cephfs/data/hh_rlhf/backup/harmless_train.json\")\n",
    "        dev_conversations += load_hh_rlhf(\"/cephfs/data/hh_rlhf/backup/harmless_dev.json\")\n",
    "        # test_conversations += load_hh_rlhf(\"/cephfs/data/hh_rlhf/backup/harmless_test.json\")\n",
    "    \n",
    "    # Define variables to store train, dev and test convos\n",
    "    convos, dconvos, test_threads = defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "    \n",
    "    # Loop through all conversations in training\n",
    "    for conversation in train_conversations:\n",
    "        \n",
    "        # Choose the first interaction as the key to access rest of conversation from a start interaction\n",
    "        initial_thread = \"\".join([turn[0] + turn[1] for turn in conversation['chosen'][:2]])\n",
    "        \n",
    "        # Store the full conversation including the initial interaction in key in a tuple\n",
    "        # First element of tuple is the total no. of statements by humans+assistant in the full convo\n",
    "        # Second element is a list of lists where each individual list consists of a statement\n",
    "        convos[initial_thread].append((len(conversation['chosen']), conversation['chosen']))\n",
    "\n",
    "    # Loop through all conversations in dev\n",
    "    for conversation in dev_conversations:\n",
    "        initial_thread = \"\".join([turn[0] + turn[1] for turn in conversation['chosen'][:2]])\n",
    "        dconvos[initial_thread].append((len(conversation['chosen']), conversation['chosen']))\n",
    "    \n",
    "    # First sort by descending length of convo for each key, longest convo first\n",
    "    # It then returns the longest convo for each key ensuring we keep the longest interaction\n",
    "    convos = [sorted(convos[initial_thread], reverse = True)[0][1] for initial_thread in convos]\n",
    "    dconvos = [sorted(dconvos[initial_thread], reverse = True)[0][1] for initial_thread in dconvos]\n",
    "    \n",
    "    # Variable to store text snippets from train conversations along with rejected responses\n",
    "    docs = []\n",
    "    for conversation in train_conversations:\n",
    "        \n",
    "        # 1st part grabs the last 2 sentences from the chosen interaction\n",
    "        # 2nd part adds the assistant's rejected response to it for better training\n",
    "        docs += [x[1] for x in conversation['chosen'][-2:]] + [conversation['rejected'][-1][1]]\n",
    "    \n",
    "    # Prints the final number of threads being used for training and development\n",
    "    print(\"Numbers of training and development threads: \", len(convos), len(dconvos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "204651ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## BABYLM ##############################################\n",
    "# Contains longer textual passages which might be narrative driven\n",
    "# Here docs contains textual segments which might be important for language modelling\n",
    "if (\"babylm_10M\" in data_set) or (\"babylm_100M\" in data_set):\n",
    "    \n",
    "    # If 10M dataset\n",
    "    if (\"babylm_10M\" in data_set):\n",
    "        \n",
    "        # List all the files in directory and check if they have the training extesion\n",
    "        for fname in tqdm(os.listdir(\"/cephfs/data/babylm_data/babylm_10M/\"), desc = \"Loading 10M training tokens\"):\n",
    "            if \".train\" == fname[-6:]:\n",
    "                \n",
    "                # Load the docs and convos for this dataset\n",
    "                docs, convos = load_BBLM(\"/cephfs/data/babylm_data/babylm_10M/\", fname, docs, convos)\n",
    "\n",
    "    # If 100M dataset\n",
    "    if (\"babylm_100M\" in data_set):\n",
    "        for fname in tqdm(os.listdir(\"/cephfs/data/babylm_data/babylm_100M/\"), desc = \"Loading 100M training tokens\"):\n",
    "            if \".train\" == fname[-6:]:\n",
    "                docs, convos = load_BBLM(\"/cephfs/data/babylm_data/babylm_100M/\", fname, docs, convos)\n",
    "    \n",
    "    # Load all the dev docs and convos regardless\n",
    "    for fname in tqdm(os.listdir(\"/cephfs/data/babylm_data/babylm_dev/\"), desc = \"Loading development tokens\"):\n",
    "        if \".dev\" == fname[-4:]:\n",
    "            ddocs, dconvos = load_BBLM(\"/cephfs/data/babylm_data/babylm_dev/\", fname, ddocs, dconvos)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0caa882",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## BWB ##############################################\n",
    "# Perhaps textual data from books, articles, or informational documents.\n",
    "if \"BWB\" in data_set:\n",
    "    bwb_train_docs, bwb_test_docs = load_bwb(\"/cephfs/data/bwb/datasets/\")\n",
    "    docs += bwb_train_docs\n",
    "    # dconvos += [[[\"Human: \", x + \"\\n\\n\"]] for x in tqdm(bwb_test_docs, desc = \"Loading BWB testing texts\") if x]\n",
    "    convos += [[[\"Human: \", x + \"\\n\\n\"]] for x in tqdm(bwb_train_docs, desc = \"Loading BWB training texts\") if x]\n",
    "    del bwb_train_docs, bwb_test_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef4e2460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Okay. What else is needed to play, and what are the rules?\\n\\n', 'A horseshoe is usually made out of metal and is about 3 to 3.5 inches long and around 1 inch thick. The horseshoe should also have a 2 inch by 3 inch flat at the bottom where the rubber meets the metal. We also need two stakes and six horseshoes.', 'Horseshoes are either metal or plastic discs. The horseshoes come in different weights, and the lighter ones are easier to throw, so they are often the standard for beginning players.', 'any other ideas? they are fidgeting\\n\\n', 'Kids are great at fidgeting, so let’s make sure we give them permission to fidget.', 'Yeah, definitely. What works for you personally? What works for other people?', 'Thanks. And what are some typical forms required?\\n\\n', 'At the marina level, there’s usually the rental contract that covers the slip, and the paperwork to get insurance for the boat. You’ll also need to register your boat with the state, usually by mail, but they’ll also likely be able to help you if you go to the state office in person. At the yacht club level, you’ll usually need', 'Oh, I’m afraid I’m not very familiar with these processes myself. If you want, I can look it up for you, or you can ask me to do so when you need the information.', \"How can I store food if I don't have a pantry?\\n\\n\", 'You could store the food in a refrigerator, the top cupboards in your kitchen, the freezer, or even in a hole in the ground.', \"By storing food in your refrigerator or freezer.  You have access to a large volume of your home's space in these places.\", 'That sounds cool\\n\\n', 'I’m a programmer, so I’m really into good programming texts, like “The Practice of Programming” or “JavaScript: The Good Parts.” But that might be a bit too much for a 9 year old, so', 'Do you want some quotes?', 'Then why do they sell it?\\n\\n', 'Automotive manufacturers sell the high-octane fuel for you to use to make the engine run better. It’s a trade-off. The fuel has more energy than the regular gas, but it also burns faster and can make your engine more prone to damage.', 'There are various reasons.  The gasoline may not be entirely identical, and if you have a few items that are very sensitive to fuel, you may need it for your car.  However, most items should be OK with regular gas, and probably wouldn’t be damaged by it either.', \"No, that's it.  Thanks!\\n\\n\", 'You’re welcome.  I’m always here to help.']\n"
     ]
    }
   ],
   "source": [
    "print(docs[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e648e6",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Define Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29bc2010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the configuration params for current model medium\n",
    "config = get_config(model_size = model_size)\n",
    "\n",
    "# Name the current tokenizer combo of dataset+model names\n",
    "tokenizer_name = f\"{data_set}-{model_size}\" # helpful-base-medium\n",
    "\n",
    "# Create the tokenizer object inherited from HF PreTrainedTokenizer class therefore init params not in custom\n",
    "tokenizer = SAFFUTokenizer(config)\n",
    "\n",
    "# Determine the directory where you wanna retreive tokenizer from\n",
    "tokenizer_directory = \"../../code/cache/\"\n",
    "\n",
    "# Determine the directory where you wanna store tokenizer\n",
    "save_directory = './cache/'\n",
    "\n",
    "# Form the vocab file with a of directory, model path in tokenization_saffu.py, and name if given\n",
    "vocab_file = os.path.join(tokenizer_directory, tokenizer._model_path,\n",
    "                          (tokenizer_name + \"-\" if tokenizer_name else \"\") + \"vocab.json\")\n",
    "\n",
    "# True if retraining the tokenizer, False to load an existing one available\n",
    "reload = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f5b48",
   "metadata": {},
   "source": [
    "### Preload existing or train new vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d35385b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer: helpful-base-tiny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-tokenizing 131505 documents: 100%|█| 131505/131505 [00:07<00:00, 18719.00it/\n",
      "Counting token frequencies: 100%|███| 131505/131505 [00:00<00:00, 604560.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training bpe tokenizer\n",
      "\n",
      "numbers of samples, pre-tokens, and target bpe pieces for covering of pre-tokens:  131505 4667373 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing: 100%|████████████████████| 87646/87646 [00:02<00:00, 35516.34it/s]\n",
      "Fitting:  88%|█████████████████████████▌   | 3609/4096 [00:30<00:04, 119.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built a vocabulary of 4096 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building sub-token reference dictionary: 100%|█| 87646/87646 [00:14<00:00, 6184.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portion of model's 87646 reference tokens covered: 0.9995664377153549\n",
      "Portion of model's 87646 reference tokens covered: 0.9995664377153549\n"
     ]
    }
   ],
   "source": [
    "# If we are not reloading and the vocab_file path exists then\n",
    "if not reload and os.path.exists(vocab_file):\n",
    "    \n",
    "    # Print message for loading tokenizer\n",
    "    print(f\"Loading tokenizer: {tokenizer_name}\\n\")\n",
    "    \n",
    "    # Store the loaded tokenizer from the directory into result\n",
    "    result = tokenizer.load(tokenizer_name, load_directory = tokenizer_directory)\n",
    "\n",
    "# If we are either reloading or the vocab_file path doesn't exist then    \n",
    "else:\n",
    "    \n",
    "    # Print the training message\n",
    "    print(f\"Training tokenizer: {tokenizer_name}\")\n",
    "    \n",
    "    # Train our tokenizer\n",
    "    tokenizer.train(tokenizer.pretokenize_documents(docs))\n",
    "    \n",
    "    # Save the vocabulary in the the directory specified\n",
    "    tokenizer.save_vocabulary(tokenizer_name, save_directory = tokenizer_directory)\n",
    "\n",
    "# Set the vocabulary breaking the words into subwords using BPE\n",
    "tokenizer.set_vocabulary() # Prints message showing % of original tokens represented by new vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ecd3d6",
   "metadata": {},
   "source": [
    "### Print metrics and check tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be1dada5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size for experiment:  6387\n",
      "0: <pad>\n",
      "1: <oov>\n",
      "2: <sod>\n",
      "3: <eod>\n",
      "4: <frg>\n",
      "5: ,\n",
      "6:  the\n",
      "7:  you\n",
      "8:  to\n",
      "9: \n",
      "\n",
      "['The', 'se', ' ca', 'ss', 'er', 'ol', 'es', ' dis', 'gu', 'st', ' K', 'ay', 'la', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print new vocab size for this experiment after BPE\n",
    "print(\"Vocabulary size for experiment: \", len(tokenizer._vocabulary))\n",
    "\n",
    "# Print the first 10 tokens in vocabulary\n",
    "print(\"\\n\".join(f\"{idx}: {token}\" for i, (token, idx) in enumerate(tokenizer._vocabulary.items()) if i < 10))\n",
    "\n",
    "# Augment the vocabulary with Human:, Assistant: prompts which have been BPE'd increasing vocab size by 2\n",
    "if (\"Assistant: \" not in tokenizer._vocabulary) and (\"Human: \" not in tokenizer._vocabulary):\n",
    "    tokenizer.augment_vocabulary([\"Assistant: \", \"Human: \"])\n",
    "\n",
    "# Name the data file storing metadata possibly regarding this configuration\n",
    "data_file = os.path.join(tokenizer_directory, tokenizer._model_path,\n",
    "                         (tokenizer_name + \"-\" if tokenizer_name else \"\") + \n",
    "                         f\"data-space_{tokenizer.config._space}-r_{tokenizer.config._r}-b_{tokenizer.config._b}-heads_{tokenizer.config._heads}-N_{tokenizer.config._N}.json\")\n",
    "\n",
    "# Check the tokenize function with example sentence\n",
    "print(tokenizer._tokenize(\"These casseroles disgust Kayla.\"))\n",
    "\n",
    "# Check if vocabulary has BPE tokens after tokenize\n",
    "[x in tokenizer._vocabulary for x in tokenizer._tokenize(\"These casseroles disgust Kayla.\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c738b81",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e30723e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAFFUDecoder(\n",
       "  (encoder): SAFFUEncoder(\n",
       "    (logsoft): LogSoftmax(dim=0)\n",
       "    (_V): Embedding(25252, 512)\n",
       "    (BS): ModuleList(\n",
       "      (0): SAFFULayer(\n",
       "        (activate): LogSoftmax(dim=0)\n",
       "        (logsoft): LogSoftmax(dim=0)\n",
       "        (_W): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (_U): Linear(in_features=512, out_features=256, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (RS): ModuleList(\n",
       "      (0): SAFFULayer(\n",
       "        (activate): LogSoftmax(dim=0)\n",
       "        (logsoft): LogSoftmax(dim=0)\n",
       "        (_W): Linear(in_features=8, out_features=8, bias=False)\n",
       "        (_U): Linear(in_features=4096, out_features=256, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (_D): Linear(in_features=512, out_features=128, bias=False)\n",
       "  )\n",
       "  (_Uc): Linear(in_features=640, out_features=25252, bias=False)\n",
       "  (_Ud): Linear(in_features=128, out_features=101009, bias=False)\n",
       "  (logsoft): LogSoftmax(dim=0)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the modeling file to read classes from there and use funcs\n",
    "exec(open(\"../saffu/modeling_saffu.py\").read())\n",
    "\n",
    "# Define the model as an object of the decoder class which takes encoder object in constructor and cast to device\n",
    "model = SAFFUDecoder(config, SAFFUEncoder(config, tokenizer)).to(device)\n",
    "\n",
    "# Defint the stage of the model and whether we are reloading or creating new\n",
    "stage = \"init\"; reload = False\n",
    "\n",
    "# If reloading or the path doesn't exist then save this model\n",
    "if reload or (not os.path.exists(f\"../../code/models_to_test/{data_set}-{model_size}-{stage}.state\")):\n",
    "    save_model(model, data_set, model_size, stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cec776b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
