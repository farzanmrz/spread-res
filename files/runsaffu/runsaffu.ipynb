{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Notebook formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.get_cells().map(function(c) {\n",
    "    if (c.cell_type === 'code') {\n",
    "        c.code_mirror.setOption('lineWrapping', true);\n",
    "    }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import importlib to reload modules and sys and os to add the path for other imports\n",
    "import importlib\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Append the parent directory to the path to import the necessary modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import the utilities and the dataloader\n",
    "from utils import selfutil, saffuutil\n",
    "from classes import TestRNN\n",
    "\n",
    "# Now reload the modules to ensure they are up-to-date\n",
    "importlib.reload(selfutil)\n",
    "importlib.reload(saffuutil)\n",
    "importlib.reload(TestRNN)\n",
    "\n",
    "# Import the funcs needed from utils\n",
    "from utils.saffuutil import load_saffutok, dir2convos, get_saffuloader\n",
    "\n",
    "# Import the SAFFUDataLoader class\n",
    "from classes.TestRNN import TestRNN\n",
    "\n",
    "# Other regular imports\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Tokenizer \n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and execute saffu files for using functionality\n",
    "exec(open(\"../saffu/configuration_saffu.py\").read())\n",
    "exec(open(\"../saffu/tokenization_saffu.py\").read())\n",
    "exec(open(\"../saffu/utilities_saffu.py\").read())\n",
    "exec(open(\"../saffu/data_saffu.py\").read())\n",
    "exec(open(\"../saffu/modeling_saffu.py\").read())\n",
    "exec(open(\"../saffu/training_saffu.py\").read())\n",
    "exec(open(\"../saffu/inference_saffu.py\").read())\n",
    "exec(open(\"../saffu/tuning_saffu.py\").read())\n",
    "exec(open(\"../saffu/load_data.py\").read())\n",
    "\n",
    "## Set environment variables\n",
    "# Creates logger object named __main__ for debug messages\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "# Doesn't split memory chunks of more than 256 MB\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256\"\n",
    "\n",
    "# Makes code synchronous meaning GPU finishes running then CPU rund\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# Enable dynamic shape allocation of tensor sizes without predefining them\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "# Set the gpu or cpu device\n",
    "devstr = \"cuda:1\" # \"cpu\" \n",
    "gpu = False if (devstr == 'cpu') else True\n",
    "device = 'cpu' if (devstr == 'cpu') else (torch.device(devstr if torch.cuda.is_available() else 'cpu') \n",
    "                                          if devstr else torch.cuda.current_device())\n",
    "# Observe the device\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset being used, can also combine different ones with a +\n",
    "data_set = \"train\" # +harmless-base+babylm_10M+babylm_100M+BWB\n",
    "\n",
    "# Define model size from tiny, micro, small, medium, big\n",
    "model_size = \"tiny\"\n",
    "\n",
    "# Size of different datasets in millions of word tokens\n",
    "training_sizes = {\"helpful-base\": 5, \"harmless-base\": 5, \"babylm_10M\": 10, \"babylm_100M\": 100, \"BWB\": 1000,\n",
    "                 \"train\":2.1877} \n",
    "\n",
    "# Define the % of data held out for development so 1/10 of total available below\n",
    "devsample = 10 \n",
    "\n",
    "# Total size of all datasets in millions, currently 2.1877 million should be\n",
    "dataset_size =  sum([training_sizes[data_subset] for data_subset in data_set.split(\"+\")])\n",
    "\n",
    "# Get downsample size which would be 1 = 1 million below\n",
    "downsample = max(int(dataset_size / 5), 1) # roughly 5 million word-tokens per split\n",
    "\n",
    "# Hyperparameter for learning rate probably\n",
    "eta = 0.05 # 0.05\n",
    "\n",
    "# Empty lists to store document or conversation level data for normal, dev and test\n",
    "docs, ddocs, tdocs = [], [], []\n",
    "convos, dconvos, tconvos = [], [], []\n",
    "\n",
    "# Get the configuration params for current model medium\n",
    "config = get_config(model_size = model_size)\n",
    "\n",
    "# Name the current tokenizer combo of dataset+model names\n",
    "tokenizer_name = f\"{data_set}-{model_size}\" # helpful-base-medium\n",
    "\n",
    "# Create the tokenizer object inherited from HF PreTrainedTokenizer class therefore init params not in custom\n",
    "tokenizer = SAFFUTokenizer(config)\n",
    "\n",
    "# Determine the directory where you wanna retreive tokenizer from\n",
    "tokenizer_directory = \"./cache/\"\n",
    "\n",
    "# Determine the directory where you wanna store tokenizer\n",
    "save_directory = './cache/'\n",
    "\n",
    "# Form the vocab file with a of directory, model path in tokenization_saffu.py, and name if given\n",
    "vocab_file = os.path.join(tokenizer_directory, tokenizer._model_path,\n",
    "                          (tokenizer_name + \"-\" if tokenizer_name else \"\") + \"vocab.json\")\n",
    "\n",
    "# True if retraining the tokenizer, False to load an existing one available\n",
    "reload = False\n",
    "\n",
    "# Now call load func to setup tokenizer\n",
    "load_saffutok(reload, vocab_file, tokenizer, tokenizer_name, tokenizer_directory, train_dir = '../data/train/')\n",
    "\n",
    "#tokenizer.augment_vocabulary([\"Cell\"])\n",
    "\n",
    "# Name the data_file path\n",
    "data_file = os.path.join(tokenizer_directory, tokenizer._model_path,\n",
    "                         (tokenizer_name + \"-\" if tokenizer_name else \"\") + \n",
    "                         f\"data-space_{tokenizer.config._space}-r_{tokenizer.config._r}-b_{tokenizer.config._b}-heads_{tokenizer.config._heads}-N_{tokenizer.config._N}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print new vocab size for this experiment after BPE\n",
    "print(\"Vocabulary size for experiment: \", len(tokenizer._vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer._tokenize(\"These casseroles disgust Kayla.\"))\n",
    "tokenizer._vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the tokenizer and create Encoder then use that to create decoder and full model\n",
    "model = SAFFUDecoder(config, SAFFUEncoder(config, tokenizer)).to(device)\n",
    "\n",
    "# Define the current stage of the model initial\n",
    "stage = \"init\"\n",
    "\n",
    "# Set to determine whether we are reloading or not\n",
    "reload = False\n",
    "\n",
    "# If in reload mode or the path doesnt exist for this dataset-model-stage combo then save_model\n",
    "if reload or (not os.path.exists(f\"./models_to_test/{data_set}-{model_size}-{stage}.state\")):\n",
    "    \n",
    "    \"\"\"\n",
    "    Saves the following information about current dataset-model_size-stage combo of the model\n",
    "    \n",
    "    1. state - Weights/params of the model which can be loaded later for resuming training or inference\n",
    "    2. losses - Training losses over epochs\n",
    "    3. counts - Frequency of words/subwords important for BPE\n",
    "    4. vocabulary - Mapping of words to indices to form vocab with keys as words and index as value\n",
    "    5. raw_td - Merge pairs dictating how subwords were combined to form new tokens, important for BPE\n",
    "    6. subtoken_reference - Maps text to its subwords, important for mapping output to og format\n",
    "    7. docsizes - Sizes of docs or number of tokens per doc\n",
    "    8. reference - Metadata related to training data or model perhaps\n",
    "    \"\"\"\n",
    "    save_model(model, data_set, model_size, stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Step by step explanation of model:\n",
    "\n",
    "**Encoder**\n",
    "- 1. logsoft (LogSoftmax)\n",
    "    - a. Converts input tokens to log-probability representation for smoothing, preventing over/underflow, normalizing. \n",
    "    - b. Could be that the input tokens are treated as if they already carry certain relationships and LogSoftmax helps the model understand them probabilistically before passing them to Embeding layer _V\n",
    "- 2. _V (Embedding Layer)\n",
    "    - a. Is of dim: vocab_size x embed_size and converts each incoming word into embed_size vector\n",
    "    - b. Frozen during warm start and explicitly initialized to avoid changing during early training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Dataloader Setup for SAFFU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data directories\n",
    "#train_dir = '../data/train/'\n",
    "val_dir = '../data/train_small/'\n",
    "\n",
    "# Get the dataloaders\n",
    "#train_loader = get_saffuloader(train_dir, tokenizer)\n",
    "val_loader = get_saffuloader(val_dir, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_loader))\n",
    "# print(len(train_loader.x_tok))\n",
    "# print(len(train_loader.y_tok))\n",
    "# print(len(train_loader.file_paths))\n",
    "# print(train_loader.x_tok[0].shape)\n",
    "# print(train_loader.y_tok[0].shape)\n",
    "# print(train_loader.file_paths[0])\n",
    "\n",
    "# Print the total length of the validation loader\n",
    "print(\"Total length of val_loader (number of batches):\", len(val_loader))\n",
    "\n",
    "# Print the number of elements in val_loader.x_tok\n",
    "print(\"Number of elements in x_tok (validation tokens):\", len(val_loader.x_tok))\n",
    "\n",
    "# Print the number of elements in val_loader.y_tok\n",
    "print(\"Number of elements in y_tok (validation metadata):\", len(val_loader.y_tok))\n",
    "\n",
    "# Print the number of file paths in val_loader.file_paths\n",
    "print(\"Number of file paths (corresponding to each element):\", len(val_loader.file_paths))\n",
    "\n",
    "# Print the shape of the first example from x_tok\n",
    "print(\"Shape of the first x_tok example (input tensor):\", val_loader.x_tok[0].shape)\n",
    "\n",
    "# Print the full tensor of the first x_tok example\n",
    "print(\"First x_tok tensor:\\n\", val_loader.x_tok[0])\n",
    "\n",
    "# Print the shape of the first example from y_tok\n",
    "print(\"Shape of the first y_tok example (metadata tensor):\", val_loader.y_tok[0].shape)\n",
    "\n",
    "# Print the full tensor of the first y_tok example\n",
    "print(\"First y_tok tensor:\\n\", val_loader.y_tok[0])\n",
    "\n",
    "# Print the first file path in val_loader.file_paths\n",
    "print(\"First file path:\", val_loader.file_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# Streamline TestRNN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import importlib to reload modules and sys and os to add the path for other imports\n",
    "# import importlib\n",
    "# import sys\n",
    "# import os\n",
    "\n",
    "# # Append the parent directory to the path to import the necessary modules\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# # Import the utilities and the dataloader\n",
    "# from utils import selfutil\n",
    "# from classes import SpreadsheetDataLoader, TestRNN\n",
    "\n",
    "# # Now reload the modules to ensure they are up-to-date\n",
    "# importlib.reload(selfutil)\n",
    "# importlib.reload(SpreadsheetDataLoader)\n",
    "# importlib.reload(TestRNN)\n",
    "\n",
    "# # Import the funcs needed from utils\n",
    "# from utils.selfutil import get_vocabulary, create_embeddings, to_gpu\n",
    "\n",
    "# # Import the SpreadsheetDataLoader class\n",
    "# from classes.SpreadsheetDataLoader import SpreadsheetDataLoader\n",
    "# from classes.TestRNN import TestRNN\n",
    "\n",
    "# # Other regular imports\n",
    "# import torch.nn as nn\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "# import gc\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import math\n",
    "# import time\n",
    "\n",
    "# # Set the directory containing the spreadsheets\n",
    "# data_dir = '../data/train_small/'\n",
    "\n",
    "# # Get the list of file paths\n",
    "# spreadsheet_vocab,file_paths = get_vocabulary(data_dir)\n",
    "\n",
    "# # Print info\n",
    "# print(f'\\n\\nVocabulary size: {len(spreadsheet_vocab._word2idx)}')\n",
    "# print(f'Files Processed: {len(file_paths)}')\n",
    "\n",
    "# # Create the embeddings for each word in the vocabulary and view info\n",
    "# spreadsheet_wvs = create_embeddings(spreadsheet_vocab)\n",
    "# print(f'Word Embeddings Shape: {spreadsheet_wvs.shape}')\n",
    "# print(f'\\nExample Embedding for <unk> at index 0:\\n{spreadsheet_wvs[0]}')\n",
    "\n",
    "# # Create the SpreadsheetDataLoader object with the vocabulary and file paths and view\n",
    "# check_loader = SpreadsheetDataLoader(file_paths, spreadsheet_vocab)\n",
    "# print(f'Spreadsheets Processed: {len(check_loader)}')\n",
    "# print(f'x_tok Tensor Shape: {check_loader.x_tok[0].shape}')\n",
    "# print(f'y_tok Tensor Shape: {check_loader.y_tok[0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestRNN2(nn.Module):\n",
    "\n",
    "    # Constructor of the RNN_LM class, initializing the layers and weights\n",
    "    def __init__(self, hidden_state_dim, rnn_layers, embedding_matrix, dropout_rate=0.0, nonlinearity='relu'):\n",
    "\n",
    "        # Ensures functions of parent class nn.Module are called in subclass RNN_LM\n",
    "        super(TestRNN2, self).__init__()\n",
    "\n",
    "        # Rows of embed matrix = Each word in the vocabulary\n",
    "        self.vocab_size = embedding_matrix.shape[0]  # vocab_size = 34057\n",
    "\n",
    "        # Cols of embed matrix = Length of each embedding vector\n",
    "        self.embedding_dim = embedding_matrix.shape[1]  # embed_dim = 50\n",
    "\n",
    "        # The dimension of the hidden state vector 'h' for each step/token\n",
    "        self.hidden_dim = hidden_state_dim  # hid_dim = 100\n",
    "\n",
    "        # Number of recurrent layers we will use\n",
    "        self.rnn_layers = rnn_layers  # rnn_layers = 2\n",
    "\n",
    "        # Creates an embedding layer from the pre-trained embedding matrix that maps input tokens to their corresponding word vectors\n",
    "        # If freezing then embeddings don't change during training, we need False because we need them to finetune to our task\n",
    "        self._embed = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "\n",
    "        # Randomly zeroes out a percentage of input units determined by dropout_rate for each update during training\n",
    "        self._drop = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # RNN layer with 'relu' nonlinearity but not managing exploding gradients, dropout and multiple recurrent layers\n",
    "        self._rnn = nn.RNN(\n",
    "            self.embedding_dim,\n",
    "            self.hidden_dim,\n",
    "            self.rnn_layers,\n",
    "            nonlinearity=nonlinearity,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        # Linear layer to map the concatenated hidden states to logits (1 to predict bold or not)\n",
    "        self._pred = nn.Linear(2 * self.hidden_dim, 1)\n",
    "\n",
    "    def cell_hs(self, x):\n",
    "\n",
    "        # Set the manual seed for reproducibility\n",
    "        torch.manual_seed(0)\n",
    "\n",
    "        # Initialize H_local as a zero tensor with the appropriate shape (num_cells, hidden_dim)\n",
    "        H_local = torch.zeros(x.shape[1] * x.shape[2], self.hidden_dim, device=x.device) # cells x hidden_dim\n",
    "        \n",
    "#         # DEBUG PRINT\n",
    "#         print(f'Input x: {x.shape}')\n",
    "#         print(f'\\nH_local before {H_local.shape}:\\n{H_local}')\n",
    "\n",
    "        # Iterate over each cell\n",
    "        for cell in tqdm(range(x.shape[1] * x.shape[2]),desc=\"Getting Cells\"):\n",
    "            \n",
    "            # Get the current row and col\n",
    "            row = cell // x.shape[2]\n",
    "            col = cell % x.shape[2]\n",
    "            \n",
    "            # Extract cell tokens across batches for current cell\n",
    "            celltoks_across_batch = x[:, row, col, :] # batch_size x tokens\n",
    "\n",
    "            # Get tokens in embedding dim and apply dropout\n",
    "            embedded_toks = self._drop(self._embed(celltoks_across_batch)) # batch_size x tokens x embed_dim\n",
    "\n",
    "            # Now run RNN on dropout\n",
    "            _, h = self._rnn(embedded_toks)\n",
    "            \n",
    "            # Store hidden state from last rnn layer for last token in H_local tensor\n",
    "            H_local[cell] = h[-1, -1, :]\n",
    "            \n",
    "#             # DEBUG PRINT\n",
    "#             if cell == 0:\n",
    "#                 print(f'\\nInside Cell {cell}\\nRow {row}, Col {col}')\n",
    "#                 print(f'\\nCell Across {celltoks_across_batch.shape}:\\n{celltoks_across_batch}')\n",
    "#                 print(f'\\nCell Embedded Toks {embedded_toks.shape}:\\n{embedded_toks}')\n",
    "#                 print(f'\\nRNN H {h.shape}:\\n{h}')\n",
    "#                 print(f'\\nLast RNN Layer Last Token HS {H_local[cell].shape}:\\n{H_local[cell]}')\n",
    "\n",
    "            # Delete intermediate tensors to free up memory\n",
    "            del celltoks_across_batch\n",
    "            del embedded_toks\n",
    "            del h\n",
    "\n",
    "\n",
    "        # Now get the sum of all the HS in size cells x hidden_dim and subtract individual HS\n",
    "        ans = H_local.sum(dim=0, keepdim=True) - H_local # cells x hidden_dim\n",
    "        \n",
    "#         # DEBUG PRINT\n",
    "#         print(f'\\nFinal H_local {H_local.shape}:\\n{H_local}')\n",
    "#         print(f'\\nFinal Returned Tensor {ans.shape}:\\n{ans}')\n",
    "\n",
    "        # Delete the H_local \n",
    "        del H_local\n",
    "        \n",
    "        # Calculate the global sum and return the adjusted tensor\n",
    "        return ans\n",
    "    \n",
    "    # Forward function\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Set the manual seed\n",
    "        torch.manual_seed(0)\n",
    "\n",
    "        # Global hidden states containing info around current cell already on gpu\n",
    "        H_global = self.cell_hs(x) # cells x hidden_dim\n",
    "\n",
    "        # Tensor to store the full macro cube of size batch x rows x cols\n",
    "        S_cube = torch.zeros((x.shape[0], x.shape[1], x.shape[2]), device=x.device)\n",
    "        \n",
    "#         # DEBUG PRINT\n",
    "#         print(f'\\nInput x {x.shape}')\n",
    "#         print(f'\\nInitial H_global {H_global.shape}:\\n{H_global}')\n",
    "#         print(f'\\nInitial S_cube {S_cube.shape}:\\n{S_cube}')\n",
    "\n",
    "        # Loop through all rows x cols cells\n",
    "        for cell in range(x.shape[1] * x.shape[2]):\n",
    "            \n",
    "            # Get the current row and col\n",
    "            row = cell // x.shape[2]\n",
    "            col = cell % x.shape[2]\n",
    "            \n",
    "            # Extract cell tokens across batches for current cell\n",
    "            celltoks_across_batch = x[:, row, col, :] # batch_size x tokens\n",
    "            \n",
    "            # Get tokens in embedding dim and apply dropout\n",
    "            embedded_toks = self._drop(self._embed(celltoks_across_batch)) # batch_size x tokens x embed_dim\n",
    "\n",
    "            # Now run RNN on embedded toks\n",
    "            z, _ = self._rnn(embedded_toks) # batch_size x tokens x hidden_dim\n",
    "            \n",
    "            # Get z for last token across all batches and hidden dim\n",
    "            z_lasttok = z[:, -1, :] # batch_size x hidden_dim\n",
    "            \n",
    "            # Extract H_global for current cell and introduce first dimension, then expand first dim to batch_size\n",
    "            H_cell = H_global[cell].unsqueeze(0).expand(x.shape[0], -1) # batch_size x hidden_dim\n",
    "\n",
    "            # Concatenate global/local context of cell along first dim batch_size then apply dropout\n",
    "            concat_hs = self._drop(torch.cat((z_lasttok, H_cell), dim = 1)) # batch_size x (2 * hidden_dim)\n",
    "            \n",
    "            # Make preds using this HS and adjust to be batch_size, set to current location in S_cube\n",
    "            S_cube[:, row, col] = self._pred(concat_hs).view(-1) # batch_size\n",
    "            \n",
    "#             # DEBUG PRINT\n",
    "#             if cell == 0:\n",
    "#                 print(f'\\nInside Cell {cell}\\nRow {row}, Col {col}')\n",
    "#                 print(f'\\nCell Across {celltoks_across_batch.shape}:\\n{celltoks_across_batch}')\n",
    "#                 print(f'\\nCell Embedded Toks {embedded_toks.shape}:\\n{embedded_toks}')\n",
    "#                 print(f'\\nRNN Z {z.shape}:\\n{z}')\n",
    "#                 print(f'\\nRNN Z Last Token {z_lasttok.shape}:\\n{z_lasttok}')\n",
    "#                 print(f'\\nH_cell global HS for cell {H_cell.shape}:\\n{H_cell}')\n",
    "#                 print(f'\\nConcatenated HS {concat_hs.shape}:\\n{concat_hs}')\n",
    "#                 print(f'\\nPredictions {S_cube[:, row, col].shape}:\\n{S_cube[:, row, col]}')\n",
    "\n",
    "\n",
    "            # Delete intermediate tensors to free up memory\n",
    "            del celltoks_across_batch\n",
    "            del embedded_toks\n",
    "            del z\n",
    "            del z_lasttok\n",
    "            del H_cell\n",
    "            del concat_hs\n",
    "        \n",
    "        \n",
    "#         # DEBUG PRINT\n",
    "#         print(f'\\nFinal S_cube {S_cube.shape}:\\n{S_cube}')\n",
    "        \n",
    "        # Delete H_global finally\n",
    "        del H_global\n",
    "        \n",
    "        # Return the final S_cube\n",
    "        return S_cube\n",
    "        \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA Vars to avoid randomization\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Create a DataLoader from your check_loader\n",
    "test_loader = torch.utils.data.DataLoader(check_loader, batch_size=4, shuffle=False)\n",
    "\n",
    "# Get one batch from the DataLoader\n",
    "batch = next(iter(test_loader))\n",
    "\n",
    "# Move batch to current gpu device\n",
    "exfile = batch['x_tok'].to(device)\n",
    "\n",
    "# Define NN Params\n",
    "hidden_state_dim = 100\n",
    "rnn_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and print\n",
    "rnn_model2 = TestRNN2(hidden_state_dim, rnn_layers, spreadsheet_wvs).to(device)\n",
    "print(rnn_model2)\n",
    "\n",
    "# Run the forward method\n",
    "out2 = rnn_model2.forward(exfile)\n",
    "\n",
    "# Print the shape of S_cube\n",
    "print(\"RNN2 Output shape:\", out2.shape)\n",
    "\n",
    "# # Calculate the absolute difference between the two tensors\n",
    "# absolute_diff = torch.abs(out - out2)\n",
    "\n",
    "# # Calculate the mean of the absolute differences\n",
    "# mean_absolute_diff = torch.mean(absolute_diff)\n",
    "\n",
    "# print(f\"Mean absolute difference: {mean_absolute_diff.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# Trying New Model with SAFFU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA Vars to avoid randomization\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Create a DataLoader from your check_loader\n",
    "test_loader = torch.utils.data.DataLoader(val_loader, batch_size=2, shuffle=False)\n",
    "\n",
    "# Get one batch from the DataLoader\n",
    "batch = next(iter(test_loader))\n",
    "\n",
    "# Extract single x_tok example from batch\n",
    "ex_input = batch['x_tok'][0]\n",
    "\n",
    "# Extract single cell's tokens\n",
    "cell_tokens = ex_input[0,0,:]\n",
    "#cell_tokens = cell_tokens.unsqueeze(0)\n",
    "\n",
    "# Observe shape and tensor\n",
    "print(f'Single x_tok tensor from val_loader {ex_input.shape}')\n",
    "print(f'\\nTokens for first cell shaped {cell_tokens.shape}:\\n{cell_tokens}')\n",
    "\n",
    "# # Pass the single tensor through SAFFU model\n",
    "# one_pass = model.forward(cell_tokens, cell_tokens.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking input with single cell tokens in encoder\n",
    "output = model.encoder(cell_tokens)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "# Tuning\n",
    "\n",
    "## Setup params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total params in the model so the number of elements for each param name all together\n",
    "total_params = 0\n",
    "\n",
    "# Total learnable params\n",
    "total_learnable = 0\n",
    "\n",
    "# Iterate through each macro param in model's params like Psi_b, _V.weight, RS.0._U.weight etc.   \n",
    "for name, param in model.named_parameters():\n",
    "    \n",
    "    # Current param count is product of tensor dims (log-exp avoids overflow -> one less than actual)\n",
    "    curr_params = int(np.exp(sum(np.log(param.shape))))\n",
    "    \n",
    "    # Add curr_params to learnable params if curr_param requires grad\n",
    "    total_learnable += curr_params if param.requires_grad else 0\n",
    "    \n",
    "    # Add curr_params to total_params\n",
    "    total_params += curr_params\n",
    "\n",
    "# Print the ratio of learnable to all params\n",
    "print(f\"Total numbers of learnable/all parameters: {total_learnable}/{total_params}\")\n",
    "\n",
    "# Set grad to false to freeze embedding layer since we are warm starting\n",
    "model.encoder._V.weight.requires_grad = False\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 691\n",
    "\n",
    "# Define whether warm starting or not\n",
    "warm_start = True\n",
    "\n",
    "# Determine if verbose output needed when running tuner.warm_start()\n",
    "verbose = True\n",
    "\n",
    "# Adds spaces with tokens if set to False else removes them if True\n",
    "ignore_space = False\n",
    "\n",
    "# Case sensitive if False and lowercases everything if True\n",
    "ignore_case = False\n",
    "\n",
    "# Used for initializating _V (embeddings) matrix during the warm start. Higher = Richer init repr for each token\n",
    "warm_vecs = 1*(2**0 + 0.99999) # = 2\n",
    "\n",
    "# Do-nothing transform, sets part embeddings to identity stabilizing over-aggressive/random init states\n",
    "identity_ratio = 2**(-1) # = 0.5\n",
    "\n",
    "# Inverse Context Freq: co-occurrence counts reweight words based on rarity/importance\n",
    "# Highlights contextually significant words leading to better differentiation if set to True\n",
    "icf = True \n",
    "\n",
    "# Indicates if token labels are log/linear. Log reduces impact of high freq elements, linear simpler \n",
    "log_label = False\n",
    "\n",
    "# Number of distinct clusters to assign to tokens during training\n",
    "nlabels = 1*(2**0) # = 1 = No subdivision cluster of toks\n",
    "\n",
    "# Should model consider centroids during clustering/quantization process\n",
    "centroids = False\n",
    "\n",
    "# Iterations to refine label assignments in training. Higher = Reclusturing more for each token getting more accurate groups\n",
    "label_iterations = 1*(2**10) # = 1024\n",
    "\n",
    "# Determine epochs and scale by factor of 1024 to account for smaller datasets\n",
    "epochs = int(np.max([int(downsample/5), 1]))*(2**10)\n",
    "\n",
    "# Determine if reloading or new\n",
    "reload = False\n",
    "\n",
    "# Determine patience to of observing no loss reduction\n",
    "patience = 2**1\n",
    "\n",
    "# Finally name the file used for warm start\n",
    "warm_file = \"\".join([data_file[:-5] + \"-\", \n",
    "                     f\"b_{tokenizer.config._bits}-hb_{tokenizer.config._hidden}-\",\n",
    "                     f\"we_{int(tokenizer.config._wave_encode)}-oa_{tokenizer.config._o_agg}-ra_{tokenizer.config._r_agg}-ba_{tokenizer.config._b_agg}-\",\n",
    "                     f\"mr_{int(tokenizer.config._mask_r)}-mb_{int(tokenizer.config._mask_b)}-md_{tokenizer.config._model_documents}-\",\n",
    "                     f\"is_{int(ignore_space)}-ic_{int(ignore_case)}-ws_{int(warm_start)}-wv_{int(warm_vecs)}-ds_{downsample}-seed_{seed}\"])\n",
    "\n",
    "# Define traning and dev directories\n",
    "train_dir = '../data/train/'; dev_dir = '../data/train_small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the convos lists required for tuner\n",
    "convos = dir2convos(train_dir)\n",
    "dconvos = dir2convos(dev_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "convos[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## Warm Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tuner\n",
    "tuner = SAFFUTuner(ignore_case, ignore_space, devstr, warm_vecs, identity_ratio = identity_ratio,\n",
    "                   label_iterations = label_iterations, log_label = log_label, nlabels = nlabels,\n",
    "                   centroids = centroids, icf = icf)\n",
    "\n",
    "# Warm start with params\n",
    "tuner.warm_start(model, convos, dconvos, downsample*10, seed, epochs, \n",
    "                 patience, devsample = devsample, model_file = warm_file,\n",
    "                 reload = reload, verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_params, total_learnable = 0, 0\n",
    "# for name, param in model.named_parameters():\n",
    "#     total_params += int(np.exp(sum(np.log(param.shape))))\n",
    "#     if param.requires_grad:\n",
    "#         total_learnable += int(np.exp(sum(np.log(param.shape)))) # param.shape[0]*param.shape[1]\n",
    "#         # print(name, param.shape[0]*param.shape[1])\n",
    "\n",
    "# print(f\"Total numbers of learnable/all parameters: {total_learnable}/{total_params}\")\n",
    "\n",
    "# model.encoder._V.weight.requires_grad = False\n",
    "\n",
    "# seed = 691; ignore_space = False; ignore_case = False; warm_start = True; verbose = True; \n",
    "# warm_vecs = 1*(2**0 + 0.99999); identity_ratio = 2**(-1); icf = True \n",
    "# log_label = False; nlabels = 1*(2**0); centroids = False; label_iterations = 1*(2**10) # None \n",
    "# epochs = int(np.max([int(downsample/5), 1]))*(2**5)\n",
    "# patience = 2**1\n",
    "# reload = False\n",
    "# warm_file = \"\".join([data_file[:-5] + \"-\", \n",
    "#                      f\"b_{tokenizer.config._bits}-hb_{tokenizer.config._hidden}-\",\n",
    "#                      f\"we_{int(tokenizer.config._wave_encode)}-oa_{tokenizer.config._o_agg}-ra_{tokenizer.config._r_agg}-ba_{tokenizer.config._b_agg}-\",\n",
    "#                      f\"mr_{int(tokenizer.config._mask_r)}-mb_{int(tokenizer.config._mask_b)}-md_{tokenizer.config._model_documents}-\",\n",
    "#                      f\"is_{int(ignore_space)}-ic_{int(ignore_case)}-ws_{int(warm_start)}-wv_{int(warm_vecs)}-ds_{downsample}-seed_{seed}\"])\n",
    "# tuner = SAFFUTuner(ignore_case, ignore_space, devstr, warm_vecs, identity_ratio = identity_ratio, \n",
    "#                    label_iterations = label_iterations, log_label = log_label, nlabels = nlabels, centroids = centroids, icf = icf)\n",
    "# tuner.warm_start(model, convos, dconvos, downsample*10, seed, epochs, patience, devsample = devsample, model_file = warm_file, reload = reload, verbose = verbose"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
