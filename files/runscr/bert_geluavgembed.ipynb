{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import importlib to reload modules and sys and os to add the path for other imports\n",
    "import importlib\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Append the parent directory to the path to import the necessary modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import the utilities and the dataloader\n",
    "from utils import trainutil, inferutil, setuputil\n",
    "\n",
    "# Now reload the modules to ensure they are up-to-date\n",
    "importlib.reload(setuputil)\n",
    "importlib.reload(trainutil)\n",
    "importlib.reload(inferutil)\n",
    "\n",
    "# Import the funcs needed from utils\n",
    "from utils.setuputil import setup_config, display_config\n",
    "from utils.trainutil import train_model\n",
    "from utils.inferutil import infer_one, infer_full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the input config file\n",
    "input_config = {\n",
    "    # Environment and Model Info\n",
    "    \"env\": \"gcp\",                \n",
    "    \"approach\": \"bert\",         \n",
    "    \"model_name\": \"TestBert\",     \n",
    "    \"model_base\": \"prajjwal1/bert-tiny\",  \n",
    "    \n",
    "    # System Configuration\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"threads\": 14,\n",
    "    \"seed\": 42,\n",
    "    \n",
    "    # Data Configuration\n",
    "    \"data_dir\": \"../../data/farzan\",\n",
    "    \"data_ds\": \"manual\",\n",
    "    \n",
    "    # Model Parameters\n",
    "    \"rows\": 100,\n",
    "    \"cols\": 100,\n",
    "    \"tokens\": 32,\n",
    "    \n",
    "    # Training Parameters\n",
    "    \"batch\": 40,\n",
    "    \"lr\": 5e-3,\n",
    "    \"mu\": 0.25,\n",
    "    \"epochs\": 20,\n",
    "    \"patience\": 2,\n",
    "    \"save_int\": 10,\n",
    "    \"save_dir\": '../models/'\n",
    "}\n",
    "config = setup_config(input_config)\n",
    "display_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define local variables from the config dictionary\n",
    "DEVICE = config[\"DEVICE\"]\n",
    "THREADS = config[\"THREADS\"]\n",
    "\n",
    "# Data loaders and vocab\n",
    "train_loader = config[\"train_loader\"]\n",
    "val_loader = config[\"val_loader\"]\n",
    "test_loader = config[\"test_loader\"]\n",
    "tokenizer = config[\"tokenizer\"]\n",
    "\n",
    "# Training parameters\n",
    "batch_size = config[\"batch\"]\n",
    "lr = config[\"lr\"]\n",
    "mu = config[\"mu\"]\n",
    "epochs = config[\"epochs\"]\n",
    "patience = config[\"patience\"]\n",
    "save_int = config[\"save_int\"]\n",
    "save_dir = config[\"save_dir\"]\n",
    "save_name = config[\"save_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class TestBERT(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-cased\", dropout_rate=0.05):\n",
    "        super(TestBERT, self).__init__()\n",
    "\n",
    "        # 1. Load pretrained BERT\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # 2. Define a dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # 3. Non-linear activation (GELU)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        # 4. Final predictor (1-dim output per cell)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        # 1) Allocate the (batch_size, rows, cols) S_cube\n",
    "        S_cube = torch.zeros(\n",
    "            (input_ids.shape[0], input_ids.shape[1], input_ids.shape[2]),\n",
    "            device=input_ids.device,\n",
    "        )\n",
    "\n",
    "        # 2) Loop over cells in row-major order\n",
    "        for cell in tqdm(\n",
    "            range(input_ids.shape[1] * input_ids.shape[2]), desc=\"Forward\"\n",
    "        ):\n",
    "\n",
    "            # In one shot, store logits â†’ S_cube\n",
    "            # cell // input_ids.shape[2] = row, cell % input_ids.shape[2] = col\n",
    "            S_cube[\n",
    "                :, cell // input_ids.shape[2], cell % input_ids.shape[2]\n",
    "            ] = self.classifier(\n",
    "                self.gelu(\n",
    "                    self.dropout(\n",
    "                        self.bert(\n",
    "                            input_ids[\n",
    "                                :,\n",
    "                                cell // input_ids.shape[2],\n",
    "                                cell % input_ids.shape[2],\n",
    "                                :,\n",
    "                            ],\n",
    "                            attention_mask=attention_mask[\n",
    "                                :,\n",
    "                                cell // input_ids.shape[2],\n",
    "                                cell % input_ids.shape[2],\n",
    "                                :,\n",
    "                            ],\n",
    "                        ).pooler_output\n",
    "                    )\n",
    "                )\n",
    "            ).view(\n",
    "                -1\n",
    "            )\n",
    "\n",
    "        return S_cube\n",
    "\n",
    "\n",
    "# class TestBERT(nn.Module):\n",
    "#     def __init__(self, model_name=\"bert-base-cased\", dropout_rate=0.05):\n",
    "#         super(TestBERT, self).__init__()\n",
    "\n",
    "#         # 1. Load pretrained BERT\n",
    "#         self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "#         # 2. Define a dropout\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "#         # 3. Non-linear activation (GELU)\n",
    "#         self.gelu = nn.GELU()\n",
    "\n",
    "#         # 4. Final predictor (1-dim output per cell)\n",
    "#         self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "\n",
    "#         # 1. Print the overall shapes\n",
    "#         # print(\"batch_size:\", input_ids.shape[0])\n",
    "#         # print(\"rows:\",      input_ids.shape[1])\n",
    "#         # print(\"cols:\",      input_ids.shape[2])\n",
    "#         # print(\"tokens:\",    input_ids.shape[3])\n",
    "\n",
    "#         # 2. Initialize S_cube => (batch_size, rows, cols)\n",
    "#         S_cube = torch.zeros(\n",
    "#             (input_ids.shape[0], input_ids.shape[1], input_ids.shape[2]),\n",
    "#             device=input_ids.device\n",
    "#         )\n",
    "\n",
    "#         # 3. Loop over all cells\n",
    "#         for cell in tqdm(range(input_ids.shape[1] * input_ids.shape[2]), desc = 'Forward'):\n",
    "\n",
    "#             r = cell // input_ids.shape[2]\n",
    "#             c = cell %  input_ids.shape[2]\n",
    "\n",
    "#             # Extract the slice for current cell (batch_size x tokens)\n",
    "#             cell_input_ids  = input_ids[:, r, c, :]\n",
    "#             cell_attn_mask  = attention_mask[:, r, c, :]\n",
    "\n",
    "#             # Pass them through the BERT model\n",
    "#             outputs = self.bert(cell_input_ids, attention_mask=cell_attn_mask)\n",
    "\n",
    "#             # pooler_out => (batch_size, hidden_dim)\n",
    "#             pooler_out = outputs.pooler_output\n",
    "\n",
    "#             # Inlined pipeline: dropout -> GELU -> classifier => (batch_size, 1)\n",
    "#             logits = self.classifier(self.gelu(self.dropout(pooler_out)))\n",
    "\n",
    "#             # Flatten (batch_size, 1) => (batch_size,)\n",
    "#             logits_flat = logits.view(-1)\n",
    "\n",
    "#             # Populate S_cube => shape: (batch_size, rows, cols)\n",
    "#             S_cube[:, r, c] = logits_flat\n",
    "\n",
    "#             # If this is the first cell, do some prints and break\n",
    "#             if r == 0 and c == 0:\n",
    "#                 print(f\"\\nFirst cell => row={r}, col={c}\")\n",
    "#                 print(f\"cell_input_ids.shape: {cell_input_ids.shape}\")\n",
    "#                 print(f\"cell_attn_mask.shape: {cell_attn_mask.shape}\")\n",
    "#                 print(f\"logits.shape: {logits.shape}\")\n",
    "#                 print(f\"logits_flat.shape: {logits_flat.shape}\")\n",
    "#                 print(f\"S_cube[:, {r}, {c}].shape: {S_cube[:, r, c].shape}\")\n",
    "\n",
    "#                 break  # Stop after the first cell\n",
    "\n",
    "#         # 4. Print the shape of S_cube\n",
    "#         # print(f\"\\nS_cube.shape: {S_cube.shape}\")\n",
    "\n",
    "#         # Return S_cube or None, depending on your use case\n",
    "#         return S_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "#  Full Notebook Code with ONLY Mixed Precision (fp16), No DeepSpeed\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# 1) Standard imports\n",
    "import importlib\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# Append the parent directory to the path to import the necessary modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import the utilities and the dataloader\n",
    "from utils import trainutil, inferutil, setuputil\n",
    "\n",
    "# Now reload the modules to ensure they are up-to-date\n",
    "importlib.reload(setuputil)\n",
    "importlib.reload(trainutil)\n",
    "importlib.reload(inferutil)\n",
    "\n",
    "# Import the funcs needed from utils\n",
    "from utils.setuputil import setup_bert_config, display_bert_config\n",
    "from utils.trainutil import train_model\n",
    "from utils.inferutil import infer_one, infer_full\n",
    "\n",
    "# Define the input config file\n",
    "setup_config = {\n",
    "    \"model_name\": \"prajjwal1/bert-tiny\",\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"threads\": 8,\n",
    "    \"seed\": 0,\n",
    "    \"data_dir\": \"../../data/farzan/\",\n",
    "    \"data_ds\": \"manual\",\n",
    "    \"rows\": 100,\n",
    "    \"cols\": 100,\n",
    "    \"tokens\": 32\n",
    "}\n",
    "\n",
    "# Get the actual to use config file and view\n",
    "config = setup_bert_config(setup_config)\n",
    "display_bert_config(config)\n",
    "\n",
    "# Define local variables as per the variables from config\n",
    "DEVICE = config['DEVICE']\n",
    "THREADS = config['THREADS']\n",
    "train_loader = config['train_loader']\n",
    "val_loader = config['val_loader']\n",
    "test_loader = config['test_loader']\n",
    "model_name = config['model_name']\n",
    "tokenizer = config['tokenizer']\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class TestBERT(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-cased\", dropout_rate=0.05):\n",
    "        super(TestBERT, self).__init__()\n",
    "\n",
    "        # 1. Load pretrained BERT\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Enable gradient checkpointing if desired\n",
    "        self.bert.gradient_checkpointing_enable()\n",
    "\n",
    "        # 2. Define a dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # 3. Non-linear activation (GELU)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        # 4. Final predictor (1-dim output per cell)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        # 1) Allocate the (batch_size, rows, cols) S_cube\n",
    "        S_cube = torch.zeros(\n",
    "            (input_ids.shape[0], input_ids.shape[1], input_ids.shape[2]),\n",
    "            device=input_ids.device,\n",
    "        )\n",
    "\n",
    "        # 2) Loop over cells in row-major order\n",
    "        for cell in tqdm(\n",
    "            range(input_ids.shape[1] * input_ids.shape[2]), desc=\"Forward\"\n",
    "        ):\n",
    "\n",
    "            # cell // input_ids.shape[2] = row, cell % input_ids.shape[2] = col\n",
    "            S_cube[\n",
    "                :, cell // input_ids.shape[2], cell % input_ids.shape[2]\n",
    "            ] = (\n",
    "                self.classifier(\n",
    "                    self.gelu(\n",
    "                        self.dropout(\n",
    "                            self.bert(\n",
    "                                input_ids[:, cell // input_ids.shape[2],\n",
    "                                          cell % input_ids.shape[2], :],\n",
    "                                attention_mask=attention_mask[:, cell // input_ids.shape[2],\n",
    "                                                              cell % input_ids.shape[2], :]\n",
    "                            ).pooler_output\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                .view(-1)\n",
    "            )\n",
    "\n",
    "        return S_cube\n",
    "\n",
    "\n",
    "# 1) Create model and move to GPU\n",
    "untrained_model = TestBERT(model_name=model_name).to(DEVICE)\n",
    "\n",
    "# 2) Single-batch DataLoader\n",
    "check_loader = torch.utils.data.DataLoader(train_loader, batch_size=1, shuffle=False)\n",
    "batch = next(iter(check_loader))\n",
    "\n",
    "ex_xtok = batch[\"x_tok\"].to(DEVICE)\n",
    "ex_xmask = batch[\"x_masks\"].to(DEVICE)\n",
    "\n",
    "# 3) FP16 forward pass with torch.cuda.amp\n",
    "with autocast():\n",
    "    out = untrained_model.forward(ex_xtok, ex_xmask)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
