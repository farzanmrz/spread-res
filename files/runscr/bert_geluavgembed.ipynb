{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d65851d-3622-4eec-8475-e76f99e0d9c4",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "74ae2fc8-f6d6-473c-b023-aeaeb617270a",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-01-03T23:37:54.015249Z",
     "start_time": "2025-01-03T23:37:51.247105Z"
    }
   },
   "source": [
    "# Import importlib to reload modules and sys and os to add the path for other imports\n",
    "import importlib\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Append the parent directory to the path to import the necessary modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import the utilities and the dataloader\n",
    "from utils import trainutil, inferutil, setuputil\n",
    "\n",
    "# Now reload the modules to ensure they are up-to-date\n",
    "importlib.reload(setuputil)\n",
    "importlib.reload(trainutil)\n",
    "importlib.reload(inferutil)\n",
    "\n",
    "# Import the funcs needed from utils\n",
    "from utils.setuputil import setup_config, display_config\n",
    "from utils.trainutil import train_model\n",
    "from utils.inferutil import infer_one, infer_full\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "4bb8749f-c28d-46a8-8712-595caf4c967e",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "e002e5a6-33ca-45bc-85ea-825b9bc56713",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-01-03T23:38:05.526002Z",
     "start_time": "2025-01-03T23:37:57.647994Z"
    }
   },
   "source": [
    "# Define the input config file\n",
    "input_config = {\n",
    "    # Environment and Model Info\n",
    "    \"env\": \"local\",                \n",
    "    \"approach\": \"bert\",         \n",
    "    \"model_name\": \"TestBert\",     \n",
    "    \"model_base\": \"prajjwal1/bert-tiny\",  \n",
    "    \n",
    "    # System Configuration\n",
    "    \"device\": \"mps\",\n",
    "    \"threads\": 8,\n",
    "    \"seed\": 42,\n",
    "    \n",
    "    # Data Configuration\n",
    "    \"data_dir\": \"../../data/farzan\",\n",
    "    \"data_ds\": \"manual\",\n",
    "    \n",
    "    # Model Parameters\n",
    "    \"rows\": 100,\n",
    "    \"cols\": 100,\n",
    "    \"tokens\": 32,\n",
    "    \n",
    "    # Training Parameters\n",
    "    \"batch\": 40,\n",
    "    \"lr\": 5e-3,\n",
    "    \"mu\": 0.25,\n",
    "    \"epochs\": 20,\n",
    "    \"patience\": 2,\n",
    "    \"save_int\": 10,\n",
    "    \"save_dir\": '../models/'\n",
    "}\n",
    "config = setup_config(input_config)\n",
    "display_config(config)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 40/40 [00:04<00:00,  8.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "40(P) = 40(G) + 0(E)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 5/5 [00:00<00:00, 9727.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5(P) = 5(G) + 0(E)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 5/5 [00:00<00:00, 18724.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5(P) = 5(G) + 0(E)\n",
      "\n",
      "Configuration for BERT approach:\n",
      "{\n",
      "  \"env\": \"local\",\n",
      "  \"approach\": \"bert\",\n",
      "  \"model_base\": \"prajjwal1/bert-tiny\",\n",
      "  \"model_name\": \"TestBert\",\n",
      "  \"rows\": 100,\n",
      "  \"cols\": 100,\n",
      "  \"tokens\": 32,\n",
      "  \"DEVICE\": \"mps\",\n",
      "  \"THREADS\": 8,\n",
      "  \"seed\": 42,\n",
      "  \"data_ds\": \"manual\",\n",
      "  \"data_dir\": \"../../data/farzan\",\n",
      "  \"train_dir\": \"../../data/farzan/manual_train\",\n",
      "  \"val_dir\": \"../../data/farzan/manual_val\",\n",
      "  \"test_dir\": \"../../data/farzan/manual_test\",\n",
      "  \"tokenizer\": \"<BERT Tokenizer Object>\",\n",
      "  \"train_loader\": \"<LoaderBert Object>\",\n",
      "  \"val_loader\": \"<LoaderBert Object>\",\n",
      "  \"test_loader\": \"<LoaderBert Object>\",\n",
      "  \"batch\": 40,\n",
      "  \"lr\": 0.005,\n",
      "  \"mu\": 0.25,\n",
      "  \"epochs\": 20,\n",
      "  \"patience\": 2,\n",
      "  \"save_int\": 10,\n",
      "  \"save_dir\": \"../models/\",\n",
      "  \"save_name\": \"lber42_TestBert_manual_100x100x32_bsz40lr5e-3ep20pa2\"\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "c5ea7f38-b515-42fa-b03c-9d5546ee865e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T23:38:20.487917Z",
     "start_time": "2025-01-03T23:38:20.484020Z"
    }
   },
   "source": [
    "# Define local variables from the config dictionary\n",
    "DEVICE = config[\"DEVICE\"]\n",
    "THREADS = config[\"THREADS\"]\n",
    "\n",
    "# Data loaders and vocab\n",
    "train_loader = config[\"train_loader\"]\n",
    "val_loader = config[\"val_loader\"]\n",
    "test_loader = config[\"test_loader\"]\n",
    "tokenizer = config[\"tokenizer\"]\n",
    "\n",
    "# Training parameters\n",
    "batch_size = config[\"batch\"]\n",
    "lr = config[\"lr\"]\n",
    "mu = config[\"mu\"]\n",
    "epochs = config[\"epochs\"]\n",
    "patience = config[\"patience\"]\n",
    "save_int = config[\"save_int\"]\n",
    "save_dir = config[\"save_dir\"]\n",
    "save_name = config[\"save_name\"]"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "7b4febea-dc13-4a94-8c5c-25f8f15a791b",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c1330ae-16f9-49e9-9564-23d652b4d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class TestBERT(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-cased\", dropout_rate=0.05):\n",
    "        super(TestBERT, self).__init__()\n",
    "\n",
    "        # 1. Load pretrained BERT\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # 2. Define a dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # 3. Non-linear activation (GELU)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        # 4. Final predictor (1-dim output per cell)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        # 1) Allocate the (batch_size, rows, cols) S_cube\n",
    "        S_cube = torch.zeros(\n",
    "            (input_ids.shape[0], input_ids.shape[1], input_ids.shape[2]),\n",
    "            device=input_ids.device,\n",
    "        )\n",
    "\n",
    "        # 2) Loop over cells in row-major order\n",
    "        for cell in tqdm(\n",
    "            range(input_ids.shape[1] * input_ids.shape[2]), desc=\"Forward\"\n",
    "        ):\n",
    "\n",
    "            # In one shot, store logits → S_cube\n",
    "            # cell // input_ids.shape[2] = row, cell % input_ids.shape[2] = col\n",
    "            S_cube[\n",
    "                :, cell // input_ids.shape[2], cell % input_ids.shape[2]\n",
    "            ] = self.classifier(\n",
    "                self.gelu(\n",
    "                    self.dropout(\n",
    "                        self.bert(\n",
    "                            input_ids[\n",
    "                                :,\n",
    "                                cell // input_ids.shape[2],\n",
    "                                cell % input_ids.shape[2],\n",
    "                                :,\n",
    "                            ],\n",
    "                            attention_mask=attention_mask[\n",
    "                                :,\n",
    "                                cell // input_ids.shape[2],\n",
    "                                cell % input_ids.shape[2],\n",
    "                                :,\n",
    "                            ],\n",
    "                        ).pooler_output\n",
    "                    )\n",
    "                )\n",
    "            ).view(\n",
    "                -1\n",
    "            )\n",
    "\n",
    "        return S_cube\n",
    "\n",
    "\n",
    "# class TestBERT(nn.Module):\n",
    "#     def __init__(self, model_name=\"bert-base-cased\", dropout_rate=0.05):\n",
    "#         super(TestBERT, self).__init__()\n",
    "\n",
    "#         # 1. Load pretrained BERT\n",
    "#         self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "#         # 2. Define a dropout\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "#         # 3. Non-linear activation (GELU)\n",
    "#         self.gelu = nn.GELU()\n",
    "\n",
    "#         # 4. Final predictor (1-dim output per cell)\n",
    "#         self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "\n",
    "#         # 1. Print the overall shapes\n",
    "#         # print(\"batch_size:\", input_ids.shape[0])\n",
    "#         # print(\"rows:\",      input_ids.shape[1])\n",
    "#         # print(\"cols:\",      input_ids.shape[2])\n",
    "#         # print(\"tokens:\",    input_ids.shape[3])\n",
    "\n",
    "#         # 2. Initialize S_cube => (batch_size, rows, cols)\n",
    "#         S_cube = torch.zeros(\n",
    "#             (input_ids.shape[0], input_ids.shape[1], input_ids.shape[2]),\n",
    "#             device=input_ids.device\n",
    "#         )\n",
    "\n",
    "#         # 3. Loop over all cells\n",
    "#         for cell in tqdm(range(input_ids.shape[1] * input_ids.shape[2]), desc = 'Forward'):\n",
    "\n",
    "#             r = cell // input_ids.shape[2]\n",
    "#             c = cell %  input_ids.shape[2]\n",
    "\n",
    "#             # Extract the slice for current cell (batch_size x tokens)\n",
    "#             cell_input_ids  = input_ids[:, r, c, :]\n",
    "#             cell_attn_mask  = attention_mask[:, r, c, :]\n",
    "\n",
    "#             # Pass them through the BERT model\n",
    "#             outputs = self.bert(cell_input_ids, attention_mask=cell_attn_mask)\n",
    "\n",
    "#             # pooler_out => (batch_size, hidden_dim)\n",
    "#             pooler_out = outputs.pooler_output\n",
    "\n",
    "#             # Inlined pipeline: dropout -> GELU -> classifier => (batch_size, 1)\n",
    "#             logits = self.classifier(self.gelu(self.dropout(pooler_out)))\n",
    "\n",
    "#             # Flatten (batch_size, 1) => (batch_size,)\n",
    "#             logits_flat = logits.view(-1)\n",
    "\n",
    "#             # Populate S_cube => shape: (batch_size, rows, cols)\n",
    "#             S_cube[:, r, c] = logits_flat\n",
    "\n",
    "#             # If this is the first cell, do some prints and break\n",
    "#             if r == 0 and c == 0:\n",
    "#                 print(f\"\\nFirst cell => row={r}, col={c}\")\n",
    "#                 print(f\"cell_input_ids.shape: {cell_input_ids.shape}\")\n",
    "#                 print(f\"cell_attn_mask.shape: {cell_attn_mask.shape}\")\n",
    "#                 print(f\"logits.shape: {logits.shape}\")\n",
    "#                 print(f\"logits_flat.shape: {logits_flat.shape}\")\n",
    "#                 print(f\"S_cube[:, {r}, {c}].shape: {S_cube[:, r, c].shape}\")\n",
    "\n",
    "#                 break  # Stop after the first cell\n",
    "\n",
    "#         # 4. Print the shape of S_cube\n",
    "#         # print(f\"\\nS_cube.shape: {S_cube.shape}\")\n",
    "\n",
    "#         # Return S_cube or None, depending on your use case\n",
    "#         return S_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fc45c9c-b142-4f61-8f8b-50982c2e62ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|████████████████████████████████████████| 40/40 [00:22<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "40(P) = 40(G) + 0(E)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████████████████████████████████████| 5/5 [00:00<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5(P) = 5(G) + 0(E)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████████████████████████████████████| 5/5 [00:00<00:00, 11.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5(P) = 5(G) + 0(E)\n",
      "\n",
      "Final BERT configuration:\n",
      "{\n",
      "  \"model_name\": \"prajjwal1/bert-tiny\",\n",
      "  \"data_dir\": \"../../data/farzan/\",\n",
      "  \"DEVICE\": \"cuda:0\",\n",
      "  \"THREADS\": 8,\n",
      "  \"data_ds\": \"manual\",\n",
      "  \"train_dir\": \"../../data/farzan/manual_train\",\n",
      "  \"val_dir\": \"../../data/farzan/manual_val\",\n",
      "  \"test_dir\": \"../../data/farzan/manual_test\",\n",
      "  \"rows\": 100,\n",
      "  \"cols\": 100,\n",
      "  \"tokens\": 32,\n",
      "  \"tokenizer\": \"<ModernBert Tokenizer Object>\",\n",
      "  \"train_loader\": \"<Train BertLoader Object>\",\n",
      "  \"val_loader\": \"<Validation BertLoader Object>\",\n",
      "  \"test_loader\": \"<Test BertLoader Object>\"\n",
      "}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nArtifact name: 'trace_shape_events' not registered,please call register_artifact('trace_shape_events') in torch._logging.registrations.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py:1793\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[0;34m(self, module_name)\u001B[0m\n\u001B[1;32m   1792\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1793\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1794\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/importlib/__init__.py:126\u001B[0m, in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m    125\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1050\u001B[0m, in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1027\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1006\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:688\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[0;34m(spec)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap_external>:883\u001B[0m, in \u001B[0;36mexec_module\u001B[0;34m(self, module)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:241\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[0;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:25\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcheckpoint\u001B[39;00m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpackaging\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m version\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:15\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfx\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtraceback\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mfx_traceback\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_functorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_aot_autograd\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_fun\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_pytree\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tree_map\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/functional_utils.py:19\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_subclasses\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmeta_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_sparse_any\n\u001B[0;32m---> 19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfx\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msymbolic_shapes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m definitely_true, sym_eq\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmultiprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreductions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StorageWeakRef\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:51\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfx\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _config \u001B[38;5;28;01mas\u001B[39;00m config\n\u001B[0;32m---> 51\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfx\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrecording\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     52\u001B[0m     FakeTensorMeta,\n\u001B[1;32m     53\u001B[0m     ShapeEnvEvent,\n\u001B[1;32m     54\u001B[0m     record_shapeenv_event,\n\u001B[1;32m     55\u001B[0m     replay_shape_env_events,\n\u001B[1;32m     56\u001B[0m     shape_env_check_state_equal\n\u001B[1;32m     57\u001B[0m )\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfx\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msym_node\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SymNode, SymTypes\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/fx/experimental/recording.py:14\u001B[0m\n\u001B[1;32m     13\u001B[0m log \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m---> 14\u001B[0m trace_shape_events_log \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_logging\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetArtifactLogger\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrace_shape_events\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m     16\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShapeEnvEvent\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrecord_shapeenv_event\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNotEqualError\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     26\u001B[0m ]\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/_logging/_internal.py:532\u001B[0m, in \u001B[0;36mgetArtifactLogger\u001B[0;34m(module_qname, artifact_name)\u001B[0m\n\u001B[1;32m      0\u001B[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001B[0;31mValueError\u001B[0m: Artifact name: 'trace_shape_events' not registered,please call register_artifact('trace_shape_events') in torch._logging.registrations.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 115\u001B[0m\n\u001B[1;32m    111\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m S_cube\n\u001B[1;32m    114\u001B[0m \u001B[38;5;66;03m# 1) Create model and move to GPU\u001B[39;00m\n\u001B[0;32m--> 115\u001B[0m untrained_model \u001B[38;5;241m=\u001B[39m \u001B[43mTestBERT\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(DEVICE)\n\u001B[1;32m    117\u001B[0m \u001B[38;5;66;03m# 2) Single-batch DataLoader\u001B[39;00m\n\u001B[1;32m    118\u001B[0m check_loader \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataLoader(train_loader, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Cell \u001B[0;32mIn[4], line 65\u001B[0m, in \u001B[0;36mTestBERT.__init__\u001B[0;34m(self, model_name, dropout_rate)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28msuper\u001B[39m(TestBERT, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[1;32m     64\u001B[0m \u001B[38;5;66;03m# 1. Load pretrained BERT\u001B[39;00m\n\u001B[0;32m---> 65\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbert \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;66;03m# Enable gradient checkpointing if desired\u001B[39;00m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbert\u001B[38;5;241m.\u001B[39mgradient_checkpointing_enable()\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    559\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m    560\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    561\u001B[0m     )\n\u001B[1;32m    562\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m--> 563\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m \u001B[43m_get_model_class\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_model_mapping\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    564\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m    565\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    566\u001B[0m     )\n\u001B[1;32m    567\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    568\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    570\u001B[0m )\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:388\u001B[0m, in \u001B[0;36m_get_model_class\u001B[0;34m(config, model_mapping)\u001B[0m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_model_class\u001B[39m(config, model_mapping):\n\u001B[0;32m--> 388\u001B[0m     supported_models \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_mapping\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(supported_models, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[1;32m    390\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m supported_models\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:763\u001B[0m, in \u001B[0;36m_LazyAutoMapping.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    761\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_type \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping:\n\u001B[1;32m    762\u001B[0m     model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping[model_type]\n\u001B[0;32m--> 763\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_attr_from_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    765\u001B[0m \u001B[38;5;66;03m# Maybe there was several model types associated with this config.\u001B[39;00m\n\u001B[1;32m    766\u001B[0m model_types \u001B[38;5;241m=\u001B[39m [k \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_config_mapping\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m v \u001B[38;5;241m==\u001B[39m key\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m]\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:777\u001B[0m, in \u001B[0;36m_LazyAutoMapping._load_attr_from_module\u001B[0;34m(self, model_type, attr)\u001B[0m\n\u001B[1;32m    775\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m module_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules:\n\u001B[1;32m    776\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules[module_name] \u001B[38;5;241m=\u001B[39m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransformers.models\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 777\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgetattribute_from_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_modules\u001B[49m\u001B[43m[\u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattr\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:693\u001B[0m, in \u001B[0;36mgetattribute_from_module\u001B[0;34m(module, attr)\u001B[0m\n\u001B[1;32m    691\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(attr, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    692\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(getattribute_from_module(module, a) \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m attr)\n\u001B[0;32m--> 693\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mhasattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattr\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    694\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(module, attr)\n\u001B[1;32m    695\u001B[0m \u001B[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001B[39;00m\n\u001B[1;32m    696\u001B[0m \u001B[38;5;66;03m# object at the top level.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py:1781\u001B[0m, in \u001B[0;36m_LazyModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1779\u001B[0m     value \u001B[38;5;241m=\u001B[39m Placeholder\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m-> 1781\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_class_to_module\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1782\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(module, name)\n\u001B[1;32m   1783\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py:1795\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[0;34m(self, module_name)\u001B[0m\n\u001B[1;32m   1793\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m module_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m   1794\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m-> 1795\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   1796\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to import \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m because of the following error (look up to see its\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1797\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m traceback):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1798\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nArtifact name: 'trace_shape_events' not registered,please call register_artifact('trace_shape_events') in torch._logging.registrations."
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------\n",
    "#  Full Notebook Code with ONLY Mixed Precision (fp16), No DeepSpeed\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# 1) Standard imports\n",
    "import importlib\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# Append the parent directory to the path to import the necessary modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import the utilities and the dataloader\n",
    "from utils import trainutil, inferutil, setuputil\n",
    "\n",
    "# Now reload the modules to ensure they are up-to-date\n",
    "importlib.reload(setuputil)\n",
    "importlib.reload(trainutil)\n",
    "importlib.reload(inferutil)\n",
    "\n",
    "# Import the funcs needed from utils\n",
    "from utils.setuputil import setup_bert_config, display_bert_config\n",
    "from utils.trainutil import train_model\n",
    "from utils.inferutil import infer_one, infer_full\n",
    "\n",
    "# Define the input config file\n",
    "setup_config = {\n",
    "    \"model_name\": \"prajjwal1/bert-tiny\",\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"threads\": 8,\n",
    "    \"seed\": 0,\n",
    "    \"data_dir\": \"../../data/farzan/\",\n",
    "    \"data_ds\": \"manual\",\n",
    "    \"rows\": 100,\n",
    "    \"cols\": 100,\n",
    "    \"tokens\": 32\n",
    "}\n",
    "\n",
    "# Get the actual to use config file and view\n",
    "config = setup_bert_config(setup_config)\n",
    "display_bert_config(config)\n",
    "\n",
    "# Define local variables as per the variables from config\n",
    "DEVICE = config['DEVICE']\n",
    "THREADS = config['THREADS']\n",
    "train_loader = config['train_loader']\n",
    "val_loader = config['val_loader']\n",
    "test_loader = config['test_loader']\n",
    "model_name = config['model_name']\n",
    "tokenizer = config['tokenizer']\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class TestBERT(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-cased\", dropout_rate=0.05):\n",
    "        super(TestBERT, self).__init__()\n",
    "\n",
    "        # 1. Load pretrained BERT\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Enable gradient checkpointing if desired\n",
    "        self.bert.gradient_checkpointing_enable()\n",
    "\n",
    "        # 2. Define a dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # 3. Non-linear activation (GELU)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        # 4. Final predictor (1-dim output per cell)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        # 1) Allocate the (batch_size, rows, cols) S_cube\n",
    "        S_cube = torch.zeros(\n",
    "            (input_ids.shape[0], input_ids.shape[1], input_ids.shape[2]),\n",
    "            device=input_ids.device,\n",
    "        )\n",
    "\n",
    "        # 2) Loop over cells in row-major order\n",
    "        for cell in tqdm(\n",
    "            range(input_ids.shape[1] * input_ids.shape[2]), desc=\"Forward\"\n",
    "        ):\n",
    "\n",
    "            # cell // input_ids.shape[2] = row, cell % input_ids.shape[2] = col\n",
    "            S_cube[\n",
    "                :, cell // input_ids.shape[2], cell % input_ids.shape[2]\n",
    "            ] = (\n",
    "                self.classifier(\n",
    "                    self.gelu(\n",
    "                        self.dropout(\n",
    "                            self.bert(\n",
    "                                input_ids[:, cell // input_ids.shape[2],\n",
    "                                          cell % input_ids.shape[2], :],\n",
    "                                attention_mask=attention_mask[:, cell // input_ids.shape[2],\n",
    "                                                              cell % input_ids.shape[2], :]\n",
    "                            ).pooler_output\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                .view(-1)\n",
    "            )\n",
    "\n",
    "        return S_cube\n",
    "\n",
    "\n",
    "# 1) Create model and move to GPU\n",
    "untrained_model = TestBERT(model_name=model_name).to(DEVICE)\n",
    "\n",
    "# 2) Single-batch DataLoader\n",
    "check_loader = torch.utils.data.DataLoader(train_loader, batch_size=1, shuffle=False)\n",
    "batch = next(iter(check_loader))\n",
    "\n",
    "ex_xtok = batch[\"x_tok\"].to(DEVICE)\n",
    "ex_xmask = batch[\"x_masks\"].to(DEVICE)\n",
    "\n",
    "# 3) FP16 forward pass with torch.cuda.amp\n",
    "with autocast():\n",
    "    out = untrained_model.forward(ex_xtok, ex_xmask)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83f013c9-3af7-4ab0-8d0f-0132df57e78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward: 100%|███████████████████████████| 10000/10000 [00:24<00:00, 409.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f452d771-1242-48df-aac1-2165a709deec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
