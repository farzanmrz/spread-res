{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d65851d-3622-4eec-8475-e76f99e0d9c4",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74ae2fc8-f6d6-473c-b023-aeaeb617270a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import importlib to reload modules and sys and os to add the path for other imports\n",
    "import importlib\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Append the parent directory to the path to import the necessary modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import the utilities and the dataloader\n",
    "from utils import trainutil, inferutil, setuputil\n",
    "from classes import GeluAvgEmbed\n",
    "\n",
    "# Now reload the modules to ensure they are up-to-date\n",
    "importlib.reload(setuputil)\n",
    "importlib.reload(trainutil)\n",
    "importlib.reload(inferutil)\n",
    "#importlib.reload(GeluAvgEmbed)\n",
    "\n",
    "# Import the funcs needed from utils\n",
    "from utils.setuputil import setup_bert_config, display_bert_config\n",
    "from utils.trainutil import train_model\n",
    "from utils.inferutil import infer_one, infer_full\n",
    "\n",
    "# Import the model class\n",
    "#from classes.GeluAvgEmbed import GeluAvgEmbed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb8749f-c28d-46a8-8712-595caf4c967e",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e002e5a6-33ca-45bc-85ea-825b9bc56713",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0420de1ad5d648288326e311af8c1bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968340cf75284908b8852c6a90c38f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|                                                 | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  \n",
      "Processing files: 100%|████████████████████████████████████████| 40/40 [00:23<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "40(P) = 40(G) + 0(E)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████████████████████████████████████| 5/5 [00:00<00:00,  8.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5(P) = 5(G) + 0(E)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████████████████████████████████████| 5/5 [00:00<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5(P) = 5(G) + 0(E)\n",
      "\n",
      "Final BERT configuration:\n",
      "{\n",
      "  \"model_name\": \"prajjwal1/bert-tiny\",\n",
      "  \"data_dir\": \"../../data/farzan/\",\n",
      "  \"DEVICE\": \"cuda:0\",\n",
      "  \"THREADS\": 8,\n",
      "  \"data_ds\": \"manual\",\n",
      "  \"train_dir\": \"../../data/farzan/manual_train\",\n",
      "  \"val_dir\": \"../../data/farzan/manual_val\",\n",
      "  \"test_dir\": \"../../data/farzan/manual_test\",\n",
      "  \"rows\": 100,\n",
      "  \"cols\": 100,\n",
      "  \"tokens\": 32,\n",
      "  \"tokenizer\": \"<ModernBert Tokenizer Object>\",\n",
      "  \"train_loader\": \"<Train BertLoader Object>\",\n",
      "  \"val_loader\": \"<Validation BertLoader Object>\",\n",
      "  \"test_loader\": \"<Test BertLoader Object>\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Define the input config file\n",
    "setup_config = {\n",
    "    \"model_name\": \"prajjwal1/bert-tiny\",\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"threads\": 8,\n",
    "    \"seed\": 0,\n",
    "    \"data_dir\": \"../../data/farzan/\",\n",
    "    \"data_ds\": \"manual\",\n",
    "    \"rows\": 100,\n",
    "    \"cols\": 100,\n",
    "    \"tokens\": 32\n",
    "}\n",
    "\n",
    "# Get the actual to use config file and view\n",
    "config = setup_bert_config(setup_config)\n",
    "display_bert_config(config)\n",
    "\n",
    "# Define local variables as per the variables from config\n",
    "DEVICE = config['DEVICE']\n",
    "THREADS = config['THREADS']\n",
    "train_loader = config['train_loader']\n",
    "val_loader = config['val_loader']\n",
    "test_loader = config['test_loader']\n",
    "model_name = config['model_name']\n",
    "tokenizer = config['tokenizer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4febea-dc13-4a94-8c5c-25f8f15a791b",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c1330ae-16f9-49e9-9564-23d652b4d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class TestBERT(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-cased\", dropout_rate=0.05):\n",
    "        super(TestBERT, self).__init__()\n",
    "\n",
    "        # 1. Load pretrained BERT\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # 2. Define a dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # 3. Non-linear activation (GELU)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        # 4. Final predictor (1-dim output per cell)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        # 1) Allocate the (batch_size, rows, cols) S_cube\n",
    "        S_cube = torch.zeros(\n",
    "            (input_ids.shape[0], input_ids.shape[1], input_ids.shape[2]),\n",
    "            device=input_ids.device,\n",
    "        )\n",
    "\n",
    "        # 2) Loop over cells in row-major order\n",
    "        for cell in tqdm(\n",
    "            range(input_ids.shape[1] * input_ids.shape[2]), desc=\"Forward\"\n",
    "        ):\n",
    "\n",
    "            # In one shot, store logits → S_cube\n",
    "            # cell // input_ids.shape[2] = row, cell % input_ids.shape[2] = col\n",
    "            S_cube[\n",
    "                :, cell // input_ids.shape[2], cell % input_ids.shape[2]\n",
    "            ] = self.classifier(\n",
    "                self.gelu(\n",
    "                    self.dropout(\n",
    "                        self.bert(\n",
    "                            input_ids[\n",
    "                                :,\n",
    "                                cell // input_ids.shape[2],\n",
    "                                cell % input_ids.shape[2],\n",
    "                                :,\n",
    "                            ],\n",
    "                            attention_mask=attention_mask[\n",
    "                                :,\n",
    "                                cell // input_ids.shape[2],\n",
    "                                cell % input_ids.shape[2],\n",
    "                                :,\n",
    "                            ],\n",
    "                        ).pooler_output\n",
    "                    )\n",
    "                )\n",
    "            ).view(\n",
    "                -1\n",
    "            )\n",
    "\n",
    "        return S_cube\n",
    "\n",
    "\n",
    "# class TestBERT(nn.Module):\n",
    "#     def __init__(self, model_name=\"bert-base-cased\", dropout_rate=0.05):\n",
    "#         super(TestBERT, self).__init__()\n",
    "\n",
    "#         # 1. Load pretrained BERT\n",
    "#         self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "#         # 2. Define a dropout\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "#         # 3. Non-linear activation (GELU)\n",
    "#         self.gelu = nn.GELU()\n",
    "\n",
    "#         # 4. Final predictor (1-dim output per cell)\n",
    "#         self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "\n",
    "#         # 1. Print the overall shapes\n",
    "#         # print(\"batch_size:\", input_ids.shape[0])\n",
    "#         # print(\"rows:\",      input_ids.shape[1])\n",
    "#         # print(\"cols:\",      input_ids.shape[2])\n",
    "#         # print(\"tokens:\",    input_ids.shape[3])\n",
    "\n",
    "#         # 2. Initialize S_cube => (batch_size, rows, cols)\n",
    "#         S_cube = torch.zeros(\n",
    "#             (input_ids.shape[0], input_ids.shape[1], input_ids.shape[2]),\n",
    "#             device=input_ids.device\n",
    "#         )\n",
    "\n",
    "#         # 3. Loop over all cells\n",
    "#         for cell in tqdm(range(input_ids.shape[1] * input_ids.shape[2]), desc = 'Forward'):\n",
    "\n",
    "#             r = cell // input_ids.shape[2]\n",
    "#             c = cell %  input_ids.shape[2]\n",
    "\n",
    "#             # Extract the slice for current cell (batch_size x tokens)\n",
    "#             cell_input_ids  = input_ids[:, r, c, :]\n",
    "#             cell_attn_mask  = attention_mask[:, r, c, :]\n",
    "\n",
    "#             # Pass them through the BERT model\n",
    "#             outputs = self.bert(cell_input_ids, attention_mask=cell_attn_mask)\n",
    "\n",
    "#             # pooler_out => (batch_size, hidden_dim)\n",
    "#             pooler_out = outputs.pooler_output\n",
    "\n",
    "#             # Inlined pipeline: dropout -> GELU -> classifier => (batch_size, 1)\n",
    "#             logits = self.classifier(self.gelu(self.dropout(pooler_out)))\n",
    "\n",
    "#             # Flatten (batch_size, 1) => (batch_size,)\n",
    "#             logits_flat = logits.view(-1)\n",
    "\n",
    "#             # Populate S_cube => shape: (batch_size, rows, cols)\n",
    "#             S_cube[:, r, c] = logits_flat\n",
    "\n",
    "#             # If this is the first cell, do some prints and break\n",
    "#             if r == 0 and c == 0:\n",
    "#                 print(f\"\\nFirst cell => row={r}, col={c}\")\n",
    "#                 print(f\"cell_input_ids.shape: {cell_input_ids.shape}\")\n",
    "#                 print(f\"cell_attn_mask.shape: {cell_attn_mask.shape}\")\n",
    "#                 print(f\"logits.shape: {logits.shape}\")\n",
    "#                 print(f\"logits_flat.shape: {logits_flat.shape}\")\n",
    "#                 print(f\"S_cube[:, {r}, {c}].shape: {S_cube[:, r, c].shape}\")\n",
    "\n",
    "#                 break  # Stop after the first cell\n",
    "\n",
    "#         # 4. Print the shape of S_cube\n",
    "#         # print(f\"\\nS_cube.shape: {S_cube.shape}\")\n",
    "\n",
    "#         # Return S_cube or None, depending on your use case\n",
    "#         return S_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fc45c9c-b142-4f61-8f8b-50982c2e62ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|████████████████████████████████████████| 40/40 [00:22<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "40(P) = 40(G) + 0(E)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████████████████████████████████████| 5/5 [00:00<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5(P) = 5(G) + 0(E)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████████████████████████████████████| 5/5 [00:00<00:00, 11.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5(P) = 5(G) + 0(E)\n",
      "\n",
      "Final BERT configuration:\n",
      "{\n",
      "  \"model_name\": \"prajjwal1/bert-tiny\",\n",
      "  \"data_dir\": \"../../data/farzan/\",\n",
      "  \"DEVICE\": \"cuda:0\",\n",
      "  \"THREADS\": 8,\n",
      "  \"data_ds\": \"manual\",\n",
      "  \"train_dir\": \"../../data/farzan/manual_train\",\n",
      "  \"val_dir\": \"../../data/farzan/manual_val\",\n",
      "  \"test_dir\": \"../../data/farzan/manual_test\",\n",
      "  \"rows\": 100,\n",
      "  \"cols\": 100,\n",
      "  \"tokens\": 32,\n",
      "  \"tokenizer\": \"<ModernBert Tokenizer Object>\",\n",
      "  \"train_loader\": \"<Train BertLoader Object>\",\n",
      "  \"val_loader\": \"<Validation BertLoader Object>\",\n",
      "  \"test_loader\": \"<Test BertLoader Object>\"\n",
      "}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nArtifact name: 'trace_shape_events' not registered,please call register_artifact('trace_shape_events') in torch._logging.registrations.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py:1793\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:25\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:15\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraceback\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfx_traceback\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_aot_autograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_fun\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree_map\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/functional_utils.py:19\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_subclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeta_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_sparse_any\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_shapes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m definitely_true, sym_eq\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreductions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StorageWeakRef\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _config \u001b[38;5;28;01mas\u001b[39;00m config\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecording\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     52\u001b[0m     FakeTensorMeta,\n\u001b[1;32m     53\u001b[0m     ShapeEnvEvent,\n\u001b[1;32m     54\u001b[0m     record_shapeenv_event,\n\u001b[1;32m     55\u001b[0m     replay_shape_env_events,\n\u001b[1;32m     56\u001b[0m     shape_env_check_state_equal\n\u001b[1;32m     57\u001b[0m )\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msym_node\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SymNode, SymTypes\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/fx/experimental/recording.py:14\u001b[0m\n\u001b[1;32m     13\u001b[0m log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m trace_shape_events_log \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_logging\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetArtifactLogger\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrace_shape_events\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShapeEnvEvent\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecord_shapeenv_event\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNotEqualError\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_logging/_internal.py:532\u001b[0m, in \u001b[0;36mgetArtifactLogger\u001b[0;34m(module_qname, artifact_name)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mValueError\u001b[0m: Artifact name: 'trace_shape_events' not registered,please call register_artifact('trace_shape_events') in torch._logging.registrations.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 115\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m S_cube\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# 1) Create model and move to GPU\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m untrained_model \u001b[38;5;241m=\u001b[39m \u001b[43mTestBERT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# 2) Single-batch DataLoader\u001b[39;00m\n\u001b[1;32m    118\u001b[0m check_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train_loader, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[4], line 65\u001b[0m, in \u001b[0;36mTestBERT.__init__\u001b[0;34m(self, model_name, dropout_rate)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28msuper\u001b[39m(TestBERT, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# 1. Load pretrained BERT\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Enable gradient checkpointing if desired\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert\u001b[38;5;241m.\u001b[39mgradient_checkpointing_enable()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    560\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    561\u001b[0m     )\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m \u001b[43m_get_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    566\u001b[0m     )\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:388\u001b[0m, in \u001b[0;36m_get_model_class\u001b[0;34m(config, model_mapping)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_model_class\u001b[39m(config, model_mapping):\n\u001b[0;32m--> 388\u001b[0m     supported_models \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(supported_models, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m supported_models\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:763\u001b[0m, in \u001b[0;36m_LazyAutoMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping:\n\u001b[1;32m    762\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping[model_type]\n\u001b[0;32m--> 763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;66;03m# Maybe there was several model types associated with this config.\u001b[39;00m\n\u001b[1;32m    766\u001b[0m model_types \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;241m==\u001b[39m key\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:777\u001b[0m, in \u001b[0;36m_LazyAutoMapping._load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[1;32m    776\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[module_name] \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 777\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:693\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attr, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(getattribute_from_module(module, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attr)\n\u001b[0;32m--> 693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr)\n\u001b[1;32m    695\u001b[0m \u001b[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;66;03m# object at the top level.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py:1781\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1779\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1781\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1782\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py:1795\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1795\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1797\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1798\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nArtifact name: 'trace_shape_events' not registered,please call register_artifact('trace_shape_events') in torch._logging.registrations."
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------\n",
    "#  Full Notebook Code with ONLY Mixed Precision (fp16), No DeepSpeed\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# 1) Standard imports\n",
    "import importlib\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# Append the parent directory to the path to import the necessary modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import the utilities and the dataloader\n",
    "from utils import trainutil, inferutil, setuputil\n",
    "\n",
    "# Now reload the modules to ensure they are up-to-date\n",
    "importlib.reload(setuputil)\n",
    "importlib.reload(trainutil)\n",
    "importlib.reload(inferutil)\n",
    "\n",
    "# Import the funcs needed from utils\n",
    "from utils.setuputil import setup_bert_config, display_bert_config\n",
    "from utils.trainutil import train_model\n",
    "from utils.inferutil import infer_one, infer_full\n",
    "\n",
    "# Define the input config file\n",
    "setup_config = {\n",
    "    \"model_name\": \"prajjwal1/bert-tiny\",\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"threads\": 8,\n",
    "    \"seed\": 0,\n",
    "    \"data_dir\": \"../../data/farzan/\",\n",
    "    \"data_ds\": \"manual\",\n",
    "    \"rows\": 100,\n",
    "    \"cols\": 100,\n",
    "    \"tokens\": 32\n",
    "}\n",
    "\n",
    "# Get the actual to use config file and view\n",
    "config = setup_bert_config(setup_config)\n",
    "display_bert_config(config)\n",
    "\n",
    "# Define local variables as per the variables from config\n",
    "DEVICE = config['DEVICE']\n",
    "THREADS = config['THREADS']\n",
    "train_loader = config['train_loader']\n",
    "val_loader = config['val_loader']\n",
    "test_loader = config['test_loader']\n",
    "model_name = config['model_name']\n",
    "tokenizer = config['tokenizer']\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class TestBERT(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-cased\", dropout_rate=0.05):\n",
    "        super(TestBERT, self).__init__()\n",
    "\n",
    "        # 1. Load pretrained BERT\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Enable gradient checkpointing if desired\n",
    "        self.bert.gradient_checkpointing_enable()\n",
    "\n",
    "        # 2. Define a dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # 3. Non-linear activation (GELU)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        # 4. Final predictor (1-dim output per cell)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        # 1) Allocate the (batch_size, rows, cols) S_cube\n",
    "        S_cube = torch.zeros(\n",
    "            (input_ids.shape[0], input_ids.shape[1], input_ids.shape[2]),\n",
    "            device=input_ids.device,\n",
    "        )\n",
    "\n",
    "        # 2) Loop over cells in row-major order\n",
    "        for cell in tqdm(\n",
    "            range(input_ids.shape[1] * input_ids.shape[2]), desc=\"Forward\"\n",
    "        ):\n",
    "\n",
    "            # cell // input_ids.shape[2] = row, cell % input_ids.shape[2] = col\n",
    "            S_cube[\n",
    "                :, cell // input_ids.shape[2], cell % input_ids.shape[2]\n",
    "            ] = (\n",
    "                self.classifier(\n",
    "                    self.gelu(\n",
    "                        self.dropout(\n",
    "                            self.bert(\n",
    "                                input_ids[:, cell // input_ids.shape[2],\n",
    "                                          cell % input_ids.shape[2], :],\n",
    "                                attention_mask=attention_mask[:, cell // input_ids.shape[2],\n",
    "                                                              cell % input_ids.shape[2], :]\n",
    "                            ).pooler_output\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                .view(-1)\n",
    "            )\n",
    "\n",
    "        return S_cube\n",
    "\n",
    "\n",
    "# 1) Create model and move to GPU\n",
    "untrained_model = TestBERT(model_name=model_name).to(DEVICE)\n",
    "\n",
    "# 2) Single-batch DataLoader\n",
    "check_loader = torch.utils.data.DataLoader(train_loader, batch_size=1, shuffle=False)\n",
    "batch = next(iter(check_loader))\n",
    "\n",
    "ex_xtok = batch[\"x_tok\"].to(DEVICE)\n",
    "ex_xmask = batch[\"x_masks\"].to(DEVICE)\n",
    "\n",
    "# 3) FP16 forward pass with torch.cuda.amp\n",
    "with autocast():\n",
    "    out = untrained_model.forward(ex_xtok, ex_xmask)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83f013c9-3af7-4ab0-8d0f-0132df57e78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward: 100%|███████████████████████████| 10000/10000 [00:24<00:00, 409.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f452d771-1242-48df-aac1-2165a709deec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
