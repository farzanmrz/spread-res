{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import importlib to reload modules and sys and os to add the path for other imports\n",
    "import importlib\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Append the parent directory to the path to import the necessary modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import the utilities and the dataloader\n",
    "from utils import trainutil, inferutil, setuputil\n",
    "\n",
    "# Now reload the modules to ensure they are up-to-date\n",
    "importlib.reload(setuputil)\n",
    "importlib.reload(trainutil)\n",
    "importlib.reload(inferutil)\n",
    "\n",
    "# Import the funcs needed from utils\n",
    "from utils.setuputil import setup_config, display_config\n",
    "from utils.trainutil import train_model\n",
    "from utils.inferutil import infer_one, infer_full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the input config file\n",
    "input_config = {\n",
    "    # Environment and Model Info\n",
    "    \"env\": \"gcp\",                \n",
    "    \"approach\": \"bert\",         \n",
    "    \"model_name\": \"TestBert\",     \n",
    "    \"model_base\": \"prajjwal1/bert-tiny\",  \n",
    "    \n",
    "    # System Configuration\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"threads\": 14,\n",
    "    \"seed\": 42,\n",
    "    \n",
    "    # Data Configuration\n",
    "    \"data_dir\": \"../../data/farzan\",\n",
    "    \"data_ds\": \"manual\",\n",
    "    \n",
    "    # Model Parameters\n",
    "    \"rows\": 100,\n",
    "    \"cols\": 100,\n",
    "    \"tokens\": 32,\n",
    "    \n",
    "    # Training Parameters\n",
    "    \"batch\": 2,\n",
    "    \"lr\": 1e-4,\n",
    "    \"mu\": 0.25,\n",
    "    \"epochs\": 3,\n",
    "    \"patience\": 2,\n",
    "    \"save_int\": 0,\n",
    "    \"save_dir\": '../models/'\n",
    "}\n",
    "config = setup_config(input_config)\n",
    "display_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define local variables from the config dictionary\n",
    "DEVICE = config[\"DEVICE\"]\n",
    "THREADS = config[\"THREADS\"]\n",
    "gber42_TinyBert_manual_100x100x32\n",
    "# Data loaders and vocab\n",
    "train_loader = config[\"train_loader\"]\n",
    "val_loader = config[\"val_loader\"]\n",
    "test_loader = config[\"test_loader\"]\n",
    "tokenizer = config[\"tokenizer\"]\n",
    "model_base = config['model_base']\n",
    "\n",
    "# Training parameters\n",
    "batch_size = config[\"batch\"]\n",
    "lr = config[\"lr\"]\n",
    "mu = config[\"mu\"]\n",
    "epochs = config[\"epochs\"]\n",
    "patience = config[\"patience\"]\n",
    "save_int = config[\"save_int\"]\n",
    "save_dir = config[\"save_dir\"]\n",
    "save_name = config[\"save_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Checker Code for Loader Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Check\n",
    "# # Retrieve the first item's data from train_loader\n",
    "# first_item = train_loader[0]\n",
    "\n",
    "# # Extract tensors and file path\n",
    "# x_tok = first_item['x_tok']\n",
    "# x_masks = first_item['x_masks']\n",
    "# y_tok = first_item['y_tok']\n",
    "# file_path = first_item['file_paths']\n",
    "\n",
    "# # Print the file path first\n",
    "# print(\"File path:\", file_path)\n",
    "\n",
    "# # Print the shapes of the tensors\n",
    "# print(\"Shape of x_tok:\", x_tok.shape)\n",
    "# print(\"Shape of x_masks:\", x_masks.shape)\n",
    "# print(\"Shape of y_tok:\", y_tok.shape)\n",
    "\n",
    "# # Define cell location\n",
    "# row = 3\n",
    "# col = 5\n",
    "\n",
    "# # Extract data for the specific cell at (row, col)\n",
    "# xtok_cell = x_tok[row, col, :]  # Tokenized input IDs for the cell\n",
    "# xmask_cell = x_masks[row, col, :] if x_masks.numel() > 0 else None  # Attention mask for the cell (if applicable)\n",
    "# ytok_cell = y_tok[row, col, :]  # Metadata tensor for the cell\n",
    "\n",
    "\n",
    "# # Print extracted cell data\n",
    "# print(f\"\\nx_tok at cell ({row}, {col}):\\n\", xtok_cell.tolist())\n",
    "# if xmask_cell is not None:\n",
    "#     print(f\"\\nx_masks at cell ({row}, {col}):\\n\", xmask_cell.tolist())\n",
    "# print(f\"\\ny_tok at cell ({row}, {col}):\\n\", ytok_cell.tolist())\n",
    "\n",
    "# # Decode x_tok of the cell into a list of words using the tokenizer\n",
    "# decoded_words = tokenizer.decode(xtok_cell.tolist(), skip_special_tokens=False).split()\n",
    "# print(f\"\\nDecoded words at cell ({row}, {col}):\\n\", decoded_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Imports\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import AutoModel\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Test model using tinybert for us\n",
    "# class BertTiny(nn.Module):\n",
    "#     def __init__(self, model_base=\"prajjwal1/bert-tiny\", dropout_rate=0.05):\n",
    "#         super(BertTiny, self).__init__()\n",
    "\n",
    "#         # 1. Load pretrained BERT\n",
    "#         self.bert = AutoModel.from_pretrained(model_base)\n",
    "\n",
    "#         # 2. Define a dropout\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "#         # 3. Non-linear activation (GELU)\n",
    "#         self.gelu = nn.GELU()\n",
    "\n",
    "#         # 4. Final predictor (1-dim output per cell)\n",
    "#         self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "\n",
    "#         # 1. Print the overall shapes\n",
    "#         # print(\"batch_size:\", input_ids.shape[0])\n",
    "#         # print(\"rows:\",      input_ids.shape[1])\n",
    "#         # print(\"cols:\",      input_ids.shape[2])\n",
    "#         # print(\"tokens:\",    input_ids.shape[3])\n",
    "\n",
    "#         # 2. Initialize S_cube => (batch_size, rows, cols)\n",
    "#         S_cube = torch.zeros(\n",
    "#             (input_ids.shape[0], input_ids.shape[1], input_ids.shape[2]),\n",
    "#             device=input_ids.device\n",
    "#         )\n",
    "\n",
    "#         # 3. Loop over all cells\n",
    "#         for cell in tqdm(range(input_ids.shape[1] * input_ids.shape[2]), desc = 'Forward'):\n",
    "\n",
    "#             r = cell // input_ids.shape[2]\n",
    "#             c = cell %  input_ids.shape[2]\n",
    "\n",
    "#             # Extract the slice for current cell (batch_size x tokens)\n",
    "#             cell_input_ids  = input_ids[:, r, c, :]\n",
    "#             cell_attn_mask  = attention_mask[:, r, c, :]\n",
    "\n",
    "#             # Pass them through the BERT model\n",
    "#             outputs = self.bert(cell_input_ids, attention_mask=cell_attn_mask)\n",
    "\n",
    "#             # pooler_out => (batch_size, hidden_dim)\n",
    "#             pooler_out = outputs.pooler_output\n",
    "\n",
    "#             # Inlined pipeline: dropout -> GELU -> classifier => (batch_size, 1)\n",
    "#             logits = self.classifier(self.gelu(self.dropout(pooler_out)))\n",
    "\n",
    "#             # Flatten (batch_size, 1) => (batch_size,)\n",
    "#             logits_flat = logits.view(-1)\n",
    "\n",
    "#             # Populate S_cube => shape: (batch_size, rows, cols)\n",
    "#             S_cube[:, r, c] = logits_flat\n",
    "\n",
    "#             # If this is the first cell, do some prints and break\n",
    "#             # if r == 0 and c == 0:\n",
    "#             #     print(f\"\\nFirst cell => row={r}, col={c}\")\n",
    "#             #     print(f\"cell_input_ids.shape: {cell_input_ids.shape}\")\n",
    "#             #     print(f\"cell_attn_mask.shape: {cell_attn_mask.shape}\")\n",
    "#             #     print(f\"logits.shape: {logits.shape}\")\n",
    "#             #     print(f\"logits_flat.shape: {logits_flat.shape}\")\n",
    "#             #     print(f\"S_cube[:, {r}, {c}].shape: {S_cube[:, r, c].shape}\")\n",
    "\n",
    "#                 #break  # Stop after the first cell\n",
    "\n",
    "#         # 4. Print the shape of S_cube\n",
    "#         # print(f\"\\nS_cube.shape: {S_cube.shape}\")\n",
    "\n",
    "#         # Return S_cube or None, depending on your use case\n",
    "#         return S_cube\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class BertTiny(nn.Module):\n",
    "    def __init__(self, model_base=\"bert-base-cased\", dropout_rate=0.05):\n",
    "        super(BertTiny, self).__init__()\n",
    "\n",
    "        # 1. Load pretrained BERT\n",
    "        self.bert = AutoModel.from_pretrained(model_base)\n",
    "\n",
    "        # 2. Define a dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # 3. Non-linear activation (GELU)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        # 4. Final predictor (1-dim output per cell)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        # 1) Allocate the (batch_size, rows, cols) S_cube\n",
    "        S_cube = torch.zeros(\n",
    "            (input_ids.shape[0], input_ids.shape[1], input_ids.shape[2]),\n",
    "            device=input_ids.device,\n",
    "        )\n",
    "\n",
    "        # 2) Loop over cells in row-major order\n",
    "        for cell in range(input_ids.shape[1] * input_ids.shape[2]):\n",
    "\n",
    "            # In one shot, store logits â†’ S_cube\n",
    "            # cell // input_ids.shape[2] = row, cell % input_ids.shape[2] = col\n",
    "            S_cube[\n",
    "                :, cell // input_ids.shape[2], cell % input_ids.shape[2]\n",
    "            ] = self.classifier(\n",
    "                self.gelu(\n",
    "                    self.dropout(\n",
    "                        self.bert(\n",
    "                            input_ids[\n",
    "                                :,\n",
    "                                cell // input_ids.shape[2],\n",
    "                                cell % input_ids.shape[2],\n",
    "                                :,\n",
    "                            ],\n",
    "                            attention_mask=attention_mask[\n",
    "                                :,\n",
    "                                cell // input_ids.shape[2],\n",
    "                                cell % input_ids.shape[2],\n",
    "                                :,\n",
    "                            ],\n",
    "                        ).pooler_output\n",
    "                    )\n",
    "                )\n",
    "            ).view(\n",
    "                -1\n",
    "            )\n",
    "\n",
    "        return S_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1) Create model and move to GPU Observe its architecture\n",
    "untrained_model = BertTiny(model_base=model_base).to(DEVICE)\n",
    "print(untrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2) Single-batch DataLoader\n",
    "# check_loader = torch.utils.data.DataLoader(train_loader, batch_size=2, shuffle=False)\n",
    "# batch = next(iter(check_loader))\n",
    "\n",
    "# ex_xtok = batch[\"x_tok\"].to(DEVICE)\n",
    "# ex_xmask = batch[\"x_masks\"].to(DEVICE)\n",
    "\n",
    "# # Forward pass for a single batch\n",
    "# output = untrained_model(input_ids=ex_xtok, attention_mask=ex_xmask)\n",
    "\n",
    "# # Print the output shape\n",
    "# print(\"Output shape (S_cube):\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os  # For file and directory operations\n",
    "import time  # For generating the timestamp in filenames\n",
    "import torch  # Core PyTorch library\n",
    "import torch.nn as nn  # For defining loss functions\n",
    "import math  # For calculating exponential in perplexity calculation\n",
    "from tqdm import tqdm  # For progress bars in training and validation loops\n",
    "import sys\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Define a new function to train the BertTiny model using attention masks\n",
    "def train_bert(model, train_data, val_data, DEVICE, batch_size=8, lr=1.4e-5, mu=0.25, max_epochs=4, patience=3, save_int=2, save_dir='../models/', save_name='bert_', config=None):\n",
    "    \n",
    "    # --------------------------------------------------------------------\n",
    "    # Everything remains the same up until we get to the forward pass. \n",
    "    # We still set up logging, create train_loader, val_loader, define loss, etc.\n",
    "    # --------------------------------------------------------------------\n",
    "    \n",
    "    # Set the option in torch to print full tensor\n",
    "    torch.set_printoptions(profile=\"full\")\n",
    "    \n",
    "    # Check if save_int > 0 and save_dir exists\n",
    "    if save_int > 0 and not os.path.exists(save_dir):\n",
    "        raise ValueError(f\"Directory '{save_dir}' DNE\")\n",
    "    \n",
    "    # Generate timestamp for naming checkpoints and logs\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Construct checkpoint paths\n",
    "    model_path = os.path.join(save_dir, f\"{save_name}_{timestamp}.pth\")\n",
    "    log_file = os.path.join(save_dir, f\"{save_name}_{timestamp}.txt\")\n",
    "    \n",
    "    # Write config to log if provided (and remove non-serializable items)\n",
    "    if config is not None and save_int > 0:\n",
    "        import json\n",
    "        import copy\n",
    "        \n",
    "        config_serializable = copy.deepcopy(config)\n",
    "        del config_serializable[\"DEVICE\"]\n",
    "        del config_serializable[\"train_loader\"]\n",
    "        del config_serializable[\"val_loader\"]\n",
    "        del config_serializable[\"test_loader\"]\n",
    "        \n",
    "        with open(log_file, 'w') as log:\n",
    "            log.write(\"\\nFinal configuration:\\n\")\n",
    "            log.write(json.dumps(config_serializable, indent=2))\n",
    "            log.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "    \n",
    "    # --------------------------------------------------------------------\n",
    "    # Create optimizer as before\n",
    "    opt = torch.optim.Adagrad(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Create the DataLoader for train and validation sets\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Calculate class imbalance as before\n",
    "    num_bold_cells = sum((batch['y_tok'][:, :, :, 6] == 1).sum() for batch in train_loader)\n",
    "    num_nonbold_cells = sum((batch['y_tok'][:, :, :, 6] == 0).sum() for batch in train_loader)\n",
    "    class_imbalance = num_nonbold_cells / num_bold_cells\n",
    "    \n",
    "    # Binary cross-entropy loss with logits\n",
    "    loss_fn = nn.BCEWithLogitsLoss(\n",
    "        pos_weight=torch.tensor([class_imbalance], dtype=torch.float).to(DEVICE)\n",
    "    )\n",
    "    \n",
    "    # Initialize training parameters\n",
    "    epoch = 0\n",
    "    best_avgtrloss = float('inf')\n",
    "    best_perp = float('inf')\n",
    "    best_epoch = 0\n",
    "    best_avgvalloss = float('inf')\n",
    "    best_valperp = float('inf')\n",
    "    nimp_ctr = 0\n",
    "    training = True\n",
    "    \n",
    "    # --------------------------------------------------------------------\n",
    "    # Main training loop\n",
    "    # --------------------------------------------------------------------\n",
    "    while training and (epoch < max_epochs):\n",
    "        \n",
    "        print(f'Epoch {epoch}')\n",
    "        if save_int > 0:\n",
    "            with open(log_file, 'a') as log:\n",
    "                log.write(f\"\\nEpoch {epoch}\\n\")\n",
    "        \n",
    "        curr_trloss, curr_valloss = 0, 0\n",
    "        \n",
    "        # Put model in train mode\n",
    "        model.train()\n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # Train step\n",
    "        # ----------------------------------------------------------------\n",
    "        for i, batch in enumerate(tqdm(train_loader, desc='Batch Processing')):\n",
    "            \n",
    "            # Zero out gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # ----------------------------------------------------------------\n",
    "            # CHANGED LINE: Now pass both input_ids and attention_mask to model\n",
    "            logits = model(\n",
    "                batch['x_tok'].to(DEVICE),\n",
    "                batch['x_masks'].to(DEVICE)   # <--- Pass attention_mask here\n",
    "            ).view(-1)\n",
    "            \n",
    "            # ----------------------------------------------------------------\n",
    "            # Same as original: define labels\n",
    "            labels = batch['y_tok'][:, :, :, 6].to(DEVICE).view(-1).float()\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(logits, labels)\n",
    "            \n",
    "            # Accumulate training loss\n",
    "            curr_trloss += loss.detach().cpu().item()\n",
    "            \n",
    "            # Backprop\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=mu)\n",
    "            \n",
    "            # Update model parameters\n",
    "            opt.step()\n",
    "            \n",
    "            # Clear memory\n",
    "            del loss\n",
    "        \n",
    "        # Put model in eval mode\n",
    "        model.eval()\n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # Validation step\n",
    "        # ----------------------------------------------------------------\n",
    "        for i, batch in enumerate(tqdm(val_loader, desc='Validation Processing')):\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # ----------------------------------------------------------------\n",
    "                # CHANGED LINE: Pass both input_ids and attention_mask to model\n",
    "                val_logits = model(\n",
    "                    batch['x_tok'].to(DEVICE),\n",
    "                    batch['x_masks'].to(DEVICE)  # <--- Pass attention_mask here\n",
    "                ).view(-1)\n",
    "                \n",
    "                # Labels remain the same\n",
    "                val_labels = batch['y_tok'][:, :, :, 6].to(DEVICE).view(-1).float()\n",
    "                \n",
    "                # Compute validation loss\n",
    "                val_loss = loss_fn(val_logits, val_labels)\n",
    "                \n",
    "                curr_valloss += val_loss.detach().cpu().item()\n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # Same perplexity calculations as original\n",
    "        # ----------------------------------------------------------------\n",
    "        curr_avgtrloss = curr_trloss / len(train_loader)\n",
    "        curr_perp = math.exp(curr_trloss / (len(train_loader) * batch_size * 2500))\n",
    "        curr_avgvalloss = curr_valloss / len(val_loader)\n",
    "        curr_valperp = math.exp(curr_valloss / (len(val_loader) * batch_size * 2500))\n",
    "        \n",
    "        # Print stats\n",
    "        print(f'Train Loss: {curr_avgtrloss}, Perplexity: {curr_perp}')\n",
    "        print(f'Val Loss: {curr_avgvalloss}, Perplexity: {curr_valperp}\\n')\n",
    "        if save_int > 0:\n",
    "            with open(log_file, 'a') as log:\n",
    "                log.write(f'Train Loss: {curr_avgtrloss}, Perplexity: {curr_perp}\\n')\n",
    "                log.write(f'Val Loss: {curr_avgvalloss}, Perplexity: {curr_valperp}\\n')\n",
    "        \n",
    "        # Early stopping checks\n",
    "        if curr_valperp < best_valperp:\n",
    "            best_perp = curr_perp\n",
    "            best_valperp = curr_valperp\n",
    "            best_avgtrloss = curr_avgtrloss\n",
    "            best_avgvalloss = curr_avgvalloss\n",
    "            best_epoch = epoch\n",
    "            nimp_ctr = 0\n",
    "        else:\n",
    "            nimp_ctr += 1\n",
    "        \n",
    "        if nimp_ctr >= patience:\n",
    "            print(f'\\nEARLY STOPPING at epoch {epoch}, best epoch {best_epoch}')\n",
    "            print(f'Train Loss = {best_avgtrloss}, Perplexity = {best_perp}')\n",
    "            print(f'Val Loss = {best_avgvalloss}, Perplexity = {best_valperp}')\n",
    "            if save_int > 0:\n",
    "                with open(log_file, 'a') as log:\n",
    "                    log.write(f'\\nEARLY STOPPING at epoch {epoch}, best epoch {best_epoch}\\n')\n",
    "                    log.write(f'Train Loss = {best_avgtrloss}, Perplexity = {best_perp}\\n')\n",
    "                    log.write(f'Val Loss = {best_avgvalloss}, Perplexity = {best_valperp}\\n')\n",
    "            training = False\n",
    "        \n",
    "        # Save model periodically\n",
    "        if save_int > 0 and (epoch + 1) % save_int == 0:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(\"Model Saved\")\n",
    "            with open(log_file, 'a') as log:\n",
    "                log.write(\"Model Saved\\n\")\n",
    "        \n",
    "        epoch += 1\n",
    "        print()\n",
    "    \n",
    "    # Final save\n",
    "    if save_int > 0:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # Print final results\n",
    "    print(f'\\nTRAINING DONE at epoch {epoch-1}, best epoch {best_epoch}')\n",
    "    print(f'Train Loss = {best_avgtrloss}, Perplexity = {best_perp}')\n",
    "    print(f'Val Loss = {best_avgvalloss}, Perplexity = {best_valperp}')\n",
    "    if save_int > 0:\n",
    "        with open(log_file, 'a') as log:\n",
    "            log.write(f'\\nTRAINING DONE at epoch {epoch-1}, best epoch {best_epoch}\\n')\n",
    "            log.write(f'Train Loss = {best_avgtrloss}, Perplexity = {best_perp}\\n')\n",
    "            log.write(f'Val Loss = {best_avgvalloss}, Perplexity = {best_valperp}\\n')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Call the train_bert function with the loaded model and config hyperparameters\n",
    "trained_model = train_bert(\n",
    "    untrained_model,                  # Pass the BertTiny model\n",
    "    train_loader,   # Training dataset\n",
    "    val_loader,     # Validation dataset\n",
    "    DEVICE,                 # Device for computation (CPU/GPU)\n",
    "    batch_size=batch_size,  # Batch size from config\n",
    "    lr=lr,                  # Learning rate from config\n",
    "    mu=mu,                  # Gradient clipping max norm from config\n",
    "    max_epochs=epochs,      # Maximum number of epochs from config\n",
    "    patience=patience,      # Early stopping patience\n",
    "    save_int=save_int,      # Interval at which to save model\n",
    "    save_dir=save_dir,      # Directory path to save checkpoints\n",
    "    save_name=save_name,    # Base name used for saving checkpoints/logs\n",
    "    config=config           # Full config for logging\n",
    ")\n",
    "\n",
    "\n",
    "# # Call the train_bert function with the loaded model and config hyperparameters\n",
    "# trained_model = train_bert(\n",
    "#     untrained_model,                  # Pass the BertTiny model\n",
    "#     train_loader,   # Training dataset\n",
    "#     val_loader,     # Validation dataset\n",
    "#     DEVICE,                 # Device for computation (CPU/GPU)\n",
    "#     batch_size=2,  # Batch size from config\n",
    "#     lr=1e-5,                  # Learning rate from config\n",
    "#     mu=mu,                  # Gradient clipping max norm from config\n",
    "#     max_epochs=4,      # Maximum number of epochs from config\n",
    "#     patience=2,      # Early stopping patience\n",
    "#     save_int=0,      # Interval at which to save model\n",
    "#     save_dir=save_dir,      # Directory path to save checkpoints\n",
    "#     save_name=save_name,    # Base name used for saving checkpoints/logs\n",
    "#     config=config           # Full config for logging\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import inferutil\n",
    "importlib.reload(inferutil)\n",
    "from utils.inferutil import binfer_one\n",
    "\n",
    "# Params\n",
    "loc = 0\n",
    "thresh = 0.6\n",
    "cond = '>'\n",
    "disp_max=True\n",
    "\n",
    "# inference on single position of train loader params\n",
    "binfer_one(\n",
    "    trained_model,\n",
    "    train_loader,\n",
    "    loc=loc,\n",
    "    threshold=thresh,\n",
    "    condition=cond,\n",
    "    disp_max=disp_max,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "binfer_one(\n",
    "    trained_model,\n",
    "    val_loader,\n",
    "    loc=loc,\n",
    "    threshold=thresh,\n",
    "    condition=cond,\n",
    "    disp_max=disp_max,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
