
Final configuration:
{
  "env": "local",
  "approach": "simple",
  "model_name": "SimpleGeluEmbedAvg",
  "data_dir": "../../data/farzan",
  "data_ds": "manual",
  "seed": 42,
  "THREADS": 12,
  "vocab_size": 5597,
  "vocab_space": true,
  "vocab_case": "both",
  "train_dir": "../../data/farzan/manual_train",
  "val_dir": "../../data/farzan/manual_val",
  "test_dir": "../../data/farzan/manual_test",
  "rows": 100,
  "cols": 100,
  "tokens": 32,
  "batch": 40,
  "lr": 0.1,
  "mu": 0.25,
  "epochs": 20,
  "patience": 2,
  "save_int": 10,
  "save_dir": "../models/",
  "save_name": "local_simple_SimpleGeluEmbedAvg_s42__manual_100x100x32__bSp5k__b40lr1e-1e20p2"
}

================================================================================


Epoch 0
Train Loss: 1.77257239818573, Perplexity: 1.0000177258810834
Val Loss: 0.6548740267753601, Perplexity: 1.0000065487617107

Epoch 1
Train Loss: 0.6655376553535461, Perplexity: 1.0000066553987006
Val Loss: 0.4066166281700134, Perplexity: 1.0000040661745486

Epoch 2
Train Loss: 0.40872201323509216, Perplexity: 1.000004087228485
Val Loss: 0.3377818167209625, Perplexity: 1.000003377823872

Epoch 3
Train Loss: 0.3398129642009735, Perplexity: 1.0000033981354157
Val Loss: 0.34999874234199524, Perplexity: 1.0000034999935483

Epoch 4
Train Loss: 0.34511300921440125, Perplexity: 1.0000034511360474
Val Loss: 0.3129536509513855, Perplexity: 1.0000031295414065

Epoch 5
Train Loss: 0.3140209913253784, Perplexity: 1.0000031402148437
Val Loss: 0.3154143989086151, Perplexity: 1.0000031541489633

Epoch 6
Train Loss: 0.31196776032447815, Perplexity: 1.0000031196824695
Val Loss: 0.2869951128959656, Perplexity: 1.0000028699552472

Epoch 7
Train Loss: 0.28805768489837646, Perplexity: 1.000002880580998
Val Loss: 0.28063681721687317, Perplexity: 1.00000280637211

Epoch 8
Train Loss: 0.2790367901325226, Perplexity: 1.0000027903717945
Val Loss: 0.26742202043533325, Perplexity: 1.00000267422378

Epoch 9
Train Loss: 0.2687374949455261, Perplexity: 1.0000026873785604
Val Loss: 0.2604251801967621, Perplexity: 1.000002604255193
Model Saved

Epoch 10
Train Loss: 0.26110780239105225, Perplexity: 1.0000026110814328
Val Loss: 0.25038865208625793, Perplexity: 1.0000025038896556

Epoch 11
Train Loss: 0.25309497117996216, Perplexity: 1.0000025309529146
Val Loss: 0.24344007670879364, Perplexity: 1.0000024344037302

Epoch 12
Train Loss: 0.24591268599033356, Perplexity: 1.0000024591298835
Val Loss: 0.23437660932540894, Perplexity: 1.00000234376884

Epoch 13
Train Loss: 0.2394736409187317, Perplexity: 1.0000023947392767
Val Loss: 0.2278386801481247, Perplexity: 1.000002278389397

Epoch 14
Train Loss: 0.23135265707969666, Perplexity: 1.000002313529247
Val Loss: 0.21909460425376892, Perplexity: 1.0000021909484427

Epoch 15
Train Loss: 0.22481927275657654, Perplexity: 1.0000022481952549
Val Loss: 0.21295155584812164, Perplexity: 1.0000021295178259

Epoch 16
Train Loss: 0.21807877719402313, Perplexity: 1.0000021807901498
Val Loss: 0.20443807542324066, Perplexity: 1.000002044382844

Epoch 17
Train Loss: 0.21201667189598083, Perplexity: 1.0000021201689666
Val Loss: 0.19923624396324158, Perplexity: 1.0000019923644243

Epoch 18
Train Loss: 0.2060946375131607, Perplexity: 1.000002060948499
Val Loss: 0.19068101048469543, Perplexity: 1.0000019068119228

Epoch 19
Train Loss: 0.19849054515361786, Perplexity: 1.0000019849074215
Val Loss: 0.18611028790473938, Perplexity: 1.0000018611046109
Model Saved

TRAINING DONE at epoch 19, best epoch 19
Train Loss = 0.19849054515361786, Perplexity = 1.0000019849074215
Val Loss = 0.18611028790473938, Perplexity = 1.0000018611046109
