{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2009a09e-44ff-4c6f-91e2-60772199654b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2634c25a-5709-4a83-b41f-d063aeab2008",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "821e2590-98b0-47b7-a7df-c42a91b33d39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import importlib to reload modules and sys and os to add the path for other imports\n",
    "import importlib\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Append the parent directory to the path to import the necessary modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import utilities\n",
    "from utils import setuputil, trainutil, inferutil\n",
    "from classes.models import SimpleGeluEmbed\n",
    "\n",
    "# Reload the necessary modules to ensure they are up-to-date\n",
    "importlib.reload(setuputil)\n",
    "importlib.reload(trainutil)\n",
    "importlib.reload(inferutil)\n",
    "importlib.reload(SimpleGeluEmbed)\n",
    "\n",
    "# Import the required utils\n",
    "from utils.setuputil import setup_simple_config, display_simple_config\n",
    "from utils.trainutil import train_model\n",
    "from utils.inferutil import infer_one, infer_full\n",
    "\n",
    "# Import the SimpleGeluEmbedAdd class\n",
    "from classes.models.SimpleGeluEmbed import SimpleGeluEmbedAdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f28913e1-e9ac-4a7b-a3eb-8b85eff4c78e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define setup config\n",
    "setup_config = {\n",
    "    # Environment and Model Info\n",
    "    \"env\": \"local\",                \n",
    "    \"approach\": \"simple\",         \n",
    "    \"model_name\": \"SimpleGeluEmbedAdd\",\n",
    "    \n",
    "    # System Configuration\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"threads\": 12,\n",
    "    \"seed\": 42,\n",
    "    \n",
    "    # Data Configuration\n",
    "    \"data_dir\": \"../../data/farzan\",\n",
    "    \"data_ds\": 2000,\n",
    "    \n",
    "    # Model Parameters\n",
    "    \"rows\": 100,\n",
    "    \"cols\": 100,\n",
    "    \"tokens\": 32,\n",
    "    \n",
    "    # Vocabulary Parameters\n",
    "    \"vocab_size\": 150000,\n",
    "    \"vocab_space\": True,\n",
    "    \"vocab_case\": \"both\",\n",
    "    \n",
    "    # Training Parameters\n",
    "    \"batch\": 40,\n",
    "    \"lr\": 5e-3,\n",
    "    \"mu\": 0.25,\n",
    "    \"epochs\": 20,\n",
    "    \"patience\": 2,\n",
    "    \"save_int\": 10,\n",
    "    \"save_dir\": '../models/'\n",
    "}\n",
    "\n",
    "# Define setup config\n",
    "setup_config2 = {\n",
    "    # Environment and Model Info\n",
    "    \"env\": \"local\",                \n",
    "    \"approach\": \"simple\",         \n",
    "    \"model_name\": \"SimpleGeluEmbedAdd\",\n",
    "    \n",
    "    # System Configuration\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"threads\": 12,\n",
    "    \"seed\": 0,\n",
    "    \n",
    "    # Data Configuration\n",
    "    \"data_dir\": \"../../data/farzan\",\n",
    "    \"data_ds\": 2000,\n",
    "    \n",
    "    # Model Parameters\n",
    "    \"rows\": 100,\n",
    "    \"cols\": 100,\n",
    "    \"tokens\": 32,\n",
    "    \n",
    "    # Vocabulary Parameters\n",
    "    \"vocab_size\": 150000,\n",
    "    \"vocab_space\": True,\n",
    "    \"vocab_case\": \"both\",\n",
    "    \n",
    "    # Training Parameters\n",
    "    \"batch\": 40,\n",
    "    \"lr\": 5e-3,\n",
    "    \"mu\": 0.25,\n",
    "    \"epochs\": 20,\n",
    "    \"patience\": 2,\n",
    "    \"save_int\": 10,\n",
    "    \"save_dir\": '../models/'\n",
    "}\n",
    "\n",
    "# Define setup config\n",
    "setup_config3 = {\n",
    "    # Environment and Model Info\n",
    "    \"env\": \"local\",                \n",
    "    \"approach\": \"simple\",         \n",
    "    \"model_name\": \"SimpleGeluEmbedAdd\",\n",
    "    \n",
    "    # System Configuration\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"threads\": 12,\n",
    "    \"seed\": 0,\n",
    "    \n",
    "    # Data Configuration\n",
    "    \"data_dir\": \"../../data/farzan\",\n",
    "    \"data_ds\": 1000,\n",
    "    \n",
    "    # Model Parameters\n",
    "    \"rows\": 100,\n",
    "    \"cols\": 100,\n",
    "    \"tokens\": 32,\n",
    "    \n",
    "    # Vocabulary Parameters\n",
    "    \"vocab_size\": 150000,\n",
    "    \"vocab_space\": True,\n",
    "    \"vocab_case\": \"both\",\n",
    "    \n",
    "    # Training Parameters\n",
    "    \"batch\": 40,\n",
    "    \"lr\": 5e-3,\n",
    "    \"mu\": 0.25,\n",
    "    \"epochs\": 20,\n",
    "    \"patience\": 2,\n",
    "    \"save_int\": 10,\n",
    "    \"save_dir\": '../models/'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a36e61f-9ecf-4f11-8252-1729c88a8038",
   "metadata": {},
   "source": [
    "# Derive Part to use form the simple_setup func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ce3bee1-8a23-491f-849f-9dcd20e68b74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from utils.selfutil import set_seed, get_vocab, create_embeddings, get_fileList\n",
    "from classes.SpreadsheetDataLoader import SpreadsheetDataLoader\n",
    "\n",
    "def setup_simple_config(setup_config):\n",
    "    \"\"\"\n",
    "    Function to set up the configuration for a simple model, including:\n",
    "    - Seed initialization\n",
    "    - Vocabulary and embedding matrix setup\n",
    "    - Dynamic dataloader creation based on `data_ds`\n",
    "\n",
    "    Args:\n",
    "        setup_config (dict): Configuration dictionary with required keys.\n",
    "\n",
    "    Returns:\n",
    "        dict: Fully configured dictionary for the model setup.\n",
    "    \"\"\"\n",
    "\n",
    "    ######## INITIALIZE CONFIG ########\n",
    "    config = {}\n",
    "\n",
    "    ######## ENVIRONMENT & MODEL INFO ########\n",
    "    valid_envs = [\"gcp\", \"bvm\", \"local\", \"colab\"]\n",
    "    if setup_config[\"env\"] not in valid_envs:\n",
    "        raise ValueError(f\"ERR: env must be one of {valid_envs}\")\n",
    "    config[\"env\"] = setup_config[\"env\"]\n",
    "\n",
    "    valid_approaches = [\"simple\", \"saffu\", \"bert\"]\n",
    "    if setup_config[\"approach\"] not in valid_approaches:\n",
    "        raise ValueError(f\"ERR: approach must be one of {valid_approaches}\")\n",
    "    config[\"approach\"] = setup_config[\"approach\"]\n",
    "\n",
    "    config[\"model_name\"] = setup_config[\"model_name\"]\n",
    "\n",
    "    ######## DATA DIR ########\n",
    "    # Validate and set data_dir\n",
    "    if os.path.isdir(setup_config[\"data_dir\"]):\n",
    "        config[\"data_dir\"] = setup_config[\"data_dir\"]\n",
    "    else:\n",
    "        raise ValueError(f\"ERR: data_dir '{setup_config['data_dir']}' is not a valid path\")\n",
    "\n",
    "    ######## DEVICE ########\n",
    "    if (\n",
    "        setup_config[\"device\"].startswith(\"cuda\")\n",
    "        and torch.cuda.is_available()\n",
    "        and int(setup_config[\"device\"].split(\":\")[1]) < torch.cuda.device_count()\n",
    "    ):\n",
    "        config[\"DEVICE\"] = torch.device(setup_config[\"device\"])\n",
    "    elif (\n",
    "        setup_config[\"device\"].startswith(\"mps\")\n",
    "        and hasattr(torch.backends, \"mps\")\n",
    "        and torch.backends.mps.is_available()\n",
    "    ):\n",
    "        config[\"DEVICE\"] = torch.device(\"mps\")\n",
    "    else:\n",
    "        config[\"DEVICE\"] = torch.device(\"cpu\")\n",
    "    config[\"seed\"] = setup_config[\"seed\"]\n",
    "\n",
    "    ######## THREADS ########\n",
    "    if not isinstance(setup_config[\"threads\"], (int, float)):\n",
    "        raise ValueError(\"ERR: threads must be a number\")\n",
    "    if os.cpu_count() - int(setup_config[\"threads\"]) < 4:\n",
    "        raise ValueError(\n",
    "            f\"ERR: Using {int(setup_config['threads'])} threads would leave insufficient free threads.\"\n",
    "        )\n",
    "    config[\"THREADS\"] = max(1, int(setup_config[\"threads\"]))\n",
    "\n",
    "    ######## SEED ########\n",
    "    set_seed(config[\"seed\"])\n",
    "\n",
    "    ######## DATALOADER SETUP ########\n",
    "    train_dir = os.path.join(config[\"data_dir\"], \"all_train\")\n",
    "    val_dir = os.path.join(config[\"data_dir\"], \"all_val\")\n",
    "    test_dir = os.path.join(config[\"data_dir\"], \"all_test\")\n",
    "\n",
    "    data_ds = setup_config[\"data_ds\"]\n",
    "    if isinstance(data_ds, str) and data_ds in [\"manual\", \"all\"]:\n",
    "        # Setup directories for train, val, and test\n",
    "        config[\"train_dir\"] = os.path.join(config[\"data_dir\"], f\"{data_ds}_train\")\n",
    "        config[\"val_dir\"] = os.path.join(config[\"data_dir\"], f\"{data_ds}_val\")\n",
    "        config[\"test_dir\"] = os.path.join(config[\"data_dir\"], f\"{data_ds}_test\")\n",
    "\n",
    "        train_files, _ = get_fileList(config[\"train_dir\"])\n",
    "        val_files, _ = get_fileList(config[\"val_dir\"])\n",
    "        test_files, _ = get_fileList(config[\"test_dir\"])\n",
    "    elif isinstance(data_ds, int):\n",
    "        # Validate that data_ds is less than 2450\n",
    "        if data_ds > 2450:\n",
    "            raise ValueError(f\"ERR: data_ds '{data_ds}' exceeds the maximum allowed files (2450).\")\n",
    "\n",
    "        # Check if data_ds can be split evenly into 80-10-10\n",
    "        train_size = int(data_ds * 0.8)\n",
    "        val_size = int(data_ds * 0.1)\n",
    "        test_size = int(data_ds * 0.1)\n",
    "\n",
    "        # Check for residuals\n",
    "        if train_size + val_size + test_size != data_ds:\n",
    "            raise ValueError(f\"ERR: {data_ds} cannot be evenly split into 80-10-10.\")\n",
    "\n",
    "        # Dynamically select files using get_fileList\n",
    "        train_files, _ = get_fileList(train_dir, num_files=train_size, seed=config[\"seed\"])\n",
    "        val_files, _ = get_fileList(val_dir, num_files=val_size, seed=config[\"seed\"])\n",
    "        test_files, _ = get_fileList(test_dir, num_files=test_size, seed=config[\"seed\"])\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"ERR: data_ds '{data_ds}' must be a valid integer <= 2450 or one of ['manual', 'all']\"\n",
    "        )\n",
    "\n",
    "    ######## VOCAB ########\n",
    "    # Validate vocab parameters\n",
    "    if not isinstance(setup_config[\"vocab_size\"], int) or not 4 <= setup_config[\"vocab_size\"] <= 2000000:\n",
    "        raise ValueError(f\"ERR: vocab_size '{setup_config['vocab_size']}' must be an integer between 4 and 2,000,000\")\n",
    "\n",
    "    vocab_space = setup_config.get(\"vocab_space\", True)\n",
    "    if not isinstance(vocab_space, bool):\n",
    "        vocab_space = True\n",
    "\n",
    "    vocab_case = setup_config.get(\"vocab_case\", \"lower\")\n",
    "    if vocab_case not in [\"both\", \"upper\", \"lower\"]:\n",
    "        vocab_case = \"lower\"\n",
    "\n",
    "    # Generate vocab object using train_files as the source\n",
    "    config[\"vocab\"] = get_vocab(\n",
    "        train_files,\n",
    "        setup_config[\"vocab_size\"],\n",
    "        space=vocab_space,\n",
    "        case=vocab_case,\n",
    "        threads=config[\"THREADS\"]\n",
    "    )\n",
    "\n",
    "    ######## WVS ########\n",
    "    config[\"wvs\"] = create_embeddings(config[\"vocab\"])\n",
    "    config[\"vocab_size\"] = config[\"wvs\"].shape[0]\n",
    "    config[\"vocab_space\"] = vocab_space\n",
    "    config[\"vocab_case\"] = vocab_case\n",
    "\n",
    "    ######## CREATE DATA LOADERS ########\n",
    "    config[\"train_loader\"] = SpreadsheetDataLoader(\n",
    "        train_files, config[\"vocab\"], setup_config[\"rows\"], setup_config[\"cols\"], setup_config[\"tokens\"], threads=config[\"THREADS\"]\n",
    "    )\n",
    "    config[\"val_loader\"] = SpreadsheetDataLoader(\n",
    "        val_files, config[\"vocab\"], setup_config[\"rows\"], setup_config[\"cols\"], setup_config[\"tokens\"], threads=config[\"THREADS\"]\n",
    "    )\n",
    "    config[\"test_loader\"] = SpreadsheetDataLoader(\n",
    "        test_files, config[\"vocab\"], setup_config[\"rows\"], setup_config[\"cols\"], setup_config[\"tokens\"], threads=config[\"THREADS\"]\n",
    "    )\n",
    "\n",
    "    ######## TRAINING PARAMETERS ########\n",
    "    config[\"batch\"] = setup_config[\"batch\"]\n",
    "    config[\"lr\"] = setup_config[\"lr\"]\n",
    "    config[\"mu\"] = setup_config[\"mu\"]\n",
    "    config[\"epochs\"] = setup_config[\"epochs\"]\n",
    "    config[\"patience\"] = setup_config[\"patience\"]\n",
    "    config[\"save_int\"] = setup_config[\"save_int\"]\n",
    "    config[\"save_dir\"] = setup_config[\"save_dir\"]\n",
    "\n",
    "    ######## SAVE NAME ########\n",
    "    case_prefix = {\"both\": \"b\", \"upper\": \"u\", \"lower\": \"l\"}[config[\"vocab_case\"]]\n",
    "    space_str = \"Sp\" if config[\"vocab_space\"] else \"Nsp\"\n",
    "    vocab_str = f\"{case_prefix}{space_str}{config['vocab_size']//1000}k\"\n",
    "\n",
    "    save_name = \"__\".join([\n",
    "        \"_\".join([config[\"env\"], config[\"approach\"], config[\"model_name\"], f\"s{config['seed']}\"]),\n",
    "        \"_\".join([str(config[\"data_ds\"]), f\"{setup_config['rows']}x{setup_config['cols']}x{setup_config['tokens']}\"]),\n",
    "        vocab_str,\n",
    "        f\"b{setup_config['batch']}lr{setup_config['lr']:.0e}e{setup_config['epochs']}p{setup_config['patience']}\"\n",
    "    ])\n",
    "    config[\"save_name\"] = save_name\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18ff5eaf-eb8e-4111-9d6b-171365798c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stat: path should be string, bytes, os.PathLike or integer, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msetup_simple_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetup_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 124\u001b[0m, in \u001b[0;36msetup_simple_config\u001b[0;34m(setup_config)\u001b[0m\n\u001b[1;32m    121\u001b[0m     vocab_case \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Generate vocab object using train_files as the source\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mget_vocab\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43msetup_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvocab_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_case\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTHREADS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m######## WVS ########\u001b[39;00m\n\u001b[1;32m    133\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwvs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m create_embeddings(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/spread-res/files/utils/selfutil.py:283\u001b[0m, in \u001b[0;36mget_vocab\u001b[0;34m(data_dir, vocab_size, space, case, threads)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03mCollects all .xls, .xlsx, and .csv files from the specified directory,\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03mprocesses each file in parallel, counts occurrences of each token based on the specified case,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m    Vocab: A trained Vocab object.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# Get valid and invalid file path lists\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m file_paths, invalid_files \u001b[38;5;241m=\u001b[39m \u001b[43mget_fileList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# Initialize errant files counter from the length of invalid_files at the start and vocab object\u001b[39;00m\n\u001b[1;32m    286\u001b[0m init_total_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(file_paths) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(invalid_files)\n",
      "File \u001b[0;32m~/spread-res/files/utils/selfutil.py:160\u001b[0m, in \u001b[0;36mget_fileList\u001b[0;34m(directory, num_files, seed)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03mRetrieves and classifies files in a directory into valid and invalid files based on extensions,\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03mshuffles the valid files using a seed, and selects a subset if `num_files` is provided.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    ValueError: If the specified directory does not exist.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Ensure the provided directory exists\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m NOT FOUND\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Files with supported extensions\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/genericpath.py:42\u001b[0m, in \u001b[0;36misdir\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return true if the pathname refers to an existing directory.\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not list"
     ]
    }
   ],
   "source": [
    "setup_simple_config(setup_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29100ed5-572d-4848-a5a5-a0e69b44483b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n",
      "Train directory: ../../data/farzan/all_train\n",
      "Validation directory: ../../data/farzan/all_val\n",
      "Test directory: ../../data/farzan/all_test\n",
      "Success: 2000 can be split into 80-10-10.\n",
      "Selected Train files: 1600\n",
      "Selected Validation files: 200\n",
      "Selected Test files: 200\n",
      "\n",
      "First 10 Train files:\n",
      "['../../data/farzan/all_train/GTO_data_Deposit%20Caulobacter%20Biofilms%2007302018.xlsx', '../../data/farzan/all_train/July%202014%20Summary%20and%20Assignment%20Report%20-%20FINAL%20508.xls', '../../data/farzan/all_train/LED%20lamp%20Bg%20high%20and%20low%20RH.xlsx', '../../data/farzan/all_train/2019sipawardsv2.xlsx', '../../data/farzan/all_train/HEROV1_2024_MCRruns.xlsx', '../../data/farzan/all_train/WallaceMichelle_A-6hdz_Data_20170508.xlsx', '../../data/farzan/all_train/idrportfolio-by-age.xls', '../../data/farzan/all_train/RME%20Sensitivity%20Outputs%20MP%201-25-22.xlsx', '../../data/farzan/all_train/darrell_schoolcraft_000_1_1_1.pst.306.xls', '../../data/farzan/all_train/bias_data_for_hclust_time-series_11052017_508.xlsx']\n",
      "\n",
      "First 10 Validation files:\n",
      "['../../data/farzan/all_val/Carbonate%20Chemistry%2C%20water%20quality%20and%20coral%20measurements%2C%203-15-18.xlsx', '../../data/farzan/all_val/arkansas.xls', '../../data/farzan/all_val/Table%202%20and%203%20data%20Machine%20Learning%20Result.xlsx', '../../data/farzan/all_val/lindy_donoho_000_1_1_1.pst.143.xls', '../../data/farzan/all_val/Xiaoyong%20Liao%20Chemosphere%202018%20paper%20microbe%20counts%20%26%20PAH%20concentrations%20719.xlsx', '../../data/farzan/all_val/ndc-codes.xlsx', '../../data/farzan/all_val/Martin%20et%20al%202018_Data%20S1_All%20Data_SciRep_Locked%20.xlsx', '../../data/farzan/all_val/Hg%20lamp%20Aerosol%20vs%20Suspension%20run%201%20glass.xlsx', '../../data/farzan/all_val/pell-table-3-2011-12-a.xls', '../../data/farzan/all_val/Julie%20Simpson%27s%20sheet.xlsx']\n",
      "\n",
      "First 10 Test files:\n",
      "['../../data/farzan/all_test/weldeab2006.xls', '../../data/farzan/all_test/2000-northdakota-projection.xls', '../../data/farzan/all_test/SurveyCalibrationData.xlsx', '../../data/farzan/all_test/MWCNT%2055_30-1.xlsx', '../../data/farzan/all_test/dnc-data-book-fy-13_0.xlsx', '../../data/farzan/all_test/Bg%20Study%20for%20R%20Results.xlsx', '../../data/farzan/all_test/daren_farmer_000_1_1.pst.112.xls', '../../data/farzan/all_test/Figure%208%20metadata.xlsx', '../../data/farzan/all_test/diablo2011.xls', '../../data/farzan/all_test/jim_schwieger_000_1_1.pst.106.xls']\n"
     ]
    }
   ],
   "source": [
    "dynamic_dataloader_setup(setup_config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1cad89b-f2c6-473a-91e5-5e348ebcdc12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n",
      "Train directory: ../../data/farzan/all_train\n",
      "Validation directory: ../../data/farzan/all_val\n",
      "Test directory: ../../data/farzan/all_test\n",
      "Success: 1000 can be split into 80-10-10.\n",
      "Selected Train files: 800\n",
      "Selected Validation files: 100\n",
      "Selected Test files: 100\n",
      "\n",
      "First 10 Train files:\n",
      "['../../data/farzan/all_train/GTO_data_Deposit%20Caulobacter%20Biofilms%2007302018.xlsx', '../../data/farzan/all_train/July%202014%20Summary%20and%20Assignment%20Report%20-%20FINAL%20508.xls', '../../data/farzan/all_train/LED%20lamp%20Bg%20high%20and%20low%20RH.xlsx', '../../data/farzan/all_train/2019sipawardsv2.xlsx', '../../data/farzan/all_train/HEROV1_2024_MCRruns.xlsx', '../../data/farzan/all_train/WallaceMichelle_A-6hdz_Data_20170508.xlsx', '../../data/farzan/all_train/idrportfolio-by-age.xls', '../../data/farzan/all_train/RME%20Sensitivity%20Outputs%20MP%201-25-22.xlsx', '../../data/farzan/all_train/darrell_schoolcraft_000_1_1_1.pst.306.xls', '../../data/farzan/all_train/bias_data_for_hclust_time-series_11052017_508.xlsx']\n",
      "\n",
      "First 10 Validation files:\n",
      "['../../data/farzan/all_val/Carbonate%20Chemistry%2C%20water%20quality%20and%20coral%20measurements%2C%203-15-18.xlsx', '../../data/farzan/all_val/arkansas.xls', '../../data/farzan/all_val/Table%202%20and%203%20data%20Machine%20Learning%20Result.xlsx', '../../data/farzan/all_val/lindy_donoho_000_1_1_1.pst.143.xls', '../../data/farzan/all_val/Xiaoyong%20Liao%20Chemosphere%202018%20paper%20microbe%20counts%20%26%20PAH%20concentrations%20719.xlsx', '../../data/farzan/all_val/ndc-codes.xlsx', '../../data/farzan/all_val/Martin%20et%20al%202018_Data%20S1_All%20Data_SciRep_Locked%20.xlsx', '../../data/farzan/all_val/Hg%20lamp%20Aerosol%20vs%20Suspension%20run%201%20glass.xlsx', '../../data/farzan/all_val/pell-table-3-2011-12-a.xls', '../../data/farzan/all_val/Julie%20Simpson%27s%20sheet.xlsx']\n",
      "\n",
      "First 10 Test files:\n",
      "['../../data/farzan/all_test/weldeab2006.xls', '../../data/farzan/all_test/2000-northdakota-projection.xls', '../../data/farzan/all_test/SurveyCalibrationData.xlsx', '../../data/farzan/all_test/MWCNT%2055_30-1.xlsx', '../../data/farzan/all_test/dnc-data-book-fy-13_0.xlsx', '../../data/farzan/all_test/Bg%20Study%20for%20R%20Results.xlsx', '../../data/farzan/all_test/daren_farmer_000_1_1.pst.112.xls', '../../data/farzan/all_test/Figure%208%20metadata.xlsx', '../../data/farzan/all_test/diablo2011.xls', '../../data/farzan/all_test/jim_schwieger_000_1_1.pst.106.xls']\n"
     ]
    }
   ],
   "source": [
    "dynamic_dataloader_setup(setup_config3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1671cf58-ee17-490f-aed7-06411e338881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define setup config\n",
    "setup_config4 = {\n",
    "    # Environment and Model Info\n",
    "    \"env\": \"local\",                \n",
    "    \"approach\": \"simple\",         \n",
    "    \"model_name\": \"SimpleGeluEmbedAdd\",\n",
    "    \n",
    "    # System Configuration\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"threads\": 12,\n",
    "    \"seed\": 0,\n",
    "    \n",
    "    # Data Configuration\n",
    "    \"data_dir\": \"../../data/farzan\",\n",
    "    \"data_ds\": 'manual',\n",
    "    \n",
    "    # Model Parameters\n",
    "    \"rows\": 100,\n",
    "    \"cols\": 100,\n",
    "    \"tokens\": 32,\n",
    "    \n",
    "    # Vocabulary Parameters\n",
    "    \"vocab_size\": 150000,\n",
    "    \"vocab_space\": True,\n",
    "    \"vocab_case\": \"both\",\n",
    "    \n",
    "    # Training Parameters\n",
    "    \"batch\": 40,\n",
    "    \"lr\": 5e-3,\n",
    "    \"mu\": 0.25,\n",
    "    \"epochs\": 20,\n",
    "    \"patience\": 2,\n",
    "    \"save_int\": 10,\n",
    "    \"save_dir\": '../models/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f36054bb-31db-4f58-aab7-735b75b6328d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n",
      "Train directory: ../../data/farzan/all_train\n",
      "Validation directory: ../../data/farzan/all_val\n",
      "Test directory: ../../data/farzan/all_test\n",
      "Success: data_ds 'manual' is valid.\n",
      "Train files: 40\n",
      "Validation files: 5\n",
      "Test files: 5\n",
      "\n",
      "First 10 Train files:\n",
      "['../../data/farzan/manual_train/advanced-placement-science-enrollment.xlsx', '../../data/farzan/manual_train/portfolio-by-school-type.xls', '../../data/farzan/manual_train/dlportfolio-by-debt-size.xls', '../../data/farzan/manual_train/dlbyforbearancetype copy.xls', '../../data/farzan/manual_train/harassment-bullying-on-basis-of-disability-disciplined copy.xlsx', '../../data/farzan/manual_train/dlbydefermenttype copy.xls', '../../data/farzan/manual_train/portfolio-by-age copy.xls', '../../data/farzan/manual_train/harassment-bullying-on-basis-of-race-disciplined.xlsx', '../../data/farzan/manual_train/advanced-placement-enrollment (1).xlsx', '../../data/farzan/manual_train/advanced-placement-science-enrollment (1) copy.xlsx']\n",
      "\n",
      "First 10 Validation files:\n",
      "['../../data/farzan/manual_val/ffelbydefermenttype.xls', '../../data/farzan/manual_val/portfolio-by-debt-size.xls', '../../data/farzan/manual_val/Harassment-Bullying-on-basis-of-disability_reported.xlsx', '../../data/farzan/manual_val/harassment-bullying-on-basis-of-sex-disciplined.xlsx', '../../data/farzan/manual_val/advanced-placement-mathematics-enrollment.xlsx']\n",
      "\n",
      "First 10 Test files:\n",
      "['../../data/farzan/manual_test/dlportfoliobyrepaymentplan.xls', '../../data/farzan/manual_test/advanced-placement-other-subjects-enrollment (1).xlsx', '../../data/farzan/manual_test/harassment-bullying-on-basis-of-disability-reported.xlsx', '../../data/farzan/manual_test/Harassment-Bullying-on-basis-of-sex_disciplined.xlsx', '../../data/farzan/manual_test/dlportfolio-by-school-type.xls']\n"
     ]
    }
   ],
   "source": [
    "dynamic_dataloader_setup(setup_config4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a5e98d-d59e-4231-8d65-57bb082df8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
