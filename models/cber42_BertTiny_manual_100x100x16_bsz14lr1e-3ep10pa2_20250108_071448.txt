
Final configuration:
{
  "env": "colab",
  "approach": "bert",
  "THREADS": 10,
  "seed": 42,
  "model_base": "prajjwal1/bert-tiny",
  "model_name": "BertTiny",
  "rows": 100,
  "cols": 100,
  "tokens": 16,
  "data_ds": "manual",
  "data_dir": "../data",
  "train_dir": "../data/manual_train",
  "val_dir": "../data/manual_val",
  "test_dir": "../data/manual_test",
  "batch": 14,
  "lr": 0.001,
  "mu": 0.25,
  "epochs": 10,
  "patience": 2,
  "save_int": 5,
  "save_dir": "../models/",
  "save_name": "cber42_BertTiny_manual_100x100x16_bsz14lr1e-3ep10pa2"
}

================================================================================


Epoch 0
Train Loss: 1.1519421537717183, Perplexity: 1.0000329131745915
Val Loss: 0.731855034828186, Perplexity: 1.0000209103624709

Epoch 1
Train Loss: 0.6875163714090983, Perplexity: 1.0000196435178288
Val Loss: 0.4187546372413635, Perplexity: 1.000011964489781

Epoch 2
Train Loss: 0.43405912319819134, Perplexity: 1.0000124017661356
Val Loss: 0.29449328780174255, Perplexity: 1.0000084141293357

Epoch 3
Train Loss: 0.3129208783308665, Perplexity: 1.000008940636491
Val Loss: 0.23815639317035675, Perplexity: 1.0000068044915267

Epoch 4
Train Loss: 0.2540591061115265, Perplexity: 1.0000072588579485
Val Loss: 0.20240826904773712, Perplexity: 1.0000057831101234
Model Saved

Epoch 5
Train Loss: 0.21390034755071005, Perplexity: 1.0000061114571763
Val Loss: 0.1730448454618454, Perplexity: 1.000004944150664

Epoch 6
Train Loss: 0.18254104256629944, Perplexity: 1.0000052154719596
Val Loss: 0.14699922502040863, Perplexity: 1.0000041999866776

Epoch 7
Train Loss: 0.15861471990744272, Perplexity: 1.000004531859409
Val Loss: 0.1320303976535797, Perplexity: 1.000003772304191

Epoch 8
Train Loss: 0.14171247680981955, Perplexity: 1.0000040489361057
Val Loss: 0.12068536877632141, Perplexity: 1.0000034481593385

Epoch 9
Train Loss: 0.12802020708719888, Perplexity: 1.000003657726892
Val Loss: 0.11057222634553909, Perplexity: 1.0000031592114573
Model Saved

TRAINING DONE at epoch 9, best epoch 9
Train Loss = 0.12802020708719888, Perplexity = 1.000003657726892
Val Loss = 0.11057222634553909, Perplexity = 1.0000031592114573
