
Final configuration:
{
  "env": "colab",
  "approach": "bert",
  "THREADS": 10,
  "seed": 0,
  "model_base": "bert-base-cased",
  "model_name": "BertSimple",
  "rows": 100,
  "cols": 100,
  "tokens": 32,
  "data_ds": "manual",
  "data_dir": "../data",
  "train_dir": "../data/manual_train",
  "val_dir": "../data/manual_val",
  "test_dir": "../data/manual_test",
  "vocab_size": 30522,
  "hidden_size": 32,
  "num_hidden_layers": 1,
  "num_attention_heads": 1,
  "intermediate_size": 128,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "max_position_embeddings": 64,
  "type_vocab_size": 2,
  "initializer_range": 0.02,
  "layer_norm_eps": 1e-12,
  "pad_token_id": 0,
  "gradient_checkpointing": false,
  "batch_size": 20,
  "lr": 0.01,
  "mu": 0.25,
  "epochs": 10,
  "patience": 2,
  "save_int": 10,
  "save_dir": "../models/",
  "save_name": "ber0cBertSimple_manualba20lr1e-2ep10pa2_v30Kh32i128l1a1_20250122_025516"
}

================================================================================


Epoch 0
Train Loss: 1.3913556337356567, Perplexity: 1.000006956802367
Val Loss: 1.3667984008789062, Perplexity: 1.000006834015356

Epoch 1
Train Loss: 1.376751720905304, Perplexity: 1.0000068837822977
Val Loss: 1.3074920177459717, Perplexity: 1.000006537481458

Epoch 2
Train Loss: 1.2315647602081299, Perplexity: 1.0000061578427604
Val Loss: 0.8513704538345337, Perplexity: 1.0000042568613297

Epoch 3
Train Loss: 0.8314235508441925, Perplexity: 1.0000041571263951
Val Loss: 0.5750728249549866, Perplexity: 1.0000028753682586

Epoch 4
Train Loss: 0.5837427079677582, Perplexity: 1.0000029187177992
Val Loss: 0.44199296832084656, Perplexity: 1.0000022099672836

Epoch 5
Train Loss: 0.4511537700891495, Perplexity: 1.0000022557713948
Val Loss: 0.3597503900527954, Perplexity: 1.000001798753568

Epoch 6
Train Loss: 0.367121085524559, Perplexity: 1.0000018356071123
Val Loss: 0.3040100634098053, Perplexity: 1.0000015200514722

Epoch 7
Train Loss: 0.3147481232881546, Perplexity: 1.0000015737418548
Val Loss: 0.2640783190727234, Perplexity: 1.0000013203924671

Epoch 8
Train Loss: 0.2736339718103409, Perplexity: 1.000001368170795
Val Loss: 0.23374982178211212, Perplexity: 1.0000011687497918

Epoch 9
Train Loss: 0.24350056052207947, Perplexity: 1.0000012175035438
Val Loss: 0.20963366329669952, Perplexity: 1.0000010481688657
Model Saved

TRAINING DONE at epoch 9, best epoch 9
Train Loss = 0.24350056052207947, Perplexity = 1.0000012175035438
Val Loss = 0.20963366329669952, Perplexity = 1.0000010481688657
