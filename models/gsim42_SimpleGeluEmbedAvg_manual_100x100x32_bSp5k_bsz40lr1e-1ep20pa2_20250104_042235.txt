
Final configuration:
{
  "env": "gcp",
  "approach": "simple",
  "THREADS": 14,
  "seed": 42,
  "model_base": "glove50",
  "model_name": "SimpleGeluEmbedAvg",
  "rows": 100,
  "cols": 100,
  "tokens": 32,
  "data_ds": "manual",
  "data_dir": "../../data/farzan",
  "train_dir": "../../data/farzan/manual_train",
  "val_dir": "../../data/farzan/manual_val",
  "test_dir": "../../data/farzan/manual_test",
  "vocab_size": 5597,
  "vocab_space": true,
  "vocab_case": "both",
  "batch": 40,
  "lr": 0.1,
  "mu": 0.25,
  "epochs": 20,
  "patience": 2,
  "save_int": 10,
  "save_dir": "../models/",
  "save_name": "gsim42_SimpleGeluEmbedAvg_manual_100x100x32_bSp5k_bsz40lr1e-1ep20pa2"
}

================================================================================


Epoch 0
Train Loss: 1.4818493127822876, Perplexity: 1.0000148186029223
Val Loss: 1.6988940238952637, Perplexity: 1.0000169890845518

Epoch 1
Train Loss: 1.7233725786209106, Perplexity: 1.0000172338742876
Val Loss: 1.270524024963379, Perplexity: 1.0000127053209615

Epoch 2
Train Loss: 1.2792435884475708, Perplexity: 1.000012792517708
Val Loss: 1.2971382141113281, Perplexity: 1.0000129714662698

Epoch 3
Train Loss: 1.3141841888427734, Perplexity: 1.0000131419282428
Val Loss: 1.2200517654418945, Perplexity: 1.000012200592081

Epoch 4
Train Loss: 1.2254011631011963, Perplexity: 1.0000122540867118
Val Loss: 1.2002367973327637, Perplexity: 1.000012002440002

Epoch 5
Train Loss: 1.218963384628296, Perplexity: 1.0000121897081402
Val Loss: 1.1791492700576782, Perplexity: 1.0000117915622204

Epoch 6
Train Loss: 1.185173511505127, Perplexity: 1.0000118518053471
Val Loss: 1.1353275775909424, Perplexity: 1.0000113533402246

Epoch 7
Train Loss: 1.1519402265548706, Perplexity: 1.000011519468614
Val Loss: 1.140608310699463, Perplexity: 1.0000114061481566

Epoch 8
Train Loss: 1.1470907926559448, Perplexity: 1.0000114709737176
Val Loss: 1.0838297605514526, Perplexity: 1.00001083835634

Epoch 9
Train Loss: 1.10149347782135, Perplexity: 1.0000110149954429
Val Loss: 1.101728916168213, Perplexity: 1.0000110173498522
Model Saved

Epoch 10
Train Loss: 1.1073951721191406, Perplexity: 1.0000110740130377
Val Loss: 1.0391314029693604, Perplexity: 1.0000103913680196

Epoch 11
Train Loss: 1.0560309886932373, Perplexity: 1.0000105603656473
Val Loss: 1.0618973970413208, Perplexity: 1.000010619030352

Epoch 12
Train Loss: 1.0674270391464233, Perplexity: 1.0000106743273618
Val Loss: 0.9983834028244019, Perplexity: 1.000009983883867

Epoch 13
Train Loss: 1.018102765083313, Perplexity: 1.0000101810794777
Val Loss: 1.0231492519378662, Perplexity: 1.0000102315448613

Epoch 14
Train Loss: 1.0280438661575317, Perplexity: 1.0000102804915054
Val Loss: 0.9614435434341431, Perplexity: 1.0000096144816533

Epoch 15
Train Loss: 0.9814454317092896, Perplexity: 1.000009814502479
Val Loss: 0.9868742227554321, Perplexity: 1.0000098687909238

Epoch 16
Train Loss: 0.9931885004043579, Perplexity: 1.0000099319343254
Val Loss: 0.9275395274162292, Perplexity: 1.0000092754382908

Epoch 17
Train Loss: 0.9474892020225525, Perplexity: 1.000009474936907
Val Loss: 0.9526616930961609, Perplexity: 1.0000095266623092

Epoch 18
Train Loss: 0.9604434370994568, Perplexity: 1.0000096044804938
Val Loss: 0.8964037299156189, Perplexity: 1.0000089640774763

Epoch 19
Train Loss: 0.9165724515914917, Perplexity: 1.0000091657665213
Val Loss: 0.9202924966812134, Perplexity: 1.0000092029673138
Model Saved

TRAINING DONE at epoch 19, best epoch 18
Train Loss = 0.9604434370994568, Perplexity = 1.0000096044804938
Val Loss = 0.8964037299156189, Perplexity = 1.0000089640774763
