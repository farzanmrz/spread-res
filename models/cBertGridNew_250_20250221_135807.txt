
Final configuration:
{
  "env": "colab",
  "approach": "bert",
  "THREADS": 10,
  "seed": 0,
  "model_base": "bert-base-cased",
  "model_name": "BertGridNew",
  "rows": 100,
  "cols": 100,
  "tokens": 32,
  "data_ds": "250",
  "data_dir": "../data",
  "train_dir": "../data/250_train",
  "val_dir": "../data/250_val",
  "test_dir": "../data/250_test",
  "vocab_size": 30522,
  "hidden_size": 32,
  "num_hidden_layers": 1,
  "num_attention_heads": 1,
  "intermediate_size": 128,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "max_position_embeddings": 64,
  "type_vocab_size": 2,
  "initializer_range": 0.02,
  "layer_norm_eps": 1e-12,
  "pad_token_id": 0,
  "gradient_checkpointing": false,
  "batch_size": 40,
  "lr": 0.0025,
  "mu": 0.25,
  "epochs": 50,
  "patience": 1,
  "save_int": 10,
  "save_dir": "../models/",
  "save_name": "cBertGridNew_250"
}

================================================================================


Epoch 0
Train Loss: 1.2348918914794922, Perplexity: 1.0000030872344943
Val Loss: 1.1572517156600952, Perplexity: 1.0000028931334743

Epoch 1
Train Loss: 1.0061039328575134, Perplexity: 1.0000025152629954
Val Loss: 1.0047754049301147, Perplexity: 1.0000025119416673

Epoch 2
Train Loss: 0.8878884553909302, Perplexity: 1.000002219723602
Val Loss: 0.8730052709579468, Perplexity: 1.0000021825155592

Epoch 3
Train Loss: 0.7461556792259216, Perplexity: 1.000001865390938
Val Loss: 0.6871432662010193, Perplexity: 1.000001717859641

Epoch 4
Train Loss: 0.6150600671768188, Perplexity: 1.00000153765135
Val Loss: 0.5576998591423035, Perplexity: 1.0000013942506198

Epoch 5
Train Loss: 0.5408141076564789, Perplexity: 1.0000013520361832
Val Loss: 0.49724066257476807, Perplexity: 1.000001243102429

Epoch 6
Train Loss: 0.4964992940425873, Perplexity: 1.0000012412490054
Val Loss: 0.45525363087654114, Perplexity: 1.000001138134725

Epoch 7
Train Loss: 0.4651851415634155, Perplexity: 1.0000011629635301
Val Loss: 0.4241059720516205, Perplexity: 1.0000010602654923

Epoch 8
Train Loss: 0.4447296977043152, Perplexity: 1.0000011118248624
Val Loss: 0.4004344940185547, Perplexity: 1.0000010010867362

Epoch 9
Train Loss: 0.4242099165916443, Perplexity: 1.0000010605253538
Val Loss: 0.38262444734573364, Perplexity: 1.000000956561576
Model Saved

Epoch 10
Train Loss: 0.4118065655231476, Perplexity: 1.0000010295169437
Val Loss: 0.3676705062389374, Perplexity: 1.000000919176688

Epoch 11
Train Loss: 0.3998233795166016, Perplexity: 1.0000009995589483
Val Loss: 0.35383617877960205, Perplexity: 1.0000008845908381

Epoch 12
Train Loss: 0.38996751308441163, Perplexity: 1.000000974919258
Val Loss: 0.3414028286933899, Perplexity: 1.000000853507436

Epoch 13
Train Loss: 0.38124030232429507, Perplexity: 1.00000095310121
Val Loss: 0.3298434317111969, Perplexity: 1.0000008246089194

Epoch 14
Train Loss: 0.3729521632194519, Perplexity: 1.0000009323808428
Val Loss: 0.3208298683166504, Perplexity: 1.0000008020749924

Epoch 15
Train Loss: 0.3685932785272598, Perplexity: 1.000000921483621
Val Loss: 0.31168699264526367, Perplexity: 1.0000007792177852

Epoch 16
Train Loss: 0.36461885273456573, Perplexity: 1.0000009115475472
Val Loss: 0.31864315271377563, Perplexity: 1.0000007966081992

EARLY STOPPING at epoch 16, best epoch 15
Train Loss = 0.3685932785272598, Perplexity = 1.000000921483621
Val Loss = 0.31168699264526367, Perplexity = 1.0000007792177852

TRAINING DONE at epoch 16, best epoch 15
Train Loss = 0.3685932785272598, Perplexity = 1.000000921483621
Val Loss = 0.31168699264526367, Perplexity = 1.0000007792177852
