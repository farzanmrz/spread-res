
Final configuration:
{
  "env": "bvm",
  "approach": "bert",
  "THREADS": 32,
  "seed": 42,
  "model_base": "bert-base-cased",
  "model_name": "CustomSmallBertBaseCased",
  "rows": 100,
  "cols": 100,
  "tokens": 16,
  "data_ds": "manual",
  "data_dir": "../../../data/farzan",
  "train_dir": "../../../data/farzan/manual_train",
  "val_dir": "../../../data/farzan/manual_val",
  "test_dir": "../../../data/farzan/manual_test",
  "batch": 10,
  "lr": 1e-05,
  "mu": 0.25,
  "epochs": 3,
  "patience": 2,
  "save_int": 0,
  "save_dir": "../models/",
  "save_name": "bber42_CustomSmallBertBaseCased_manual_100x100x16_bsz10lr1e-5ep3pa2"
}

================================================================================


Epoch 0
Train Loss: 1.375616729259491, Perplexity: 1.0000550261830552
Val Loss: 1.3529013395309448, Perplexity: 1.0000541175178812
Model Saved

Epoch 1
Train Loss: 1.3746927976608276, Perplexity: 1.0000549892237585
Val Loss: 1.3528212308883667, Perplexity: 1.000054114313362
Model Saved

Epoch 2
Train Loss: 1.3747049570083618, Perplexity: 1.000054989710159
Val Loss: 1.3527404069900513, Perplexity: 1.0000541110802312
Model Saved

TRAINING DONE at epoch 2, best epoch 2
Train Loss = 1.3747049570083618, Perplexity = 1.000054989710159
Val Loss = 1.3527404069900513, Perplexity = 1.0000541110802312
