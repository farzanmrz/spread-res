
Final configuration:
{
  "env": "colab",
  "approach": "bert",
  "THREADS": 10,
  "seed": 0,
  "model_base": "bert-base-cased",
  "model_name": "BertGrid",
  "rows": 100,
  "cols": 100,
  "tokens": 32,
  "data_ds": "100",
  "data_dir": "../data",
  "train_dir": "../data/100_train",
  "val_dir": "../data/100_val",
  "test_dir": "../data/100_test",
  "vocab_size": 30522,
  "hidden_size": 32,
  "num_hidden_layers": 1,
  "num_attention_heads": 1,
  "intermediate_size": 128,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "max_position_embeddings": 64,
  "type_vocab_size": 2,
  "initializer_range": 0.02,
  "layer_norm_eps": 1e-12,
  "pad_token_id": 0,
  "gradient_checkpointing": false,
  "batch_size": 40,
  "lr": 0.01,
  "mu": 0.25,
  "epochs": 20,
  "patience": 2,
  "save_int": 10,
  "save_dir": "../models/",
  "save_name": "ber0cBertGrid_100ba40lr1e-2ep20pa2_v30Kh32l1i128a1"
}

================================================================================


Epoch 0
Train Loss: 1.3594487309455872, Perplexity: 1.0000033986276027
Val Loss: 1.2925019264221191, Perplexity: 1.0000032312600367

Epoch 1
Train Loss: 1.2312723398208618, Perplexity: 1.000003078185587
Val Loss: 0.8820276260375977, Perplexity: 1.0000022050714963

Epoch 2
Train Loss: 1.027685135602951, Perplexity: 1.0000025692161394
Val Loss: 0.7255550622940063, Perplexity: 1.0000018138893008

Epoch 3
Train Loss: 0.7360943257808685, Perplexity: 1.0000018402375077
Val Loss: 0.4414382576942444, Perplexity: 1.0000011035962533

Epoch 4
Train Loss: 0.4852142930030823, Perplexity: 1.0000012130364682
Val Loss: 0.3552500009536743, Perplexity: 1.0000008881253968

Epoch 5
Train Loss: 0.37018731236457825, Perplexity: 1.0000009254687092
Val Loss: 0.26203271746635437, Perplexity: 1.0000006550820082

Epoch 6
Train Loss: 0.31133443117141724, Perplexity: 1.000000778336381
Val Loss: 0.23808573186397552, Perplexity: 1.0000005952145068

Epoch 7
Train Loss: 0.2718721330165863, Perplexity: 1.0000006796805636
Val Loss: 0.31124576926231384, Perplexity: 1.000000778114726

Epoch 8
Train Loss: 0.23532254993915558, Perplexity: 1.000000588306548
Val Loss: 0.2102290540933609, Perplexity: 1.0000005255727733

Epoch 9
Train Loss: 0.21686191856861115, Perplexity: 1.0000005421549434
Val Loss: 0.6129862070083618, Perplexity: 1.0000015324666918
Model Saved

Epoch 10
Train Loss: 0.18993625044822693, Perplexity: 1.000000474840739
Val Loss: 0.6860796213150024, Perplexity: 1.0000017152005243

EARLY STOPPING at epoch 10, best epoch 8
Train Loss = 0.23532254993915558, Perplexity = 1.000000588306548
Val Loss = 0.2102290540933609, Perplexity = 1.0000005255727733

TRAINING DONE at epoch 10, best epoch 8
Train Loss = 0.23532254993915558, Perplexity = 1.000000588306548
Val Loss = 0.2102290540933609, Perplexity = 1.0000005255727733
