
Final configuration:
{
  "env": "colab",
  "approach": "bert",
  "THREADS": 10,
  "seed": 0,
  "model_base": "prajjwal1/bert-tiny",
  "model_name": "BertPreTiny",
  "rows": 100,
  "cols": 100,
  "tokens": 32,
  "data_ds": "manual",
  "data_dir": "../data",
  "train_dir": "../data/manual_train",
  "val_dir": "../data/manual_val",
  "test_dir": "../data/manual_test",
  "vocab_size": 30522,
  "hidden_size": 32,
  "num_hidden_layers": 1,
  "num_attention_heads": 1,
  "intermediate_size": 128,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "max_position_embeddings": 64,
  "type_vocab_size": 2,
  "initializer_range": 0.02,
  "layer_norm_eps": 1e-12,
  "pad_token_id": 0,
  "gradient_checkpointing": false,
  "batch_size": 5,
  "lr": 0.01,
  "mu": 0.25,
  "epochs": 20,
  "patience": 1,
  "save_int": 10,
  "save_dir": "../models/",
  "save_name": "ber0cBertPreTiny_manualba5lr1e-2ep20pa1_v30Kh32l1i128a1"
}

================================================================================


Epoch 0
Train Loss: 1.2334336265921593, Perplexity: 1.000024668976806
Val Loss: 0.6183295845985413, Perplexity: 1.0000123666681586

Epoch 1
Train Loss: 0.4336048625409603, Perplexity: 1.0000086721348536
Val Loss: 0.2667507231235504, Perplexity: 1.0000053350286937

Epoch 2
Train Loss: 0.2371909935027361, Perplexity: 1.000004743831122
Val Loss: 0.17172913253307343, Perplexity: 1.000003434588549

Epoch 3
Train Loss: 0.16250128857791424, Perplexity: 1.000003250031053
Val Loss: 0.11719363182783127, Perplexity: 1.0000023438753833

Epoch 4
Train Loss: 0.1376194730401039, Perplexity: 1.0000027523932487
Val Loss: 0.08527453243732452, Perplexity: 1.000001705492103

Epoch 5
Train Loss: 0.10103915072977543, Perplexity: 1.0000020207850564
Val Loss: 0.07011839002370834, Perplexity: 1.0000014023687838

Epoch 6
Train Loss: 0.0699339103884995, Perplexity: 1.000001398679186
Val Loss: 0.0534963458776474, Perplexity: 1.00000106992749

Epoch 7
Train Loss: 0.05271597020328045, Perplexity: 1.0000010543199598
Val Loss: 0.042722996324300766, Perplexity: 1.0000008544602916

Epoch 8
Train Loss: 0.04502499848604202, Perplexity: 1.0000009005003752
Val Loss: 0.03650033473968506, Perplexity: 1.0000007300069613

Epoch 9
Train Loss: 0.03707375191152096, Perplexity: 1.0000007414753131
Val Loss: 0.03204108774662018, Perplexity: 1.0000006408219602
Model Saved

Epoch 10
Train Loss: 0.0314075609203428, Perplexity: 1.0000006281514158
Val Loss: 0.028605453670024872, Perplexity: 1.000000572109237

Epoch 11
Train Loss: 0.029747860273346305, Perplexity: 1.0000005949573825
Val Loss: 0.026097573339939117, Perplexity: 1.000000521951603

Epoch 12
Train Loss: 0.026275618467479944, Perplexity: 1.0000005255125075
Val Loss: 0.024070613086223602, Perplexity: 1.0000004814123775

Epoch 13
Train Loss: 0.02225286024622619, Perplexity: 1.000000445057304
Val Loss: 0.022211674600839615, Perplexity: 1.0000004442335906

Epoch 14
Train Loss: 0.02151026693172753, Perplexity: 1.0000004302054313
Val Loss: 0.020904360339045525, Perplexity: 1.0000004180872941

Epoch 15
Train Loss: 0.018693776801228523, Perplexity: 1.0000003738756058
Val Loss: 0.01801743544638157, Perplexity: 1.000000360348774

Epoch 16
Train Loss: 0.01766241912264377, Perplexity: 1.000000353248445
Val Loss: 0.017183637246489525, Perplexity: 1.000000343672804

Epoch 17
Train Loss: 0.016242488753050566, Perplexity: 1.000000324849828
Val Loss: 0.01615097001194954, Perplexity: 1.0000003230194525

Epoch 18
Train Loss: 0.014695772319100797, Perplexity: 1.0000002939154895
Val Loss: 0.015386003069579601, Perplexity: 1.0000003077201087

Epoch 19
Train Loss: 0.013937639887444675, Perplexity: 1.0000002787528366
Val Loss: 0.014724005945026875, Perplexity: 1.0000002944801623
Model Saved

TRAINING DONE at epoch 19, best epoch 19
Train Loss = 0.013937639887444675, Perplexity = 1.0000002787528366
Val Loss = 0.014724005945026875, Perplexity = 1.0000002944801623
