{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Append the parent directory to the path to import the necessary modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import the utilities\n",
    "from utils import selfutil\n",
    "from utils import parseutil \n",
    "\n",
    "# Now reload the modules to ensure they are up-to-date\n",
    "importlib.reload(selfutil)\n",
    "importlib.reload(parseutil)\n",
    "\n",
    "# Import the functions needed from utils\n",
    "from utils.selfutil import get_vocabulary\n",
    "from utils.parseutil import process_spreadsheet\n",
    "\n",
    "# Other regular imports\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import shutil\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "\n",
    "# Setup device as a global constant\n",
    "devstr = \"cuda:1\"  # \"cpu\" \n",
    "gpu = False if (devstr == 'cpu') else True\n",
    "DEVICE = 'cpu' if (devstr == 'cpu') else (torch.device(devstr if torch.cuda.is_available() else 'cpu') if devstr else torch.cuda.current_device())\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Download Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get links from pages and download all XLS files\n",
    "async def download_datagov_xls(start_page=1, end_page=2, data_dir='../data/train_big/', max_size_mb=2):\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Directory '{data_dir}' does not exist. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Initialize an empty list to accumulate links\n",
    "    all_links = []\n",
    "    \n",
    "    # Loop through each page in the specified range with a progress bar\n",
    "    for page_number in tqdm(range(start_page, end_page + 1), desc=\"Getting Links\"):\n",
    "        base_url = f\"https://catalog.data.gov/dataset/?res_format=EXCEL&_res_format_limit=0&_bureauCode_limit=0&page={page_number}\"\n",
    "        # Send a request to the URL and parse the HTML content\n",
    "        soup = BeautifulSoup(requests.get(base_url).content, 'html.parser')\n",
    "        # Add the found links to the accumulated list\n",
    "        all_links.extend([link['href'] for link in soup.find_all('a', href=True) if '.xls' in link['href'].lower()])\n",
    "    \n",
    "    print(f\"Total XLS links found: {len(all_links)}\")\n",
    "    \n",
    "    # Limit concurrency to 5 simultaneous downloads\n",
    "    sem = asyncio.Semaphore(5)\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Create a tqdm progress bar for downloading\n",
    "        with tqdm(total=len(all_links), desc=\"Downloading Files\") as pbar:\n",
    "            # Loop through each link to download\n",
    "            for url in all_links:\n",
    "                async with sem:\n",
    "                    try:\n",
    "                        # Check file size using HEAD request before downloading\n",
    "                        try:\n",
    "                            response = requests.head(url, timeout=1, allow_redirects=True)\n",
    "                            if response.status_code == 200 and 'Content-Length' in response.headers:\n",
    "                                file_size_mb = int(response.headers['Content-Length']) / (1024 * 1024)  # Convert to MB\n",
    "                                if file_size_mb > max_size_mb:\n",
    "                                    # Skip downloading files larger than max_size_mb\n",
    "                                    pbar.update(1)\n",
    "                                    continue\n",
    "                            elif response.status_code == 403:\n",
    "                                # If access to headers is restricted, proceed to download anyway\n",
    "                                pass\n",
    "                            else:\n",
    "                                # Skip if unable to get a valid response for size\n",
    "                                pbar.update(1)\n",
    "                                continue\n",
    "                        except requests.exceptions.RequestException as e:\n",
    "                            # Handle any exceptions from the HEAD request\n",
    "                            pbar.update(1)\n",
    "                            continue\n",
    "\n",
    "                        # Combine directory and filename\n",
    "                        filename = os.path.join(data_dir, url.split('/')[-1])\n",
    "\n",
    "                        # Make a request to download the file\n",
    "                        async with session.get(url, timeout=1, allow_redirects=True) as response:\n",
    "                            # If the response is successful, write the file\n",
    "                            if response.status == 200:\n",
    "                                with open(filename, 'wb') as f:\n",
    "                                    f.write(await response.read())\n",
    "                            # Update progress bar regardless of success or failure\n",
    "                            pbar.update(1)\n",
    "                    \n",
    "                    # Handle timeout errors\n",
    "                    except asyncio.TimeoutError:\n",
    "                        pbar.update(1)\n",
    "                    \n",
    "                    # Handle any other errors\n",
    "                    except Exception:\n",
    "                        pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "start_page = 1\n",
    "end_page = 1\n",
    "data_dir = '../data/train/'\n",
    "\n",
    "# Run the combined function\n",
    "await download_datagov_xls(start_page, end_page, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Validate files in Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory containing the spreadsheets\n",
    "data_dir = '../data/train/'\n",
    "\n",
    "# Get the list of file paths\n",
    "spreadsheet_vocab,file_paths = get_vocabulary(data_dir)\n",
    "\n",
    "# Print info\n",
    "print(f'\\n\\nVocabulary size: {len(spreadsheet_vocab._word2idx)}')\n",
    "print(f'Files Processed: {len(file_paths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from utils.parseutil import process_spreadsheet\n",
    "\n",
    "def validate_dir_parallel(directory, vocab, max_rows=10, max_cols=10, max_size_mb=2, max_processing_time=10):\n",
    "    \"\"\"\n",
    "    Validates the contents of a given directory by ensuring all files have supported extensions (.xls, .xlsx, .csv) and are processable.\n",
    "    Unsupported files, files larger than 2MB, or those that fail to process are deleted. Deletes files if a specific type of warning occurs.\n",
    "    Utilizes parallel processing to speed up the validation.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The path to the directory to be validated.\n",
    "        vocab: The vocabulary object for encoding tokens.\n",
    "        max_rows (int, optional): The maximum number of rows to process. Defaults to 10.\n",
    "        max_cols (int, optional): The maximum number of columns to process. Defaults to 10.\n",
    "        max_size_mb (int, optional): The maximum file size in MB to process. Defaults to 2MB.\n",
    "        max_processing_time (int, optional): The maximum processing time in seconds. Defaults to 10 seconds.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # List supported file extensions\n",
    "    supported_extensions = ['.xls', '.xlsx', '.csv']\n",
    "\n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        return\n",
    "\n",
    "    # Cache file to store validated file paths\n",
    "    cache_file_path = os.path.join(directory, \"cache.json\")\n",
    "    \n",
    "    # Load existing cache data or initialize an empty list\n",
    "    if os.path.exists(cache_file_path):\n",
    "        with open(cache_file_path, \"r\") as cache_file:\n",
    "            validated_files = set(json.load(cache_file))\n",
    "    else:\n",
    "        validated_files = set()\n",
    "\n",
    "    # Gather all files in the directory\n",
    "    file_list = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    total_files = len(file_list)\n",
    "\n",
    "    # Filter out files that are already validated\n",
    "    files_to_validate = [file for file in file_list if file not in validated_files]\n",
    "    deleted_files = 0\n",
    "\n",
    "    # Function to validate a single file\n",
    "    def validate_file(file_path):\n",
    "        nonlocal deleted_files\n",
    "\n",
    "        # Skip cache.json file\n",
    "        if os.path.basename(file_path) == \"cache.json\":\n",
    "            return None\n",
    "\n",
    "        # Get the file extension\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "        # Delete file if it does not have a supported extension\n",
    "        if file_extension not in supported_extensions:\n",
    "            os.remove(file_path)\n",
    "            deleted_files += 1\n",
    "            return None\n",
    "\n",
    "        # Check the file size and delete if greater than max_size_mb\n",
    "        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        if file_size_mb > max_size_mb:\n",
    "            os.remove(file_path)\n",
    "            deleted_files += 1\n",
    "            return None\n",
    "\n",
    "        # Process the spreadsheet and delete if None is returned or takes too long\n",
    "        try:\n",
    "            # Start timing the processing\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Suppress the warning and catch it as an exception if it occurs\n",
    "            with warnings.catch_warnings(record=True) as w:\n",
    "                warnings.simplefilter(\"always\")  # Trigger all warnings\n",
    "\n",
    "                # Process the spreadsheet\n",
    "                x_tok, y_tok = process_spreadsheet(file_path, vocab=vocab, max_rows=max_rows, max_cols=max_cols)\n",
    "\n",
    "                # Check if processing was successful\n",
    "                if x_tok is None or y_tok is None:\n",
    "                    raise ValueError(\"Processing returned None\")\n",
    "\n",
    "                # If warnings were captured, identify and delete the file\n",
    "                for warning in w:\n",
    "                    if issubclass(warning.category, UserWarning):\n",
    "                        os.remove(file_path)\n",
    "                        deleted_files += 1\n",
    "                        return None\n",
    "\n",
    "            # Measure the processing time\n",
    "            processing_time = time.time() - start_time\n",
    "\n",
    "            # If processing time exceeds max_processing_time, delete the file\n",
    "            if processing_time > max_processing_time:\n",
    "                os.remove(file_path)\n",
    "                deleted_files += 1\n",
    "                return None\n",
    "\n",
    "            # Mark file as validated if all checks pass\n",
    "            return file_path\n",
    "\n",
    "        except Exception:\n",
    "            # Handle any other exceptions that occur by deleting the file\n",
    "            os.remove(file_path)\n",
    "            deleted_files += 1\n",
    "            return None\n",
    "\n",
    "    # Run validation in parallel using joblib\n",
    "    results = Parallel(n_jobs=int(os.cpu_count() / 2))(\n",
    "        delayed(validate_file)(file_path) for file_path in tqdm(files_to_validate, desc=\"Validating Files\")\n",
    "    )\n",
    "\n",
    "    # Filter out None results (which indicate deleted files) and add successfully validated files to the cache\n",
    "    validated_files.update([result for result in results if result is not None])\n",
    "\n",
    "    # Save the updated cache data back to cache.json\n",
    "    with open(cache_file_path, \"w\") as cache_file:\n",
    "        json.dump(list(validated_files), cache_file)\n",
    "\n",
    "    remaining_files = total_files - deleted_files\n",
    "    print(f\"Total files: {total_files}, Deleted files: {deleted_files}, Remaining files: {remaining_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "directory = '../data/train/'  # Make sure this path exists and contains the files\n",
    "\n",
    "# Assuming spreadsheet_vocab is already defined in your environment\n",
    "vocab = spreadsheet_vocab\n",
    "\n",
    "# Run the parallel validation function\n",
    "validate_dir_parallel(directory, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
